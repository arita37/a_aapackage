{
	"auto_complete":
	{
		"selected_items":
		[
		]
	},
	"buffers":
	[
		{
			"file": "/D/Dropbox/_text/email1.txt",
			"settings":
			{
				"buffer_size": 128533,
				"encoding": "UTF-8",
				"line_ending": "Windows"
			}
		},
		{
			"contents": "#!/bin/bash\necho \"Starting auto-add large files to GIT LFS \"\n\nif [ \"$1\" ]; then\n    maxsize=\"$1\"\nelse\nmaxsize=30485760\nfi\n\ncontent=`find . ! -path './.*'`\n\nSAVEIFS=$IFS\nIFS=$'\\n'\npaths=($content)\nIFS=$SAVEIFS\n\nfor path in \"${paths[@]}\"; do\n    if [ -f \"$path\" ]; then\n        file_size=`du -b \"$path\" | cut -f1`\n        if [ \"$file_size\" -gt \"$maxsize\" ]; then\n            git lfs track \"$path\"\n        fi\n    fi\ndone",
			"file": "/C/tmp/large-1.txt",
			"file_size": 417,
			"file_write_time": 132014539636224946,
			"settings":
			{
				"buffer_size": 417,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/D/Dropbox/_text/coding.txt",
			"settings":
			{
				"buffer_size": 278508,
				"encoding": "UTF-8",
				"line_ending": "Windows"
			}
		},
		{
			"file": "ztodo_list.txt",
			"settings":
			{
				"buffer_size": 5903,
				"line_ending": "Windows"
			}
		},
		{
			"contents": "Proba calibration :\n  actual proba == Model probabiliy.\n\n  Find Kaiso that \n    Actual proba == Model probability +- error\n\n    A set of range s1, s2, s3, ....sn  \n    Actual probability\n\n\n\n\n\n\n\n\n#########################################################################\n\naccumulate experience : 経験を積む\nmigaite: improve\njiko keihatsu : self-improvement\nkizukitai : to build\nkajika suru : visualize\nKeiei-jin ni tsutaeru : tell management.\nteiryoki :  quantitative.\nkeii-jin : Management Team\njakuten / iiten : good point\n\nOn one hand : 一方では\nOn other hand : 一方で, sono atode\n\n\n2)#### Rakuten, which role\n   Principal Data Scientist, \n   Project no kouken, kikai gakushuu no riyou\n\n   3-tsu no project :\n     1 Search Recommender system\n     2 Data processing : NLP\n     3 Scoring model : user data, score model.\n\n\n3) #### Jakuten / Tsuyuoi ten ?\n  nihongo ha mada tarinai desu\n  joujou ni joutatsu shite imasu.\n\n\n4)#### Naze Rakuten wo detai ?\n  Recruiter kara, \n  Career no tame, Axa to Axa group wo kyoumi ga aru.\n  Izen no Hoken gaisha to Big Data \n  wo ikasu koto ga arimasu.\n\n\n5)#### Manager toshite, Nani ga dekimasu ?\n   ING de assistant manager toshite,\n   financial engineering team wo kanri shite.\n   Modelling no kaizen no tame,\n\n   on other hand, kinyu system development.\n\n   Shin shouhin no kaihatsu no tame,\n      actuary to hataraita.\n\n   ooha na keiken, manager toshite, bukka ni keiken wo tsutaete, \n\n\n6)##### Hoka no bumon to, communication process dou shimasu ka ?\n   Kaisha no houshin, yari kata wo benkyou shimasu.\n\n\n7) ############### Naze  Axa wo kyoumi ga arimasu ka ?\n  Moto moto, Izen on kinyu no keiken wo ikashitai desu.\n\n  Nagai aida ni, Axa Group wo kyoumi ga arimaru.\n  Axa Group ha Nihon de, Gaikuoku de career ga kanou ga arimasu.\n\n  Kinyou kaisha to Hoken gaisha deha arimasen.\n  Big Data ha ok desuga, Business ha \"beginner\", \n  Business keiken ga nai, hanashi nikui desu.\n\n  Hoken no pricing model wo keiken ga arimashita.\n  Actuary to hatarita kara,\n\n\n8) ############### dona project ?\nProject kouken ni shinagara,  izen no keiken wo ikashitai desu.\nShoshitara, project no sakusei.\n\n\n9) ############# Manager to Specialist no path   ?\n10 nen kan, Kinyuu to Hoken Gaisha to Big Data ga arimasuka, \nsenior manager to tame, manager toshite, keiken no share shi yasui to omoimasu.\nData Senryaku ni kouken shimasu.\n\nGenzai, specialist level ga takai to omoimasu, amari shinpo dekinai.\nDaigaku de benkyou igai, shinpo ga dekinai to omoimasu.\n\n\n\n2 tsu noo bubun ni wakeru koto gadekimasu.\n  Project no senryaku\n  Project no jitsu no gitjutsu no riyou : Deep Learning, Jitsu model.\n\n\n10) #### Big data system  no riyou ?\n   Senmon desu.\n   Search system no tsukuru tameni :\n      Kafka, Spark, Hadoop system.\n\n    \n11) #### API system wo tsukuru\n\n\n\n\n13) ####   \n\n\n\n\n\n\n\n\n\n\n##### Question to Company :   ###########################################\n  Zama na request\n\n1) Gutani teki dona Project ni sanka  shinakya ?\n\n2) Tatoeba, project no output ha\n  model mataha API system ですか？\n\n3) Team no yakimu wari\n\n4) Data senryaku\n\n \n  Team deha, Sugakusha mataha Actuary no shushin ha irashaimasuka ?\n\n\n\n\n\n\n\nYoku mirareru machigai wa, jiko shōkai o motomerareta toki ni \njiko PR o shite shimau case desu. Futatsu no chigai wa nani ka to iu to, jiko shōkai wa `aisatsu' to `komyunikēshon no kikkake-tsukuri' de, jiko PR wa `nōryoku ya iyoku no apīru'desu. \n\nMensetsu-kan kara sureba, jiko shōkai o motomete iru no ni,\n tsuyomi, sukiru, chishiki, kachikan, iyoku nado o hanasa rete wa tōtotsu-sa ni konwaku shite shimaimasu. `Shitsumon no ito o rikai shiteinai noda na' to komyunikēshonsukiru o utagawa reru koto ni mo nari kanemasen. Jiko shōkai to shite hanasu koto to, jiko PR to shite hanasu koto wa wakete kangaemashou.\n\n\n\n\n\nShūkatsu no mensetsude wa jiko PR ya shibō dōki wa kanarazu kika reru shitsumon'nanode, hotondo subete no gakusei-san ga junbi shite kimasu. Shikashi, mensetsu ga hajimatte ichiban saisho ni `kantan ni jiko shōkai o shite kudasai' to iwa reta toki ni, junbi fusoku de awatete shimau gakusei-san ga takusan irasshaimasu. Jiko shōkai to wa jiko PR to wa dō chigau nodeshou ka? Nani o hanashitara yoi nodeshou ka? Korera no gimon o kaiketsu subeku, jiko shōkai to jiko PR no chigai to jiko shōkai de hanasubeki naiyō ni tsuite go setsumei shite ikimasu. Jiko shōkai o shite kudasai to iwa rete mo awatenai yō ni, shikkari jiko shōkai no naiyō o haaku shite ikimashou. Mokuji `jiko shōkai' to `jiko PR' no chigai entorīshīto ga aru ni mo kakawarazu, mensetsu de jiko shōkai o jisshi suru riyū mensetsu de jiko shōkai ni yōsuru jikan no meyasu mensetsu no jiko shōkai ni kanarazu hitsuyōna jōhō mensetsu de yoku aru damena jiko shōkai no reibun mensetsu de tasha to sa o tsukeru, miryoku-tekina jiko shōkai no reibun mensetsu de umaku jiko shōkai o suru kotsu matome\n\n\n\n####################################################################\n部下を持ったことはありますか。\n  Hai, 2 persons wo motte imasu.\n\n\n\n・部下の指導において、重要であると考えることは何ですか。\n・翻訳業務には対応できますか。\n・通訳業務には対応できますか。\n\n・今後は、専門職としてのスキルを磨いてゆきたいですか。それとも、マネジメントを行ってみたいですか。\n\n・複数の部門との調整を行うにはどのような手段を使いますか。\n・自己啓発のために行っていることはありますか。\n・リスク管理のキャリアを築きたいと考えたのはどうしてですか。\n・ITのどのような言語を使用することができますか。\n\n・当社における大きなリスクは何であると考えますか。\n・事業保険、医療保険、死亡保険、年金保険のどの保険についても基礎的な商品知識はありますか。\n\n・リスクを可視化するにはどのような手法を使いますか。\n・リスクを表すにおいて、定量的リスク指標のなかで、一番重要であると思う指標は何ですか。\n\n・定性的リスクを経営陣に伝えるには、どのような手法を利用しますか。\n\n・苦手な業務は何ですか。\n\n\n\n\n\n—————————————————————————-----------------------------------------\n\n· Have you ever had a subordinate?\n· What do you think is important in guidance of subordinates?\n· Can you handle translation work?\n· Can you handle interpreting work?\n· Do you want to develop your professional skills in the future? Or would you like to do management?\n· What measures will you use to coordinate with multiple departments?\n· Do you have something to do for self-development?\n· Why did you want to build a career in risk management?\n· Which languages ​​like IT can you use?\n\n· What do you think is a major risk in our company?\n· Do you have any basic product knowledge about insurance such as business insurance, medical insurance, death insurance, annuity insurance?\n· What kind of method is used to visualize risk?\n• What are the most important indicators of quantitative risk indicators in representing risk?\n· What sort of method is used to convey qualitative risk to management?\n· What is a weak business?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello Sir,\n\nAm working at Rakuten Tokyo .as Principal Data Science.\nHave some interest at Indeed by having a top notch engineering reputation \n(also a friend working there).\n\nMy specialties are :\n   Quantitative Finance (modelling and al., stats)\n   Japanese NLP.\n   Fintech \n   Search Ranking Algo / Recommender system.\n   User data analysis. \n   Can speakr Japanese fluently , 10 work years exp. (finance as quant \\, mainly)\n\nAm not looking actively but happy to discuss if one role may fit.\nMy github is \nhttps://github.com/arita37\n   \n\n\n\n\n############################################################################\n############################################################################\nHello, \n\nI am principal Data Scientist at Rakuten , \nworking on large data analysis (ML prediction, classification, ...) for Japanese\nretail.\n\nI have strong interest in Cogent Lab activity due to my background in Finance, and \ncurrent activity in Data Science.\n\nI believe my technical profile and deep knowledge of business (finance, retail) can match Cogent Lab activity.\n\nFor example of projects where I can contribute :\n   Japanese retail business (forecast, recommender system, search)\n   Asset Management : Low frequency\n\n\n\n\n\n\n\n\n",
			"file": "/D/Dropbox/_text/interview3.txt",
			"file_size": 8829,
			"file_write_time": 131925219376305946,
			"settings":
			{
				"buffer_size": 7593,
				"line_ending": "Windows"
			}
		},
		{
			"file": "/D/Dropbox/inter_prep.txt",
			"settings":
			{
				"buffer_size": 4636,
				"encoding": "UTF-8 with BOM",
				"line_ending": "Windows"
			}
		},
		{
			"contents": "\n\n\nA bit pretentious title but we can see few trends coming out in this area.\n\nCommoditization of AI is already in the pipelines, the cost lower and lower every day.\n\nWith the achievements of Deep Learning into image recognition, voice recognition,..., a clear trend is drawing. This is the automation of human basics senses : seeing, listening, speaking by ML type algorithms, mostly transforming raw input data into numerical data that can be processed by other ML algorithms.\n\nThe company which benefit the most are the ones who receive significant amount of raw data on a daily basis : Intermet companies.\n\nBy open sourcing their research and implementation, they sacrificed Intellectual Property to the development of ML itself...  They believe the cost of research is too high, open source is a way to do stimulus\nand they are protected by the scale of their own data.\n\nThus, TensorFlow, Pytorch was born from previous research based framework such as Torch and Theano.\n\n\n\nAnother field which is emerging is the generation process:  generate actual raw data (ie images) from ML internal representation. Recognition and Generation processes are dual processes since both rely on the internal ML representation (ie  P(X/y)  vs  P(y/X)  )\n\nThe true question is how can we represent complex raw data into a numerical form comprehensible by Machines. \nMore, how can the machine learn the representation through algorithmic processing\n\nCurrent answer is let's use the gradient in a probabilistic way (ie Stochastic Gradient Descent).\nThis is called the connectionnist, intuitionnist way.\n\n\n\nComputing resources needed for ML generation is much higher, \nso the field is accelerated by another trend : AI specialized hardware.\nFew examples :  Nervanna, Graphcore, TPU,..., all are developing specialized hardware to accelerate\nML type computations.\n\nThis is the key of ML development, how much efficient TeraFlops is available per algorithm.\nOn one hand, research makes the algorithms more efficient, less Teraflops to get convergence.\nOn other hand, hardware research makes the hardware compute more efficiently.\n\n\n\nIn the middle of research battle for better ML algorithms, lies the Natural Language Processing\nor now Understanding. Very active topic because having the way to process automatically\nall human language ressources allows Machine to learn by themselves...\nIt looks simple but take the example of DeePL company which is using 5 PetaFlops computer\nto train their translation models.\nhttps://en.wikipedia.org/wiki/DeepL_Translator\n\n\n\nMaybe the future of AI trends lies in one picture .\n\n\n\n\n\n\nThis picture shows what Human reasoning is doing from simple recognition\nto multi-steps planning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##################################################################################\n\n\n・部下を持ったことはありますか。\n・部下の指導において、重要であると考えることは何ですか。\n・翻訳業務には対応できますか。\n・通訳業務には対応できますか。\n・今後は、専門職としてのスキルを磨いてゆきたいですか。それとも、マネジメントを行ってみたいですか。\n\n\n・複数の部門との調整を行うにはどのような手段を使いますか。\n・自己啓発のために行っていることはありますか。\n・リスク管理のキャリアを築きたいと考えたのはどうしてですか。\n・ITのどのような言語を使用することができますか。\n\n・当社における大きなリスクは何であると考えますか。\n・事業保険、医療保険、死亡保険、年金保険のどの保険についても基礎的な商品知識はありますか。\n・リスクを可視化するにはどのような手法を使いますか。\n・リスクを表すにおいて、定量的リスク指標のなかで、一番重要であると思う指標は何ですか。\n・定性的リスクを経営陣に伝えるには、どのような手法を利用しますか。\n・苦手な業務は何ですか。\n\n\n\n\n—————————————————————————\n\n· Have you ever had a subordinate?\n· What do you think is important in guidance of subordinates?\n· Can you handle translation work?\n· Can you handle interpreting work?\n· Do you want to develop your professional skills in the future? Or would you like to do management?\n· What measures will you use to coordinate with multiple departments?\n· Do you have something to do for self-development?\n· Why did you want to build a career in risk management?\n· Which languages ​​like IT can you use?\n\n· What do you think is a major risk in our company?\n· Do you have any basic product knowledge about insurance such as business insurance, medical insurance, death insurance, annuity insurance?\n· What kind of method is used to visualize risk?\n• What are the most important indicators of quantitative risk indicators in representing risk?\n· What sort of method is used to convey qualitative risk to management?\n· What is a weak business?\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello Sir,\n\nAm working at Rakuten Tokyo .as Principal Data Science.\nHave some interest at Indeed by having a top notch engineering reputation \n(also a friend working there).\n\nMy specialties are :\n   Quantitative Finance (modelling and al., stats)\n   Japanese NLP.\n   Fintech \n   Search Ranking Algo / Recommender system.\n   User data analysis. \n   Can speakr Japanese fluently , 10 work years exp. (finance as quant \\, mainly)\n\nAm not looking actively but happy to discuss if one role may fit.\nMy github is \nhttps://github.com/arita37\n   \n\n\n\n\n############################################################################\n############################################################################\nHello, \n\nI am principal Data Scientist at Rakuten , \nworking on large data analysis (ML prediction, classification, ...) for Japanese\nretail.\n\nI have strong interest in Cogent Lab activity due to my background in Finance, and \ncurrent activity in Data Science.\n\nI believe my technical profile and deep knowledge of business (finance, retail) can match Cogent Lab activity.\n\nFor example of projects where I can contribute :\n   Japanese retail business (forecast, recommender system, search)\n   Asset Management : Low frequency\n\n\n\n\n\n\n\n\n\n\n############################################################################\n############################################################################\n\n\nStackOVerflow :\nnoelkev0@gmail.com (used Sep 28)\n\n\n\npycharm\nmini conda\nevevrything\nprocess auto startu p microsfot\nsftp winscp\nMoba Xterm\nCon Emu Pack  Command Line\nAOMEIBackupper\nhttps://s3-us-west-2.amazonaws.com/github-raku/aomeisoBackupperFull.exe\n\n\n\n\n\n\n\n\n\n###########\n\nFTP \n52.26.181.200\nlogin : ubuntu\nuse oregon private key\n\nhttp://52.26.181.200:8888/tree#notebooks\naws\naccesss key\nAKIAIGDRLS6LA3ZTUIWA\n\n\nsecret\n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\n\npass\nmJMnzDKz}Xyw\n\n\n\n\n##################################################################################\nThank you for your order! (Order no. 57510669)\nYour purchased products\n\n1 x EmEditor \n\nregistration key:\nDVAZZ-285SF-WB3UQ-KBBP5-KNMNH\n\nPartition Master EaseUS\n\nXplorer 2\nEditPad Pro\nChrome, Firefox,  Save offline, Pera Pera\nFileLocator\nEverything\nTree size Professionnal\nAnaconda 2\nWinpython\nVisual Studio 2017\nAoemei Backupper\nhttps://cyberduck.io/    Google Drive / Amazon S3 computation, Best\nmoutainduck\nhttp://www.quickaccesspopup.com/\n    http://www.quickaccesspopup.com/how-do-i-enable-total-commander-support-in-quick-access-popup/\n\n[xplorer2 lite (v2.5.0.4)]\n; http://www.xyplorer.com/free.php\n; NOTE: enable \"Always open new tabs in a single window\" in Tools -> Advanced ->  Single Window Mode\n; source: Roland Toth\nAppPath=C:\\_app\\xplorer64\\xplorer2_lite.exe \nCommandLine=/M %Path%\nNewTabSwitch=\n\n\n\n\n\n#### Edipad Pro  ################################################################################################################\n    License name: Kevin Noel\n  Contact person:    Contact email: kevin.yakimono@gmail.com\n EditPad User ID: epp-318a173\n  Licensed users: 1\nDate of purchase: 2 April 2015\n\nYour license is valid for EditPad Pro versions 7.x.x on the Microsoft Windows platform.\n\nIf any of this information is incorrect, please contact sales@editpadpro.com at \n\nTo download the full version of EditPad Pro, please visit http://www.editpadpro.com/download.html where you will be asked for the contact email and user id mentioned above.  After providing those, your fully licensed copy of EditPad Pro will be generated instantly, ready for download.\n\nWhen a free minor upgrade is released, simply download the full version again as explained above, \nand install it on top of the version you already have. \n We will notify you of new versions via our monthly newsletter, to which you can subscribe at http://www.editpadpro.com/maillist.html \n\n\n\n############# Orders    #######################################################\nThank You For Your Order\nOrder ID: ZAB170222-2969-57119\nCharges will appear on your bill as: FS *xplorer2\nxplorer² professional\nJPY3,505\n\n\nYour registration key is:\n\n66|3.ZAB170222-2969-57119|X2.2|1|22.02.17|0|Kevin_Noel|noelkev0@gma\nil.com|W0uJyNM334/G+O7OaldeChI/zUYsT0lNqhgx3V9wymjdTuu+8Tr2LiTZtqjZ\nO0P4vVLzPCQw4cvwYrFKNU5e9uxs3Lolo2zI0gAPGbihpTNrlTggH5pkyx/glERREHz\ngB3cNvUB6BWI=\n\nYour registration reference# is: X2.2-7C47DC42\nPlease keep these details safe for future use!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n######   Equivalent \n/project27/ \n/project27/linux_project27\n\n\n###############################################################################################\n### Startup Loader      #######################################################################\n%load_ext autoreload\n%autoreload\nimport os, sys; CFG= {'plat': sys.platform[:3]+\"-\"+os.path.expanduser('~').split(\"\\\\\")[-1].split(\"/\")[-1], \"ver\": sys.version_info.major}\nDIRCWD= { 'win-asus1' : 'D:/_devs/Python01/project27/', 'win,unerry' : 'G:/_devs/project27/' , 'lin-noel': '/home/noel/project27/', 'lin-ubuntu': '/home/ubuntu/project27/' }[CFG['plat']]\nprint( CFG, DIRCWD, str(os)[13:])\nos.chdir(DIRCWD); sys.path.append(DIRCWD + '/aapackage' if CFG[\"ver\"] == 2  else  sys.path.append(DIRCWD + '/aapackage3')  ); # sys.path.append(DIRCWD + '/linux/aapackage')\nexec(compile(open(DIRCWD + '/aapackage/allmodule.py').read(), DIRCWD + '/aapackage/allmodule.py', 'exec'))   \n#      execfile( DIRCWD + '/aapackage/allmodule.py')\n\n\ndef execfile(filepath, globals=None, locals=None):\n    if globals is None:   globals = {}\n    globals.update({ \"__file__\": filepath,  \"__name__\": \"__main__\", })\n    with open(filepath, 'rb') as file:\n        exec(compile(file.read(), filepath, 'exec'), globals, locals)\n\n\n\n\n################################################################################################################\n###### Bash Command ############################################################################################\nNever Use  Slash /   at folder writing\nrm -rf mydir      remove all the directory without notice\nls    list current directory\npwd    current directory \ncp -r      /home/noel/anaconda2/pkgs/       /home/noel/\n\n\n####### Linux Screen attachment \nscreen -r 25555                  #### Attach session\nscreen -X -S   26655  quit       #### Kill session\nscreen       ### New session\nCtrl+A+D    ##Quit sesssion\n\n\n#### File Editing\nexport VISUAL=nano\n\ncrontab -e\n\ngrep bbiz   /var/log/syslog\n\n/home/noel/project27/scheduler/cmd_line/bbiz_run.sh\n\n\ntar -zcvf /home/noel/archive_vm_homenoel.tar.gz    /home/noel/   ####Compress Folder\n\n\nchmod +x bbiz_run.sh    #### W\nsudo ln -s /home/noel/project27/scheduler/cmd_line/bbiz_run.sh   /usr/local/bin/\n\n\nchmod +x /usr/local/bin/bbiz_run.sh \n\n\n### Backup Folder\nsudo tar -zcvf /home/noel/archive_vm_anaconda.tar.gz    /home/ubuntu/anaconda3\n\n\n\n\n\n\n\n\n\n\n##############################################################################################################\n######  AWS Oregon GPU #######################################################################################\nIP :    52.26.181.200\nSpot Instnance 54.69.65.45\n\nwhoami   :  ## user\nwhich python \nsu - ubuntu    #change to ubuntu\nsource activate sandbox\n\n\ndf command - Shows the amount of disk space used and available on Linux file systems.\ndu command - Display the amount of disk space used by the specified files and for each subdirectory.\nbtrfs fi df /device/ - Show disk space usage information for a btrfs based mount point/file system. Read more\n\n\nnetstat -tulpn \n#### Check if GPU is fine\n nvidia-smi \n\n\n\n#############################################################################################################\n##################### AWS SSH ###############################################################################\nOther Linux  :  No problem just the name of SSH\nubuntu@ExternalIPAdress\n\n\n###Convert Amazon PEM to Putty PPK\nhttps://linuxacademy.com/howtoguides/posts/show/topic/17385-use-putty-to-access-ec2-linux-instances-via-ssh-from-windows\n\n##### use PPK to connect\nD:\\_devs\\aws\\keypairs\\oregon\n\nputty -ssh    ubuntu@52.26.181.200  22\n\"putty.exe\" -load \"EC2_oregon\"\n\n\n\nCredentials File and Profiles\nInstead of keeping credentials in environment variables,\nyou can now put credentials into a single file that’s in a central location. The default location is this:\n\n~/.aws/credentials (Linux/Mac)\n%USERPROFILE%.awscredentials  (Windows)\n\n\n[default]\naws_access_key_id = ACCESS_KEY\naws_secret_access_key = SECRET_KEY\naws_session_token = TOKEN\nC:/Users/asus1/.aws/.awscredentials  \n\n\nThe AWS credentials file – located at ~/.aws/credentials on Linux, macOS, or Unix, or at C:\\Users\\USERNAME \\.aws\\credentials on Windows. This file can contain multiple named profiles in addition to a default profile.\n\n#Amazon keys\n# os.environ[\"AWS_ACCESS_KEY_ID\"] =     'AKIAJDE3GSLR5FCRIXIQ'\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = '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'\n\n\n\n############################################################################################################\n####Jupyter notebook  ######################################################################################\nnohup jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser &>> /home/ubuntu/project27/logfile_server.txt\n\n\n### CRONTAB -E\n@reboot sleep 5 &&  nohup  /home/ubuntu/anaconda3/bin/jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser  &>> /home/ubuntu/project27/logfile_jupyter_server.txt\n\nsu ubuntu -c \"/usr/bin/jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser  &>> /home/ubuntu/project27/logfile_server.txt\"\nsu <username> -c \"/usr/bin/ipython notebook --no-browser --profile <profilename> &\"\n\n\n@reboot sleep 5 && su ubuntu -c \"nohup  /home/ubuntu/anaconda3/bin/jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser  &>> /home/ubuntu/project27/logfile_jupyter_server.txt\"\n\n\n\n\nnetstat -tulpn   ###List of notebook running           \n\nkill pid  ###Kill\n\nhome/noel/project27/log_server.txt   ####Log \n\njupyter notebook password sophie237\n\n\n### to see the token and URL of jupyter notebook  ########################################################\njupyter notebook list\n\n\n####Kill Jupyter   #########\nnetstat -tulpn\n\n\n\n\n#############################################################################################################\n#### Conda Environnment    ##################################################################################\n\n#### NLP environnment  #############\nconda create --name tf_gpu_13\nconda install --yes -c conda-forge pip\n\nconda install --yes -c anaconda gensim \nhttps://pypi.python.org/pypi/fasttext\nconda install spacy\nhttps://github.com/facebookresearch\nhttps://github.com/facebookresearch/faiss\n\n\ngit clone https://github.com/facebookresearch/faiss.git    github/faiss\ncd github/faiss\n# Move the file makefile.inc  into main directory and\nsudo apt-get install libopenblas-base\n# change makefile.inc for main\n\n\nconda create --verbose --name python2   -f \"D:\\_devs\\Python01\\project27\\__config\\condaenv\\condaenv_win-asus1-7_python2_base.yml\" \n\n\nconda create --verbose --name python2  \n\n\n\n\n\n\n\n\n#### Batch Install\nconda install  -n tf_gpu_12  --no-deps --no-update-deps --verbose  --yes    regex   \n\npip install --upgrade --no-deps --force-reinstall <packagename>\n\ngit clone https://github.com/facebookresearch/faiss.git    linux_project27/mlearning/\n\n\n\nconda install --no-deps --no-update-deps --verbose  --yes   -n root   mkl=11.3.3=1\n\n\nconda uninstall --force --verbose  --yes -n root cffi=1.10.0=py27_0\n\n\nconda install --no-deps --no-update-deps --verbose  --yes   -n root   cffi=1.60.0=py27_0\n\n\n\nmini-conda\n  --->  gui_py2\n  --->  gui_py3\n  --->  python2\n  --->  python3\n\n\n\n\n\n\n\n#!/usr/bin/env bash\n\ncd ~\nwget http://repo.continuum.io/archive/Anaconda2-4.0.0-Linux-x86_64.sh\nbash Anaconda2-4.0.0-Linux-x86_64.sh -b\necho 'PATH=\"/home/ubuntu/anaconda2/bin:$PATH\"' >> .bashrc\n. .bashrc\n\njupyter notebook --generate-config\n\nkey=$(python -c \"from notebook.auth import passwd; print(passwd())\")\n\ncd ~\nmkdir certs\ncd certs\ncertdir=$(pwd)\nopenssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.key -out mycert.pem\n\ncd ~\nsed -i \"1 a\\\nc = get_config()\\\\\nc.NotebookApp.certfile = u'$certdir/mycert.pem'\\\\\nc.NotebookApp.keyfile = u'$certdir/mycert.key'\\\\\nc.NotebookApp.ip = '*'\\\\\nc.NotebookApp.open_browser = False\\\\\nc.NotebookApp.password = u'$key'\\\\\nc.NotebookApp.port = 8888\" .jupyter/jupyter_notebook_config.py\n\n\n\ntmux new -s nb\nmkdir notebook\ncd notebook\njupyter notebook\n\n\nAmazon EC2 automatically detects the public DNS name of your instance and then populates Public DNS for you. It also detects the name of the key pair that you specified when you launched the instance. Complete the following, and then choose Launch SSH Client.\n\nIn User name, enter the user name to log in to your instance.\n\nTip\nFor Amazon Linux, the user name is ec2-user. For RHEL5, the user name is either root or ec2-user. For Ubuntu, the user name is ubuntu. For Fedora, the user name is either fedora or ec2-user. For SUSE Linux, the user name is either root or ec2-user. Otherwise, if ec2-user and root don't work, check with your AMI provider.\nIn Private key path, enter the fully qualified path to your private key (.pem) file, including the key pair name; for example:\n\nC:\\KeyPairs\\my-key-pair.pem\n\n(Optional) Choose Store in browser cache to store the location of the private key in your browser cache. This enables Amazon EC2 to detect the location of the private key in subsequent browser sessions, until you clear your browser's cache.\n##########################################################################################################################\n#########################################################################################################################\n\n\n\n####Python  Environnemnt\n############################################################################################################\nDIRCWD set in Ubuntu :\n/root/.bashrc      No\n/etc/profile       No\n/etc/environment   Ok, works for Jupyter in crontab -e   HERE FOR CRONTAB -E Launch\n/home/ubuntu/.bashrc   Not works\n\n\n/root/.bash_history\n/home/ubuntu/.bash_history\n\n\n\n\nexport  CONFIGMY_ROOT_FILE=\"/home/ubuntu/project27/__config/CONFIGMY_ROOT_FILE.py\"\n\n\nLC_ALL=C fgrep -Irsl --exclude-dir={\\*ubuntu\\*}   'DIRCWD=' /\n\n\nDIRCWD=\"/home/ubuntu/project27/\"\nCONFIGMY_ROOT_FILE=\"/home/ubuntu/project27/__config/CONFIGMY_ROOT_FILE.py\"\n\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\"\n\n\n\n\n\n\n\n\n\n###################################################################################################\nOriginal Answer\nYou should set it in /etc/environment.\n\nTry sudo YOUR_TEXT_EDITOR /etc/environment (make sure to create a backup first).\n\nFor more information: EnvironmentVariables\n\n\nSystem-wide environment variables\nEnvironment variable settings that affect the system as a whole (rather then just a particular user) \nshould not be placed in any of the many system-level scripts \nthat get executed when the system or the desktop session are loaded, but into\n\n\nsudo nano /etc/environment\n\n/etc/environment - \nThis file is specifically meant for system-wide environment variable settings. \nIt is not a script file, but rather consists of assignment expressions, one per line. \nSpecifically, this file stores the system-wide locale and path settings.\n\n\nNot recommended:\n/etc/profile - \nThis file gets executed whenever a bash login shell is entered (e.g. when logging in from the console or over ssh), as well as by the DisplayManager when the desktop session loads. This is probably the file you will get referred to when asking veteran UNIX system administrators about environment variables. In Ubuntu, however, this file does little more then invoke the /etc/bash.bashrc file.\n\n/etc/bash.bashrc - This is the system-wide version of the ~/.bashrc file. \nUbuntu is configured by default to execute this file whenever a user enters a shell or the desktop environment.\n\n\n./bashrc  only for local user\n\n\n\n\n\n\n\n\n\n#### Python 2.7\nhttp://conda-test.pydata.org/docs/faq.html\n\nconda create -n python2 python=2.7 anaconda           source activate python2\nconda create --name sandbox_py2  --clone python2      source activate py2_sandbox\n\nconda create --name automl  --clone root\n\n\nconda create --name  tf_gpu_13\n\n\n\n##Python 3.6\nconda create --name tf_gpu_12  --clone root       source activate tf_cpu_12\nconda create --name tf_cpu_12  --clone root       source activate tf_gpu_12  # GPU TensorFlow\n   \nconda create --name sandbox  --clone root         source activate sandbox    \n#  Can install anything, only for sandbox install of critical \n\nconda install -c conda-forge pip\n\n\n\n#############Package  ######################################################################################\n#### Installed witout updating dependencies.\n####### CONDA :   ##################################\nhttps://conda.io/docs/troubleshooting.html#fix-broken-conda\n\n\n\n####Build Same CONDA from File:\nhttps://conda.io/docs/user-guide/tasks/manage-environments.html#building-identical-conda-environments\n\nconda list --explicit > spec-file.txt     #Export current environnment\n\nconda create --name myenv --file spec-file.txt     # Create Env using a file\n\n\n\n####### Packages    #########################################################################################\nconda config --add channels conda-forge\nconda install -c  conda-forge   pygmo      --no-update-dependencies \n\nconda install -c conda-forge qt==5.6.2\nconda install -c conda-forge partd==0.3.8\n\n\n##### Forecast model\nconda install -c conda-forge pystan=2.15.0.1\n\n\n####only linux for python 2.7\nconda install -c conda-forge fbprophet=0.1.1  ####only linux for python 2.7\n\n\nManaging conda and anaconda        ########################################################################\n\nconda info                Verify conda is installed, check version #\nconda update conda        Update conda package and environment manager to current version\nconda update anaconda     Update the anaconda meta package (the library of packages ready to install with conda command)\n\n\nManaging packages, including Python ########################################################################\n\nconda list   View list of packages and versions installed in active environment\nconda search beautiful-soup Search for a package to see if it is available to\nconda install       conda install -n bunnies beautiful-so up Install a new package\n\n\nconda install -c pandas bottleneck Install a package from a specific channel \n\n\nRemove packages, environments, or channels\nconda remove --name bunnies beautifulsoup  Remove one package from any named  environment\nconda remove beautiful-soup                Remove one package from the active environment\nconda remove --name bunnies beautiful- soup astroid          Remove multiple packages from any environment\nconda remove --name snakes --all                             Remove an environment\n\n\n###### EC2 Linux Python Package Install  ##########################################################\nPYGMO       conda install -c willemolding pygmo=1.1.6\nDEAP        conda install -c mq deap=1.1.0\nCelery task   conda install -c conda-forge celery=3.1.23\nconda install -c conda-forge ipyparallel=5.2.0\n\nconda install -c conda-forge tpot=0.6.4\nconda install -c conda-forge urllib3=1.18.1\nconda install -c conda-forge tabulate=0.7.7 \n\nconda install -c anaconda bcolz=1.0.0\nconda install -c anaconda scikit-learn=0.18.1\n\n\npip install catboost  : #category analyzer for classification\n\n\n\n\n#Error in IPython Network Need to install QT4 on Command Line\nsudo apt-get install -y python-qt4\n\n\n\n####Tensor Layer  ########################################################################\npip install git+https://github.com/zsdonghao/tensorlayer.git     --no-dependencies\n\n\nHowever, if you want to modify or extend TensorLayer, \nyou can download the repository from Github and install it as follow.\ngit clone --depth=50 --branch=master https://github.com/rhiever/tpot.git   github/rhiever/tpot\ncd to the root of the git tree\npip install -e .\n\nThis command will run the setup.py to install TensorLayer. \nThe -e reflects editable, then you can edit the source code in tensorlayer folder, \nand import the edited TensorLayer.\n\n\n\n##### Install from git\ngit clone --depth=50 --branch=master https://github.com/rhiever/tpot.git   github/rhiever/tpot\ncd github/tpot\npip install -e .\n\n\n\n##### Install from Travis  Clean #########################################################\nconda create --name myenv --file spec-file.txt   python=2.7 anaconda     \n\n\nTravis Build Files extract the pip \n\n\n\n####\ngit clone https://github.com/facebookresearch/fastText.git     github/fastext\n$ git clone https://github.com/facebookresearch/fastText.git\n$ cd github/fastext\n$ make\n\n\n\npip install tpot  --no-dependencies\n\n\n\n\n#### Jupyter Notebook Cache\nhttps://github.com/rossant/ipycache\n\n\n\n\n\n\n##### Fast Text : Gen Sim   ##############################################################\nFast Text PyFast Text\nhttps://github.com/vrasneur/pyfasttext/tree/master/test\n\n\n\n\n\n$ cat test.ini\n[First Section]\nvar = value\nkey = item\n\n[Second Section]\nothervar = othervalue\notherkey = otheritem\nAnd then:\n\nfrom ConfigParser import ConfigParser\nconfig = ConfigParser().read('test.ini'); CFG_ = {s:dict(config.items(s)) for s in config.sections()}\n\nd = {line.strip().split(' = ') for line in file(filename)}\nd = {line.strip().split(' = ') for line in open(\"config.txt\", mode='r')}\n\n\n\n#### System Variables == os.environ  Windows\nsetx DIRCWD \"D:/_devs/Python01/project27/\"   /M\nECHO  %DIRCWD%\n\n\n#### System Variables == os.environ  Ubuntu\nnano $HOME/.bashrc              #  \nsource $HOME/.bashrc            #  Reload file\n#python dircwd path\nexport DIRCWD=/home/ubuntu/project27/\n\n\nnano   ./.bashrc\nsource ./.bashrc                           #  Reload file\nexport DIRCWD=/home/ubuntu/project27/\n\n\nnano /etc/profile                   #  Reload file\nsource /etc/profile  \nexport DIRCWD=/home/ubuntu/project27/\n\n\n\nhttps://www.dabapps.com/blog/python-tools-local-continuous-integration/\n\n\n\n\n\n\n#####TPOT  ####################################################################################\ngit clone --depth=50 --branch=master https://github.com/zsdonghao/tensorlayer.git   project27/github\ncd project27/github\n\nnumpy 1.12.1\nscipy 0.19.1\nsklearn 0.18.2\ndeap==1.0\n\n\n76.67s$ source ./ci/.travis_install.sh\n39.25s$ bash ./ci/.travis_test.sh\n\nxgboost 0.6 \nupdate_checker 0.16 \ntqdm 4.14.0\n\n\n\n\n###############################################################################################\n### ML Box : Auto ML  #########################################################################\nconda install -c conda-forge xgboost    --no-dependencies\n\npip install mlbox --no-dependencies\n\n\n\n\n### Auto SK Learn   ###########################################################################\nInstall into conda using Travis YAML file and execute it.\n\n\nsource activate autosklearn\ncd /home/ubuntu/automl/autosklearn\npython   run_test.py\n\n\n\n\n\n#####################################################################################################################\ngit clone git://github.com/SpringSource/spring-data-graph-examples.git\n\ngit clone --depth=5 --branch=master https://github.com/automl/auto-sklearn.git automl/auto-sklearn\n\n\nconda install --yes gcc swig\nconda install --yes libgcc\nsudo apt-get install build-essential swig\nsudo apt-get install build-essential gcc\n\ncurl https://cdn.rawgit.com/automl/auto-sklearn/7d33420a644dd3b1e0db26664d01d856a7ca5fb2/requirements.txt   | xargs -n 1 -L 1 pip install \npip install xmltodict requests\npip install git+https://github.com/renatopp/liac-arff\npip install git+https://github.com/openml/openml-python@0b9009b0436fda77d9f7c701bd116aff4158d5e1 --no-deps\npip install auto-sklearn   --no-dependencies\n\nhttps://travis-ci.org/automl/auto-sklearn/jobs/241098164/config\n\n\npip install smac                              #Build with gcc\n\npip uninstall pyrfr \npip uninstall auto-sklearn\nCC=/home/ubuntu/anaconda3/envs/sandbox/bin/gcc pip install pyrfr auto-sklearn --no-cache-dir   --no-dependencies\n\n\nhttps://github.com/automl/auto-sklearn/issues/308\n\n\n\npip install auto-sklearn   --no-dependencies\ncurl https://rawgit.com/automl/auto-sklearn/v.0.2.0/requirements.txt | xargs -n 1 -L 1 pip install\n\nCC=/home/ubuntu/anaconda3/envs/sandbox/bin/gcc pip install pyrfr auto-sklearn   --no-cache-dir   --no-dependencies\n\n\n\n### Force installed\ncurl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install --no-cache-dir --force-reinstall -I --no-deps --upgrade\n\n\npip install autosklearn --no-cache-dir --force-reinstall -I --no-deps --upgrade\n\npip install pyrfr==0.4.0\n\n\n\n\n#####################################################################################################################\nAuto-Sklearn returns an ensemble of models.\nYou can use automl.show_models() to look at the ensemble.\nIn general, I recommend to read the paper on auto-sklearn to get an understanding what it does:\nEfficient and Robust Automated Machine Learning at NIPS'15\n\n\nimport autosklearn.classification ; import sklearn.cross_validation\nimport sklearn.datasets           ; import sklearn.metrics\ndigits = sklearn.datasets.load_digits()\nX = digits.data  ;     y = digits.target\nX_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X,y,random_state=1)\nautoml = autosklearn.classification.AutoSklearnClassifier()\nautoml.fit(X_train, y_train)\ny_hat = automl.predict(X_test)\nprint(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n#####################################################################################################################\n\n\n\n\n\nhttps://automl.github.io/auto-sklearn/stable/installation.html\nconda install -c conda-forge  lockfile\nconda install -c conda-forge  six\npip install  scikit-learn==0.18.1\n\n\n#####SMAC install  : Bayesian OPtimization  #################\nhttps://automl.github.io/SMAC3/stable/manual.html\n\n\n\n\n\n\n\n######Tensor Layer in Editable mode   ##################################################################################\npip install -e /path/to/package --no-use-wheel\n\n\nEditable Mode\nDownload the TensorLayer folder from Github.\nBefore editing the TensorLayer .py file.\nIf your script and TensorLayer folder are in the same folder, when you edit the .py inside TensorLayer folder, your script can access the new features.\nIf your script and TensorLayer folder are not in the same folder, you need to run the following command in the folder contains setup.py before you edit .py inside TensorLayer folder.\npip install -e .\n\n\n\n\n\n#### Create Temp environnment for dependencies management  #####################################\nconda install conda-execute --channel=conda-forge\n\nhttp://www.zib.de/miltenberger/ICMS_2016_PySCIPOpt.slides.html#/5\n\n\n\n####### Regression test\nsource activate sandbox\npython   /home/ubuntu/project27/ztest/test_regression_py2.py --title kevin\n\n\n\n#### conda environnment  ######################################################################\nconda info --envs  \n   root   *  /home/noel/anaconda2\n\n\n#Clone previous envs\nconda create --name tensorflow2  --clone root\n\n\n#### Activate\n source activate tensorflow2\n source deactivate tensorflow\n source activate root\n\n\n####   BUGGY be careful\nconda list --revisions\nconda install --revision   63    ###Roll Back\n\n\n\n#### Remove\nsource activate root\nconda remove --prefix tensorflow  --all\nrm -rf /home/noel/anaconda2/envs/tensorflow\n\n\nIf issues delete MANUALLY in anaconda/Pkgs\n\n\n\n\n\n\n\n####################################################################################################\n#### TensorFlow Install  ###########################################################################\nconda info --envs     #  root   *  /home/noel/anaconda2\n\nconda create --name tf_gp13  --clone tf_gp12\n\n#Clone previous envs\nconda create --name tensorflow2  --clone root\n\n\n#### Activate\nsource activate tensorflow2\nsource deactivate tensorflow\n\n\n#### \nconda list --revisions\nconda install --revision   63    ###Roll Back\n\n\n\n#### Remove\nsource activate root\nconda remove --prefix tensorflow  --all\nrm -rf /home/noel/anaconda2/envs/tensorflow\n\n\nIf issues delete MANUALLY in anaconda/Pkgs\n\n\n######  TensorFlow \nactivate tensorflow\n\n\n### Grid LSTM\nconda install  -c jjhelmus tensorflow-gpu=1.0.1\n\nconda install  --force  -c jjhelmus tensorflow-gpu=1.0.1\nconda install --force\n\n### Install from google Binaries  BIG ISSUES\n###=  Issues with PIP Never installed\n###  pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl\n\n\n#### Disable CUDA  ######################################################################\nCUDA_VISIBLE_DEVICES=\"\"\n\n\n\n#############Tensor   Flow   ############################################################\nCPU Optimized : default one\nGPU Optimized:   source activate tf_gpu_12\nCPU Optimized:  tf_cpu_12\nsource activate tf_gpu_12\n\n\n#### Install specific version of GPU  BUILD ##############################################\nconda install -c jjh_cio_testing   tensorflow-gpu=1.2.1=py36cuda8.0cudnn5.1_0\n\n\n##### Test Code \nimport tensorflow as tf; print(tf); hello = tf.constant('Hello, TensorFlow!');  sess = tf.Session();  print(sess.run(hello))\n\n\n\n########### via PIP specific version #####################################################\n### pip DOES NOT work always use conda\n## pip install tensorflow-gpu==1.3.0\nconda create --name tf_gpu_13\nconda install --yes -c conda-forge pip\nconda install --yes -c conda-forge tensorflow==1.3.0\n\n\n\n\n\n\n\n######## Deep Learning training  #########################################################\npython   /home/noel/jupyter/project/grid_lstm/train.py  --model  gridlstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_gridlstm\n\npython   /home/noel/jupyter/project/grid_lstm/train.py  --num_epochs 70   --num_layers 4   --model  gridlstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_gridlstm_4\n\npython   /home/noel/jupyter/project/grid_lstm/train.py  --num_epochs 70   --num_layers 4   --model  lstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_lstm_4\n\npython   /home/noel/jupyter/project/grid_lstm/train.py  --num_epochs 1   --num_layers 1   --model  gridlstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_gridlstm_4\n\n\n\nNamespace(batch_size=50, data_dir='/home/noel/jupyter/project/grid_lstm/data/tinyshakespeare/', decay_rate=0.97, grad_clip=5.0, learning_rate=0.002\n, model='gridlstm', num_epochs=50, num_layers=2, rnn_size=128, save_dir='/home/noel/jupyter/project/grid_lstm/save_gridlstm', save_every=1000, seq_\nlength=50)\n\n##########################################################################################\n\n\n\n\n\n\n\n\n##########################################################################################\n########  TensorFlow with Cuda packages ##################################################\n\n# This is shorthened version of blog post \n#  http://ksopyla.com/2017/02/tensorflow-gpu-virtualenv-python3/\n\n# update packages\nsudo apt-get update\nsudo apt-get upgrade\n\n#Add the ppa repo for NVIDIA graphics driver\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt-get update\n\n#Install the recommended driver (currently nvidia-378)\nsudo ubuntu-drivers autoinstall\nsudo reboot\n\n#check if drivers were installed\nnvidia-smi\n\n################ Instal CUDA Toolkit 8.0 for x64 Ubuntu 16.04 \nwget -O cuda_8_linux.run https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run\nsudo chmod +x cuda_8_linux.run\n./cuda_8.0.61_375.26_linux.run\n\n./cuda_8_linux.run\n\n\n#Do you accept the previously read EULA?\n#accept\n#Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 367.48?\n#n (we installed drivers previously)\n#Install the CUDA 8.0 Toolkit?\n#y\n#Enter Toolkit Location:\n#/usr/local/cuda-8.0 (enter)\n#Do you wish to run the installation with ‚sudo’?\n#y\n#Do you want to install a symbolic link at /usr/local/cuda?\n#y \n#Install the CUDA 8.0 Samples?\n#y \n#Enter CUDA Samples Location:\n#enter \n\n# Install cuDNN\n# go to website and download cudnn-8 https://developer.nvidia.com/cudnn\ntar -zxvf cudnn-8.0-linux-x64-v5.1.tgz \n\n# copy libs to /usr/local/cuda folder\nsudo cp -P cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp -P cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n\n# isntall python 3 and virtual env\nsudo apt install python3-pip\nsudo apt install python3-venv\n\n# create virtual environment for tensorflow\npython3 -m venv tfenv\nsource tfenv/bin/activate\n\n# Instal tensorflow package with gpu support\n(tfenv)$ pip install tensorflow-gpu\n\n\n#or CPU version\n(tfenv)$ pip install tensorflow\n\n\n# check installation, run simple python scipt from console\n$ python\n\nimport tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\ntf_session = tf.Session()\nx = tf.constant(1)\ny = tf.constant(1)\nprint(tf_session.run(x + y))\n\n\n\nfacundoq commented 4 days ago\nI think you should add that CUDA_HOME should be set and LD_LIBRARY_PATH modified for TF to find the libraries.\n\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\nexport CUDA_HOME=/usr/local/cuda\nYou can add those lines to ~/.bashrc so that they are executed each time you open as shell.\n\n\n##### parition\nsudo pvcreate /dev/sda3  # to make the partition available for LVM\n\n\nhttp://downloads.sourceforge.net/gparted/gparted-live-0.29.0-1-amd64.iso\n\n\n\n\n\n\n#### Anaconda Python 2 and Python 3 version #####################################################################################\nI also despise the virtual environment switch that Anaconda tries to force on us.\n I prefer to have both executables always instantly available from the command line. \nI'm pretty sure I had this working on a Windows machine once:\n\nInstall Anaconda2 and Anaconda3 to the C:\\ drive as \"C:\\Anaconda2\\\" and \"C:\\Anaconda3\\\" respectively.\nEdit your \"Path\" environment variable (Control Panel -> System and Security -> System -> Advanced system settings -\n> Environment Variables) and make sure that \"C:\\Anaconda2;C:\\Anaconda2\\Scripts;C:\\Anaconda2\\Library\\bin\" is in front of \"C:\\Anaconda3;C:\\Anaconda3\\Scripts;C:\\Anaconda3\\Library\\bin\".\nCopy and rename the file \"C:\\Anaconda3\\python.exe\" to \"C:\\Anaconda3\\python3.exe\".\nCopy and rename the file \"C:\\Anaconda3\\Scripts\\conda.exe\" to \"C:\\Anaconda3\\Scripts\\conda3.exe\"\nCopy and rename any other scripts you might use in \"C:\\Anaconda3\\Scripts\\\", such as \"pip.exe\" to \"pip3.exe\", etc.\nNow, when you type \"python\" or \"conda\" at the command line you will get the python2 version, and when you type \"python3\" or \"conda3\", etc. at the command line you will get the python3 version.\n\n\n##############  Python 2 and 3 compatibility\nhttp://python-future.org/compatible_idioms.html\n\n\n\n\n\n#################################################################################################\n####Spyder SSH     ##############################################################################\n## Type   %connect_info   in Jupyter to get the SSH\n{\n  \"stdin_port\": 39893, \n  \"ip\": \"127.0.0.1\", \n  \"control_port\": 58886, \n  \"hb_port\": 56808, \n  \"signature_scheme\": \"hmac-sha256\", \n  \"key\": \"d4aff2c9-ae8e-484a-a133-285f3961b53f\", \n  \"kernel_name\": \"\", \n  \"shell_port\": 38804, \n  \"transport\": \"tcp\", \n  \"iopub_port\": 37865\n}\n\n\n\n#####################################################################################################\nAmazon Costs\nEC2-Other :  0.15 USD / Day  due to SnapShot Maintaining cost\n    0.05 per GB-month of data stored   12Go :  0.80 USD / Month\n\n    EC2 - EBS Snapshot (Cost for storage of snapshots)\n    EC2 ? CloudWatch (cost for detailed monitoring, custom-metrics, and API access over the free tier (1M requests)).\n    EC2 - Elastic IP - Idle IPs, Additional IPs, IP Remaps.\n\n\n\n\n#####################################################################################################\n####VMare, local linux virtual machine  #############################################################\n3 interpreters\n\n/usr/bin/python3.5\n\n/home/noel/anaconda27/bin\n\npass: sophie237\n\ndeeplearning\ndeeplearning\n\n\nLXDE\n Ctl+Alt+F1 and login as root to access Shell at login\n\n\nsophie37\n\n\n\n########Local Virtual Machine VMaware ###################################################################\n####  Resize VM ware partition to bigger size.  #########################################################\nand download gparted iso file\n\nchange .vmx file to add\n\nF2 at start of VM\nchange start to CD ROM\n\n\nin Gparted\ndelete SWAP linux partition\nMerge of the partition\nRecreate\nSwap linux parition at end\n\nRestart\n\n\nUsing Gparted from a Live CD:\n\nSelect the swap partition\nIn the Partition menu, click on \"swapoff\"\nDelete the swap partition\nRecreate swap at the end of Unallocated (faster than moving it)\n\nOptional: Turn swap back on\nResize sda5 as desired\n\n\n### Virutal box\nsophie237\n\n\n###############################################################################################\n###############################################################################################\n\n\n\n\n\n\n\n\n\n###############################################################################################\n######   Spyder Shorcut\nG:\\_devs\\Python01\\Anaconda27\\pythonw.exe \"D:/_devs/Python01/project27/zspyder_session/spyder_launch.py\"    \"D:\\_devs\\project27\\unerry_devs.session.tar\"\n\nD:\\_devs\\project27\\\n\n\n####### Linux Screen attachment #################################################################\nscreen -r 25555                  #### Attach session\n\nscreen -X -S   26655  quit       #### Kill session\n\nscreen       ### New session\n\nCtrl+A+D    ##Quite sesssion\n\n\n\n#### File Editing\nexport VISUAL=nano\n\ncrontab -e\n\ngrep bbiz   /var/log/syslog\n\n\n\n/home/noel/project27/scheduler/cmd_line/bbiz_run.sh\n\n\n\n\ntar -zcvf /home/noel/archive_vm_homenoel.tar.gz    /home/noel/\n\n\n\nchmod +x bbiz_run.sh \nsudo ln -s /home/noel/project27/scheduler/cmd_line/bbiz_run.sh   /usr/local/bin/\n\n\n\nchmod +x /usr/local/bin/bbiz_run.sh \n\n\n\n#### Kuraya Message    ##################################################################################################\n#########################################################################################################################\n\n1) Change Under noel User\n\n\n2) To start server :\n\nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   runserver 0.0.0.0:80    &>> /home/noel/project27/log/logfile_bbiz_server.txt\n\n\nsudo -E  /home/noel/bbiz_server_run\n\n\n\n\n3) To re-start server:\n   Restart the VM\n\n\n\n\npyminifier -o   \"D:\\_devs\\google_cloud\\home\\noel\\project27\\geoapp\\geoproject\\django_app\\view_mini.py\"   --obfuscate    \"D:/_devs/google_cloud/home/noel/project27/geoapp/geoproject/django_app/view.py\" \n\n\n\n\nsudo -E /home/noel/anaconda2/bin/python　-OO -m py_compile  /home/noel/project27/scheduler/bbiz_batch_check.py 　　\n\n\n\n\n\nsudo -E /home/noel/anaconda2/bin/python　 -m compileall  -l  /home/noel/project27/scheduler/bbiz_batch_check.py 　\n\n\n\n5) 後で\nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_check.pyc --action status\n\ncheck_bbiz_batch.txt  をおねがいします。\n\n\n\n\n\n\n#####################################################################################################################################\nこのアクシオンをおねがいします\n\n1) Check File を入れて\n　　　/home/noel/project27/scheduler/\n\n\n2) Run\nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_check.pyc --action reset_all_cache_becareful\n\nsudo -E  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py\n\n\n3)  Server VM restart\n\n\n4) 後で\nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_check.pyc --action status\n\n/home/noel/project27/scheduler/check_bbiz_batch.txt  をおねがいします。\n\n\n\n\nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_check.pyc --action restore_1day\n\n\n\nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_check.pyc --action backup\n\n\n\n\n\n\n\n\n#########################################################################################################   \n####Launch Batch      ###################################################################################\n###Test if the batch is working:\nsudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py   --exit  25  &>> /home/noel/project27/scheduler/logfile_bbiz_batch.txt\n\n\n\n##### Crontab Batch\ncrontab -e\n\n0 16 * * * sudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py --project_id \"test-beaconbank-biz\"   &>> /home/noel/project27/scheduler/logfile_bbiz_batch.txt\n\n\n\"test-beaconbank-biz\"\n\n\nsudo -E /home/noel/biz_server/server/python  /home/noel/biz_server/launch/manage.py   runserver 0.0.0.0:80  \n\n\n\n\nln -s /home/noel/anaconda2/bin   \n\nln -s /home/noel/project27/geoapp/geoproject  /home/noel/biz_server/project/project27\n\nln -s  /home/noel/project27/data/count_table/    /home/noel/biz_server/data\n\nln -s  /home/noel/project27/    /home/noel/biz_server/details\n\n\n\nsudo -E  /home/noel/bbiz_server_run\n\ndos2unix /home/noel/bbiz_server_run\n\n\n#########################################################################################################################\n##### Prod Server\n  timestamp server\n  whitenoise is activated ---> Static Files must be updated by collectstatic\n\n\n1)  Modify the HTML and .js script files\n    fusioncharts   --->  kevinisthebest\n\n\n2)   Move files to whitenoise folder\n         sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   collectstatic\n\n\n\n####Launch Batch      ##################################################################################################\n###Test if the batch is working:\nsudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py  --project_id \"test-beaconbank-biz\" --exit  25\n\n\ncrontab -e\n\n0 16 * * * sudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py --project_id \"test-beaconbank-biz\"   &>> /home/noel/project27/scheduler/logfile_bbiz_batch.txt\n\n\n\n###  Test Pages\nCV page with empty values  and no empy values\n\nCV\nhttps://t-biz.beaconbank.jp/apps/2170005/beacon_groups/5370001?duration=last_365days\n\nConversion page \n\n\n\n####Launch Batch      ########################################################################\n\n###Test if the batch is working:\nsudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py  --project_id \"test-beaconbank-biz\" --exit  25\n\n\ncrontab -e\n\n0 16 * * * sudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py --project_id \"test-beaconbank-biz\"   &>> /home/noel/project27/scheduler/logfile_bbiz_batch.txt\n\n\n###########################################################################################################\n\n\n\n\n\n#### DEBUG Mode\n DJANGo_DEBUG=1 sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   runserver 0.0.0.0:8089\n\n### No Debug Mode\n sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   runserver 0.0.0.0:8000\n\n\n\n###### Launch BBiz Server\n###  COMPRESS=5 activate  JS Compression\n screen \n COMPRESS=5  sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   runserver 0.0.0.0:80    &>> /home/noel/project27/log/logfile_bbiz_server.txt\n\n\n##### Check Email\nsudo tail -f /var/mail/noel\n\n\n\n\nhttp://104.198.117.174:8889/notebooks/jupyter/Untitled.ipynb?kernel_name=python2#\n\n\n\n\n##################Batch Crontab ######################################################################\n#####################\nhttp://blog.appliedinformaticsinc.com/managing-cron-jobs-with-python-crontab/\n!!!!!  Crontab uses root login to access : Python Folder will be changed\n\n\n##### Setup in Server with Command Line\nipython\n\nfrom crontab import CronTab; cron = CronTab('noel');\njob  = cron.new(command='sudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py   &>> /home/noel/project27/scheduler/logfile_bbiz_batch.txt', comment='bbiz_01')\njob.hour.on(15);  job.enable()\ncron.write()\n\nfor job in cron: print job\n\n\n#### Check Crontab Log \ngrep python /var/log/syslog\n\n\n###  Remove specific cron job(s)\nOpen the cron file and delete the cron(s) that you want to remove\ncrontab -e\neach line represent a cron job. You can remove any cron by using ctrl+k then save and exit\nshift + Z : save exit\n\n\n\n\n\n\n###################################################################################################################\n######## Command For Django Management ############################################################################\nsudo /home/noel/anaconda2/bin/python     /home/noel/project27/scheduler/cmd_line/django_service.py  restart\n\n\n\nto run django application go to folder cmd_line and write sudo ./bash_cmd/django_run.sh\n\n\n##Launch Django\nsudo /home/noel/project27/scheduler/cmd_line/django_run.sh\n\n\n\nsudo /home/noel/anaconda2/bin/python   /home/noel/project27/scheduler/cmd_line/django_service.py restart\n\nsudo /home/noel/anaconda2/bin/python   /home/noel/project27/scheduler/cmd_line/django_service.py stop\n\n\n\n\n\n#### Bash  move script  to /usr/bin    #################################################################\nsudo cp /home/noel/project27/scheduler/cmd_line/bash_cmd/*        /usr/bin/\n\n\n\nsudo  /home/noel/anaconda2/bin/python  /home/noel/project27/scheduler/bbiz_batch_bquery.py   &>> /home/noel/project27/scheduler/logfile_bbiz_batch.txt\n\n\n\n\n##### Crontab -e : working in Jupytere\n0 15 * * * sudo /home/noel/anaconda2/bin/python /home/noel/project27/scheduler/bbiz_batch_bquery.py   &>> /home/noel/project27/scheduler/logfile_bbiz_batch.txt # bbiz_01\n\n0 4 * * * sudo /home/noel/anaconda2/bin/python /home/noel/project27/scheduler/cmd_line/django_service.py restart # restart-django-service\n0 5 * * * sudo /home/noel/anaconda2/bin/python /home/noel/project27/scheduler/cmd_line/django_check.py check # check-django-service\n\n\n\n\n### Scehduler\nfor d in cron.log: print d[‘pid’] + ” – ” + d[‘date’]\n\n\nfor job in cron: cron.remove( job )\n\n\nhttps://stackoverflow.com/questions/26835235/google-cloud-compute-vm-instances\n\n\n\n#####Job Scheduler\nhttp://blog.appliedinformaticsinc.com/managing-cron-jobs-with-python-crontab/\n\n\n\n\n####### Copy VM : Don't forget to open put the \nbiz-analytics\nhttp-server, http-servers, https-server\n104.198.94.24 \nSSH\n\n\n\n\n##### Identification:\n'LOGNAME': 'noel', 'USER', 'linux2'   \n\nPCLOC=  'win,asus1'  if os.path.expanduser('~').find('asus1') >-1  and sys.platform.find('lin')>-1 else  'win,unerry' if sys.platform.find('win')> -1 else  'lin,gcloud' if os.environ['HOME'].find('noel')>-1  else 'lin, virtualbox'\nDIRCWD= if PCLOC=='win,asus' else   'G:/_devs/project27/' if PCLOC=='win,unerry'  else  '/home/noel/' if PCLOC=='lin,gcloud' else '/media/sf_projec27'\n\n\nHOME= 'D:/_devs/Python01/project27/'  if os.path.expanduser('~').find('asus1') >-1  and sys.platform.find('win')>-1 else  'G:/_devs/google_cloud/home/noel/project27/' if sys.platform.find('win')> -1 else  '/home/noel/project27/' if os.environ['HOME'].find('noel')>-1  else '/media/sf_projec27'\n\n\n\n\n\n###################################################################################################\n######  Cloud SQL #################################################################################\nhttps://cloud.google.com/sql/docs/mysql/connect-admin-proxy\n\n# At first, type the following command to run cloud_sql_proxy on background.\n# ./cloud_sql_proxy -instances=test-beaconbank-biz:asia-northeast1:biz-db01=tcp:3306 &\n\n# And, you can connect to SQL Server with the following.\n# mysql -u bizbb -p --host 127.0.0.1\n\n# project=413736133783 \n\n\n####### Install in Linux Proxy :   ##############################################################\nsudo wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64\nsudo mv cloud_sql_proxy.linux.amd64 cloud_sql_proxy\nsudo chmod +x cloud_sql_proxy\n\n#In bash Command line\nsudo ./cloud_sql_proxy -instances=test-beaconbank-biz:asia-northeast1:biz-db01=tcp:3306 &\n\nmysql -u bizbb -p KPhgZtW7sUpX --host 127.0.0.1\n\n\nlogin/password\n\n\n\n\n##### USage in Windows platform    ############################################################\n0) https://cloud.google.com/sql/docs/mysql/connect-admin-proxy\n\n1) Download Win Client: https://dl.google.com/cloudsql/cloud_sql_proxy_x64.exe a\n\n2) cd G:\\_devs\\google_cloud\\_cloud_sql\\cloud_sql_proxy_x64.exe\n   cloud_sql_proxy_x64 -instances=test-beaconbank-biz:asia-northeast1:biz-db01=tcp:3306 &\n\n\n####Start Client session:        \nmysql -u <USERNAME> -p --host 127.0.0.1\n\nmysql -u bizbb -p KPhgZtW7sUpX --host 127.0.0.1\n\n\n\n\n\n##################################################################################################\n##################################################################################################\nhttps://unerry.docbase.io/posts/215426?list=%2F&q=\n\nInfra Docbase\nelise237\n\nTEST環境情報\n\nプロジェクトID: test-beaconbank-biz\nドメイン: t-biz.beaconbank.jp\n\nサービスアカウント用 JSON ファイル\n Key_JSON_for_GCE_test-biz\n\nLB: 130.211.24.62\nWEB01(Redis): エフェメラルIP(可変)\nWEB02: エフェメラルIP(可変)\n\nDBユーザ: beaconbank\nDBパスワード: 8gRs1T4H\n\nCORE_DBユーザ: bizbb\nCORE_DBパスワード: KPhgZtW7sUpX\n\nログの退避先: gs://test-biz-documents\n※サーバ内部ではlogrotateさせていて、\n※cronで毎朝2時にnginx, application, redisのログをCloudStorageに投げるようになっています。\n\n※Cloud SQLに接続方法はsystemdに登録されたCloud SQL Proxyを使用しています。\n　各インスタンス内からmysql -u [USER] -p -h 127.0.0.1で接続できます。\n\n※GCPは同N/W上であればインスタンス名で接続できるので、redisはインスタンス名をhostとして接続する\n##################################################################################################\n##################################################################################################\n\n\n\n\n#####Schema of database #########################################################################\nss= '''\nSELECT table_name, column_name    FROM information_schema.columns \nWHERE table_name  in ( \n    SELECT table_name \n    FROM information_schema.tables \n    WHERE table_type = 'BASE TABLE'  AND table_schema NOT IN  ('pg_catalog', 'information_schema')\n); \n'''\n\ndf= pd.read_sql(ss, engine2)\n\n\n\n\n###### Cloud SQL Connect #######################################################################\nengine2= sql.create_engine('mysql+mysqlconnector://bizbb:KPhgZtW7sUpX@104.198.113.129:3306/information_schema')\n\n\n\n###### PhP MyAdmin #############################################################################\nengine_2= sql.create_engine('mysql+mysqlconnector://root:atbms_4649@133.242.234.94:3306/atbms')\n\n\n\n\n\n\n\n\n###############################################################################################\n2017-04\n\nhttps://t-biz-analytics.beaconbank.jp/page_bi/a011/?pagename=a011&account_id=95651&application_id=5563&startdate=20170522&enddate=20170528&uurid=9ac9b780b81e4cf7b73fcef482e15db353e535d959266eb488f6403fcd9c053030a819397c10fd8304273d76b6a8c0fa\n\n\n\n\n###############################################################################################\n##############################################################################################\ndb = MySQLdb.connect( host='127.0.0.1', user=CLOUDSQL_USER, passwd=CLOUDSQL_PASSWORD)\n\nbizbb\nbiz_analytcis\n\n\n\n#############################################################################################\nfrom sqlalchemy import create_engine\nengine = create_engine('mysql+pymysql://USER:PASSWORD@127.0.0.1/DATABASE')\n\n\n\n\n\n\n\n###########################################################################################\n######  TensorFlow \nactivate tensorflow\n\n\n### Grid LSTM\nconda install  -c jjhelmus tensorflow-gpu=1.0.1\n\nconda install  --force  -c jjhelmus tensorflow-gpu=1.0.1\nconda install --force\n\n### Install from google Binaries\n###=  Issues with PIP Never installed\n###  pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl\n\n\n\n# Python\nimport tensorflow as tf; print(tf)\nhello = tf.constant('Hello, TensorFlow!')\nsess = tf.Session()\nprint(sess.run(hello))\n\n\n###### Jupyter\nsource activate tensorflow2\njupyter notebook --ip=0.0.0.0 --port=8888 --no-browser &\n\n\n### to see the token and URL of jupyter notebook\njupyter notebook list\n\n\n\n## Type   %connect_info   in Jupyter to get the SSH\n{\n  \"stdin_port\": 39893, \n  \"ip\": \"127.0.0.1\", \n  \"control_port\": 58886, \n  \"hb_port\": 56808, \n  \"signature_scheme\": \"hmac-sha256\", \n  \"key\": \"d4aff2c9-ae8e-484a-a133-285f3961b53f\", \n  \"kernel_name\": \"\", \n  \"shell_port\": 38804, \n  \"transport\": \"tcp\", \n  \"iopub_port\": 37865\n}\n\n\n#### Disable CUDA\nCUDA_VISIBLE_DEVICES=\"\"\n\n\n\n\n######## Deep Learning training\npython   /home/noel/jupyter/project/grid_lstm/train.py  --model  gridlstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_gridlstm\n\n\npython   /home/noel/jupyter/project/grid_lstm/train.py  --num_epochs 70   --num_layers 4   --model  gridlstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_gridlstm_4\n\n\npython   /home/noel/jupyter/project/grid_lstm/train.py  --num_epochs 70   --num_layers 4   --model  lstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_lstm_4\n\n\npython   /home/noel/jupyter/project/grid_lstm/train.py  --num_epochs 1   --num_layers 1   --model  gridlstm  --save_dir  /home/noel/jupyter/project/grid_lstm/save_gridlstm_4\n\n\n\nNamespace(batch_size=50, data_dir='/home/noel/jupyter/project/grid_lstm/data/tinyshakespeare/', decay_rate=0.97, grad_clip=5.0, learning_rate=0.002\n, model='gridlstm', num_epochs=50, num_layers=2, rnn_size=128, save_dir='/home/noel/jupyter/project/grid_lstm/save_gridlstm', save_every=1000, seq_\nlength=50)\n\n###########################################################################################\n###########################################################################################\n\n\n\n\n###########################################################################################\n###########################################################################################\n\n\n\n\n\n\n\n\n\n\n\n##### Test Values / URL:\n\n##### Lauchn test server:\n###  http://104.198.94.24:85/p\n\n\n\n\n##### Connected to Big Query    ##################################\ndfstat:\n\n\n\ndfstat_haishin:  haishin_view\n\n\n\n####  dfstat_homon:\nn_haishin              int64\nn_haishin_user         int64\nn_contentview          int64\nn_contentview_user     int64\nn_yanai                int64\nn_yagai                int64\nn_ya_fumei             int64\nn_koukai               int64\nn_hikoukai             int64\nn_public_fumei         int64\nn_hanoulog             int64\nn_homon                int64\n\n\n\n\n##### dfstat_conv:   # Cv1 ---> CV2  ############################\ndate           object\napp_id         int64\ndatestr        object\ndatestr2       object\naccount_id     int64\ncvgroup1       object\ncvgroup2       object\ncv_pct1        int32\ncv_pct2        int32\n\n\n\n\n###############################################################\nid\t        INTEGER\t\t\ngroup_id\tINTEGER\tNULLABLE\t\nbeacon_id\tINTEGER\tNULLABLE\t\nuse_from\tDATE\tNULLABLE\t\nuse_to\tDATE\tNULLABLE\t\ndau_price\tINTEGER\tNULLABLE\t\ndaily_price\tINTEGER\tNULLABLE\t\nprice_type\tINTEGER\tNULLABLE\t\ndeleted\tINTEGER\tNULLABLE\t\n\nbeacon_prices\n\n\n\n\n\n\n##################################################################################################################################\n##################################################################################################################################\n\nA01-1_管理ビーコン一覧.jpg\n/page_bi/a011/?pagename=a011&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\nA04-2_個別ビーコンプッシュ情報1.jpg\n/page_bi/a042_jouhou1/?pagename=a042_jouhou1&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\nA06-1_分析3.jpg\n/page_bi/a061_bunseki3/?pagename=a061_bunseki3&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\nA06-1_分析1.jpg \n/page_bi/a061_bunseki1/?pagename=a061_bunseki1&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\n\nA06-1_分析2.jpg\n/page_bi/a061_bunseki2/?pagename=a061_bunseki2&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\n\na042_jouhou3.html\nA04-2_個別ビーコンプッシュ情報3.jpg\n/page_bi/a042_jouhou3/?pagename=a042_jouhou3&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\n\na027_area.html\nA02-7_個別ビーコングループ情報1＜エリア＞.jpg\n/page_bi/a027_area/?pagename=a027_area&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\na027_cv.html\nA02-7_個別ビーコングループ情報1＜CV＞.jpg \n/page_bi/a027_cv/?pagename=a027_cv&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n\n\n\na027_cva.html\nA02-7_個別ビーコングループ情報1＜CVA＞.jpg  \n/page_bi/a027_cva/?pagename=a027_cva&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=\n##################################################################################################################################\n##################################################################################################################################\n\n\n\n\n\n\n##################################################################################################################################\n##################################################################################################################################\n\n### PROD Static Folder  , but, cannot write access to static_root/gg  ---\n---> Need to put in other folders\n### Folder used for Compress files  django comrpress\nProd Static:    /home/noel/project27/geoapp/static_root\nDjango Compress :      /home/noel/project27/geoapp/static_root/cache\nDev Static :  home\\project27\\geoapp\\geoproject\\static\\gg\\\n\nDjango Cache  : (Django managed cached): home\\project27\\jango_cache\nKevin Cache  : (Django managed cached): home\\project27\\cache\n\nKevin Data  : (Django managed cached): home\\project27\\data/\n\n\n\n\n#####Enable DEBUG Mode, Compress mode\nDEBUG=1  COMPRESS=5  sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   runserver 0.0.0.0:8000 \n\n\nIssue with Compressor:  Need to laucnh 2 times the server\n\n\n\n#### STATIC_ROOT is useless during development, it's only required for deployment.\n\nWhile in development, STATIC_ROOT does nothing. You even don't need to set it. Django looks for static files inside each app's directory (myProject/appName/static) and serves them automatically.\nThis is the magic done by manage.py runserver when DEBUG=True.\n\n#### Deployment\n\nWhen your project goes live, things differ. Most likely you will serve dynamic content using Django and static files will be served by Nginx. Why? Because Nginx is incredibly efficient and will reduce the workload off Django.\nThis is where STATIC_ROOT becomes handy, as Nginx doesn't know anything about your django project and doesn't know where to find static files.\nSo you set STATIC_ROOT = '/some/folder/' and tell Nginx to look for static files in /some/folder/. Then you run manage.py collectstatic and Django will copy static files from all the apps you have to /some/folder/.\n\n### Extra directories for static files\n\nSTATICFILES_DIRS is used to include additional directories for collectstatic to look for. For example, by default, Django doesn't recognize /myProject/static/. So you can include it yourself.\nExample\nSTATIC_URL = '/static/'\n\nif not DEBUG: \n    STATIC_ROOT = '/home/django/www-data/site.com/static/'\n\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, 'static/'),\n]\n\n\n\n\n\n<h2>行動アナリティクス</h2>\n\n.....\n\n\n<div style=\"height: 1200px; width=100%; min-width: 1024px;\"> \n\n<iframe  >\n\n</div>\n\n\n\npagename=a061_bunseki3\n\n\n:\nこんにちは、\nおすすめは　ありがとうございました。\nしかし、PowerBIは　JS restrictionがあったら、\n<DIV > tag ほうがいいとおもいます\n\n\nPageLayoutは　Design 設定ですから、　変わらない,\n\n\nこのペ－ジのテストしました: \n\n<div style=\"height: 1200px; width=100%; min-width: 1024px;\"> \n\n<iframe    style=\"width: 100%;min-width: 1024px;height: 100%;\" scrolling=\"no\" seamless=\"seamless\" frameborder=\"0\">\n\n</div>\n\n下にテストがみられます:\nsimpleですから、調整しやすいです.\nよろしくお願いします\n\n\n\n<div style=\"width:80%; height:100%; margin:0 auto; float: left;  min-width: 1024px;\">\n\n  <iframe style=\"position:absolute;top:0;left:0;width:100%; height:100%;\"\n</div>\n\n\n\nhtml,body        {height:100%;}\n.wrapper         {width:80%;height:100%;margin:0 auto;background:#CCC}\n.h_iframe        {position:relative;}\n.h_iframe .ratio {display:block;width:100%;height:auto;}\n.h_iframe iframe {position:absolute;top:0;left:0;width:100%; height:100%;}\n\n\n\n\nhttp://apps.socib.es/Leaflet.TimeDimension/examples/example12.html\n\n\n######################################\n\n\n\n\n別のテストをしました\nSimple <div> の　tag は　layoutができました :\n\n<div style=\"text-align: left;height: 500px;float: left;\">\n  \n  <iframe ></iframe>\n  \n</div>\n\nPageLayoutが Deignから、決められました,\n変わりません.\n\n\n\n\n##########################################################################################\n<div class=\"intrinsic-container\">\n  <iframe src=\"allowfullscreen\"></iframe>\n</div>\n\n\n\n<style>\n.intrinsic-container {\n  position: relative;\n  height: 0;\n  overflow: hidden;\n}\n \n/* 16x9 Aspect Ratio */\n.intrinsic-container-16x9 {\n  padding-bottom: 56.25%;\n}\n \n/* 4x3 Aspect Ratio */\n.intrinsic-container-4x3 {\n  padding-bottom: 75%;\n}\n \n.intrinsic-container iframe {\n  position: absolute;\n  top:0;\n  left: 0;\n  width: 100%;\n  height: 100%;\n}\n</style>\n\n##########################################################################################\nIntgのアカウント test5@example.com pass: testtest に変わりました。よろしくお願いします。\n\n\n\n\n\n\n\n\n\n\n\n############################################################\nvar $iframes = $( \"iframe\" );\n\n$iframes.each(function () {\n  $( this ).data( \"ratio\", this.height / this.width )\n    .removeAttr( \"width\" )\n    .removeAttr( \"height\" );\n});\n \n\n$( window ).resize( function () {\n  $iframes.each( function() {\n    var width = $( this ).parent().width();\n    var height = $( this ).parent().height();\n    $( this ).width( width )\n      .height( height );\n  });\n\n}).resize();\n\n\n\n\n\n\n\n\n\n\n\n\n\n##########################################################################################\nhttp://104.198.117.174:8000/page_bi/a061_bunseki3/\n?pagename=a061_bunseki3&account_id=95651&application_id=5563&startdate=20170505&enddate=20170511&uurid=e99b97733e506c2a241407107d7b680af4c03d6e4d8536091881654e010193270ccc7e0a990bd4c83eb9b5b99e9f05d0\n\n\n\n<div style=\"height: 1200px; width=100%; min-width: 1024px;\"> \n<iframe src=\"http://104.198.117.174:8000/page_bi/a061_bunseki3/?pagename=a061_bunseki3&account_id=95651&application_id=5563&startdate=20170505&enddate=20170511&uurid=e99b97733e506c2a241407107d7b680af4c03d6e4d8536091881654e010193270ccc7e0a990bd4c83eb9b5b99e9f05d0\" style=\"width: 100%;min-width: 1024px;height: 100%;\" scrolling=\"no\" seamless=\"seamless\" frameborder=\"0\"></iframe>\n</div>\n\n\n\n\n<div class=\"container\">\n  <iframe src=\"//www.youtube.com/embed/KMYrIi_Mt8A\" allowfullscreen></iframe>\n</div>\n\n\n\n\n\n\n\n#############################################################################################\n###  https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs\nOpen SSL certificates\n\nCountry Name (2 letter code) [AU]:US\nState or Province Name (full name) [Some-State]:New York\nLocality Name (eg, city) []:Brooklyn\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Example Brooklyn Company\nOrganizational Unit Name (eg, section) []:Technology Division\nCommon Name (e.g. server FQDN or YOUR name) []:examplebrooklyn.com\nEmail Address []:\n\n\n\n\n###### URL map details   ############################################\nhttps://cloud.google.com/compute/docs/load-balancing/http/url-map\n\n#####  Load Balancer,  1 USD,  0.025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##########################################################################\nHello,\n\nBI Pages have been published since last week.\n\nPages can be checked  from the website directly, \nhere below urls :\n\nTexts have been added and basic layout have been added.\nThis is possible that some adjustments might be needed,\nin that case, could you kindly send summary by email this week ?\n(since it takes T+3-4days to adjust the pages, depending on schedule)\n\nPages urls are here :\n(Login: id: test1@test.com, pass: testtest)\n\nA01-1_管理ビーコン一覧\t\nhttp://13.112.171.13/apps/1/beacon_groups\n\nA06-1_分析3\t\nhttp://13.112.171.13/apps/1/analytics/enduser\n\nA06-1_分析1\t\nhttp://13.112.171.13/apps/1/analytics\n\nA04-1_ビーコンプッシュ一覧\t\nhttp://13.112.171.13/apps/1/beacon_pushes\n\nA04-2_個別ビーコンプッシュ情報1\t\nhttp://13.112.171.13/apps/1/beacon_pushes/2\n\nA04-2_個別ビーコンプッシュ情報3\t\nhttp://13.112.171.13/apps/1/beacon_pushes/2/evaluation\n\nA06-1_分析2\t\nhttp://13.112.171.13/apps/1/analytics/app_dna\n\nA02-7_個別ビーコングループ情報3\t\nhttp://13.112.171.13/apps/1/beacon_groups/2/area_heat_map\n\nA02-7_個別ビーコングループ情報4＜CV&CVA＞\t\nhttp://13.112.171.13/apps/1/beacon_groups/2/cv_map\n######################################################################################################\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n########################################################################################################\nAct 4: Hello admin\nOne of Django’s unique features is that it comes with a custom administration that allows users to view, edit and create records. To see it in action, create a new superuser with permission to edit all records.\n\n$ python manage.py createsuperuser\n  noel\n  noelkev0\n  sophie237\n\n sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   createsuperuser\n\n\n\nGcloud on BizBank:\n    Open http traffic + Check on IP Adress\n    Tag of VM (http-srver) and Tag of Firewall rule should be the same.\n\n\nNeed to add internal Gcloud VM IP in Django : 10.146.0.6\n\n\n\n\n\nThis address \nhttp://104.198.117.174/page_A011?application_id=&startdate=20170101&enddate=20170101.....\n\n\nIn Google Network Setting, allow Http Requests. with IP Config\n\n\n\n\n\n##############\nBig JSON, Static JSON  ---->  Generate JSScript Library JSON   ---> Use table for rendering\n\n\n\n##### Fast Hash to encode datetime:\nhttps://www.npmjs.com/package/xxhashjs  : Javascript\nhttps://pypi.python.org/pypi/xxhash/    : Python\n\nhash(\"MMDDYYYY+sfdf\")  !=  then break\n\n\n\n\n\n\n####### Pandas Eval for fast evaluation:\ndf.eval(\"\"\"\n   ....: c = a + b\n   ....: d = a + b + c\n   ....: a = 1\"\"\", inplace=False)\n\n\n\nIn [42]: df = pd.DataFrame(np.random.randn(5, 2), columns=list('ab'))\n\nIn [43]: newcol = np.random.randn(len(df))\n\nIn [44]: df.eval('b + @newcol')\n\n\n\nIn [66]: df.query('strings == \"a\" and nums == 1')\nOut[66]: \nEmpty DataFrame\nColumns: [nums, strings]\nIndex: []\n\n\n\n\n\n\n\n\n##### Security Process:  \nhttp://stackoverflow.com/questions/298772/django-template-variables-and-javascript\n\n\n\n\n########## SSL Cerificates   #####################################################\nhttps://simpleisbetterthancomplex.com/tutorial/2016/05/11/how-to-setup-ssl-certificate-on-nginx-for-django-application.html\n\n\n#### Generate Certificates :\n\nWith Shell Access\nWe recommend that most people with shell access use the Certbot ACME client. It can automate certificate issuance and installation with no downtime. It also has expert modes for people who don’t want autoconfiguration. It’s easy to use, works on many operating systems, and has great documentation. \nVisit the Certbot site to get customized instructions for your operating system and web server.\nIf Certbot does not meet your needs, or you’d like to try something else, \nthere are many more ACME clients to choose from. Once you’ve chosen ACME client software, \nsee the documentation for that client to proceed.\nIf you’re experimenting with different ACME clients, use our staging environment to avoid hitting rate limits.\n\n\n\n\nUsually CSR openssl configuration contains by default the details as follows below:\nCommon Name (the domain name certificate should be issued for)\nCountry\nState (or province)\nLocality (or city)\nOrganization\nOrganizational Unit (Department)\nE-mail address\nTo generate the CSR code run the following code in your server terminal:\n\nopenssl req -new -newkey rsa:2048 -nodes -keyout simpleacademy.key -out simpleacademy.csr\n\n\nGrab the contents of the file simpleacademy.csr and paste it into the activation page:\n\n\n\n\nNow you will need two files to activate the server :\nsimple_academy_cert_chain.crt\nsimpleacademy.key (the key you genered while creating the CSR)\n\n\n\n\n\n################# Javascript details   ####################################################################\nSHA384 Hex is used for the transmission of UURID.\n\n今週は　Testは　GCPですから、GCPのserverを見られるのは　いつようです.\nしょして、これから、　Krayが GCPのWeb serverを利用ください.\n\nTo make it simple, one possibility is to use inside the UURID.\nAlthough this is far from ideal situation that we need to acknowledge, \nCertificate would be much better but quite heavy in this situation.\n1)\n  uurid with template :\n  \"ServerIPadress_pagename_accountid_appid_YYYYMMDDHH\"\n  35.189.149.154_a1014_25665_55552_2017010112\" \n\n\n2) Alternatively, we can use this for the client side, \n   although this is far from ideal situation.\n   Certificate would be much better.\n\n\n<script>\n   ifrm = document.createElement(\"iframe\"); \n   var url1=  url1 + '&details='+ window.location.hostname\n   ifrm.setAttribute(\"src\", url1);\n    document.body.appendChild(ifrm); \n</script>\n\n\n\nfunction createIFrame() {\n    var ref = document.referrer;\n    ifrm = document.createElement(\"iframe\"); \n    ifrm.setAttribute(\"src\", \"http://www.nba.com/?referrer=\"+ref); \n    ifrm.style.width = 640+\"px\"; \n    ifrm.style.height = 480+\"px\"; \n    document.body.appendChild(ifrm); \n} \n\ncreateIFrame();\n\n\nvar x = location.origin;\n\n\n\n\n\n######### Summary:  ##############################\n本日の話の点のまとめ:\n\nもし　確認質問/質問/修正があったら、　ここに　入れてください。\n\n1)  Method 1 の決められた:\n    UURID :  using SHA384 Hex の方法\n    \"ServerIPadress_pagename_accountid_appid_YYYYMMDDHH\"\n    \"35.189.149.154_a1014_25665_55552_2017010112\"\n\n\n2)  Hashの確認:\n   (Kray san)のServerIp (Kray san)\n     \n    Hashの確認,お互いに　計算して、Slackに出します。\n　　  例:  \"35.189.149.154_a1014_25665_55552_2017010112\"\n\n\n3)  HTTPSの確認: (Kevin)\n  　　\n\nよろしくお願いします。\n\n\n35.189.149.154 \n\n\n\nss=  \"35.189.149.154_a1014_25665_55552_2017010112\"\nSHA384 Hex: \n'0622c4aba8b26200026282b5755bdea95a586982e3d81df4f339110b125c3ee12807a2b18fa541fba7b2f2f8c3a0cd45'\n\n\n\n\"35.189.149.154_a1014_25665_55552_2017010112\"\n\n\n\n\n#############\n\n\n\n\n\n\n\n\n\n\n### Server Limit:\n220.221.95.138       35.189.149.154         10.146.0.2\n\n\n### tcp: 8080\npip install django-sslserver\n\n\n\n#### HTTPS server :  Check at Home:\nhttps://github.com/teddziuba/django-sslserver\n\n$ pip install django-sslserver\n\n\nINSTALLED_APPS = (...\n\"sslserver\",\n...\n)\n\n\n\n###Need to generate Certificate : Private Key / Public Key\nhttps://www.sslforfree.com/create?domains=104.198.117.174\n\n\n\n\n\n5) A06-1_分析1.jpg       (with no map at 1st)\n\n6) A04-2_個別ビーコンプッシュ情報3.jpg  (with  no map at 1st)\n\n7) A02-7_個別ビーコングループ情報1＜エリア＞.jpg    (with no map at 1st)\n\n8) A02-7_個別ビーコングループ情報1＜CV＞.jpg      (with no map at 1st)\n\n9) A02-7_個別ビーコングループ情報1＜CVA＞.jpg      (with no map at 1st)\n\n\nhttp://104.198.117.174:8000/page_bi/a011/?pagei=a011&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=s5f5f54s5f5sd5fs\n\n\n\npagei=a011&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=s5f5f54s5f5sd5fs#\n\n\n\"35.189.149.154_a011_95651_5563_2017042712\"\n\n\n\n\nimport hashlib\nuurid2= hashlib.sha384(ss).hexdigest()\nss=  \"35.189.149.154_a011_95651_5563_2017042712\"\nhashlib.sha384(ss).hexdigest()\n\n\n\n\n########### Linux Ubuntu 14.0でこれは　です:   #######################################################\n\n昨日:\n>>> ss= \"35.189.149.154_a1014_25665_55552_2017010112\"\n'0622c4aba8b26200026282b5755bdea95a586982e3d81df4f339110b125c3ee12807a2b18fa541fba7b2f2f8c3a0cd45'\n\n\n今日:\n>>> ss=  \"35.189.149.154_a011_95651_5563_2017042712\"\n'0011a0de3b26bd138f12788ba3310ea878eddcdf1a1a95e36722014ea91580186994701316e320454c61a588314cd4c8'\n\n\n同じです.\n\n\nhttp://104.198.117.174:8000/page_bi/a011/?pagei=a011&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101&uurid=0011a0de3b26bd138f12788ba3310ea878eddcdf1a1a95e36722014ea91580186994701316e320454c61a588314cd4c8\n\n\n\n#### Specs of Pages :   ############################################################\nhttps://docs.google.com/document/d/1VsXa2qcvM7F8pgTwWdx3gBdw1vHoSoPXg9KFmjPArf8/edit\n\n\n\n##### CSS tags ######################################################################\n####\nboostrap.min.css\n\"Hiragino Kaku Gothic ProN\", \"Yu Gothic\", YuGothic, Verdana, Meiryo, sans-serif\n\nRed Color :\n\"color\": \"E55165\"\n\n\n\n<style>\n.rTable { display: table; width: 100%; } .rTableRow { display: table-row; } .rTableHeading { display: table-header-group; background-color: #ddd; } .rTableCell, .rTableHead { display: table-cell; padding: 3px 10px; border: 1px solid #999999; } .rTableHeading { display: table-header-group; background-color: #ddd; font-weight: bold; } .rTableFoot { display: table-footer-group; font-weight: bold; background-color: #ddd; } .rTableBody { display: table-row-group; }\n\n\n.h4class { font-family: \"Hiragino Kaku Gothic ProN\", \"Yu Gothic\", YuGothic, Verdana, Meiryo, sans-serif ;     font-size: 16px; font-weight: 600; }\n\n</style>\n\n\n<h4 class=\"h4class\">リ－ビタ－ 比率</h4>\n\n\n\n\nFinished  1\n\nPage    a011.html\nPage    a061_bunseki3.html\nPage    a42_jouhou1.html\nPage    a061_bunseki1.html\nPage    a061_bunseki2.html\n\n\n\n\n\n\n#####  Details \nfont-family:\"ヒラギノ角ゴ Pro W3\", \"Hiragino Kaku Gothic Pro\",Osaka, \"メイリオ\", Meiryo, \"ＭＳ Ｐゴシック\", \"MS PGothic\", sans-serif;\n\n\n\n\n\n\nvar lineChartOptions={\n  \"chart\": { \"caption\": \"Daily Visits\", \"linethickness\": \"1\", \"showvalues\": \"0\",\"formatnumberscale\": \"0\", \"anchorradius\": \"2\",\n  \"divlinecolor\": \"666666\", \"divlinealpha\": \"30\", \"divlineisdashed\": \"1\", \"labelstep\": \"2\", \"bgcolor\": \"FFFFFF\",\n  \"showalternatehgridcolor\": \"0\", \"labelpadding\": \"10\", \"canvasborderthickness\": \"1\", \"legendiconscale\": \"1.5\",\n  \"legendshadow\": \"0\", \"legendborderalpha\": \"0\", \"canvasborderalpha\": \"50\", \"numvdivlines\": \"5\", \"vdivlinealpha\": \"20\",\n  \"showborder\": \"0\", \n\n\"anchorradius\": \"4\",  \"anchorborderthickness\": \"2\",\n\n\n\n### Pink Color\nEA7584\n\n\n\n########################################################################################################################\n##### Prod Server\n  timestamp server\n  whitenoise is activated ---> Static Files must be updated by collectstatic\n    sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   collectstatic\n\n\n\n\n\n\n####  Dev\n  Remove whitenoise when doing devs\n\n\nDJANGO_DEBUG=12 sudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   runserver 0.0.0.0:8000\n\n\n\n\n\n\nml{font-size:10px;-webkit-tap-highlight-color:rgba(0,0,0,0)}body{f\n\n\n\n\n#### Refactoring :\n\n   \n#####################################\n  <body>\n  <div class=\"container\">\n    <div align='middle'>\n\n\n<iframe width=\"1024x\" height=\"230px\"   scrolling=\"no\"    style=\"\" src=\"graph_test2.html\" frameborder=\"0\" allowFullScreen=\"False\"></iframe>\n\n\n<iframe width=\"1024x\" height=\"230px\"   scrolling=\"no\"    style=\"\" src=\"graph_test2.html\" frameborder=\"0\" allowFullScreen=\"False\"></iframe>\n\n\n<iframe width=\"1024x\" height=\"230px\"   scrolling=\"no\"    style=\"\" src=\"http://104.198.117.174:8000/page_bi/graph_test2/?pagei=a011&account_id=95651&application_id=5563&startdate=20170101&enddate=20170101\" frameborder=\"0\" allowFullScreen=\"False\"></iframe>\n\n\n\n\nIp Adress from Kray  ipadress 153.156.78.132   Kray IP\n\n\n##################################################################################\nUnerry Ip Adresss : 122.208.202.84\nAsus Ip Adress :  220.221.95.138\n\n\nUnerry G, 3G :    153.156.78.132\nPortalPoint, 5G : 122.208.202.84\n\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/visualsandbox.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/visualhostcore.min.js\"></script>\nvisualsandbox.min.js:1\n\n\ne88f171229927ce6b10947d7b8dc87d10405ccde8bc0abb0501ef032a22bc1a738dc7ab0f62a843663803b7a507c1ba0\n\n\n@uchijima ,  :\nお疲れさまです.\n\n以下の話しについて:\nServer SSL certificateのファイルは　ありませんか?\n( Certificate file も Certificate key file ) ? (edited)\nよろしく\n\n\n\n\n\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/visualsandbox.minimal.externals.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/visualssandboxcore.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/utility.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/data.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/globalize.min.js\" defer=\"\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/globalize.culture.en-us.js\" defer=\"\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/localytics.min.js\" defer=\"\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/powerbiportal.dependencies.externals.bundle.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/powerbiportal.dependencies.bundle.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/powerbiportal.common.bundle.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/powerbiportal.explore.bundle.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/powerbiportal.addons.bundle.min.js\"></script>\n<script type=\"text/javascript\" src=\"../../static/gg/powerbi/powerbiportal.web.bundle.min.js\"></script>\n<script src=\"../../static/gg/powerbi/ai.0.js\"></script>\n\n<script>\n    var powerBIAccessToken = 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6InowMzl6ZHNGdWl6cEJmQlZLMVRuMjVRSFlPMCIsImtpZCI6InowMzl6ZHNGdWl6cEJmQlZLMVRuMjVRSFlPMCJ9.eyJhdWQiOiJodHRwczovL2FuYWx5c2lzLndpbmRvd3MubmV0L3Bvd2VyYmkvYXBpIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvY2VmNzFjMDItNzRjYS00MWY0LTk5NjUtYTRhNDdhMmQ0MTliLyIsImlhdCI6MTQ5MzY5MTA3OCwibmJmIjoxNDkzNjkxMDc4LCJleHAiOjE0OTM2OTQ5NzgsImFjciI6IjEiLCJhaW8iOiJBU1FBMi84REFBQUFaZld0cStOcXhHaUkrYk9yei9idjdwT0J6cFpUSG1KbUxUaVNHT3ZmUlBNPSIsImFtciI6WyJwd2QiXSwiYXBwaWQiOiI4NzFjMDEwZi01ZTYxLTRmYjEtODNhYy05ODYxMGE3ZTkxMTAiLCJhcHBpZGFjciI6IjIiLCJlX2V4cCI6MjYyODAwLCJmYW1pbHlfbmFtZSI6IktldmluIiwiZ2l2ZW5fbmFtZSI6Ik5vZWwiLCJpcGFkZHIiOiIxNTMuMTU2Ljc4LjEzMiIsIm5hbWUiOiJOb2VsIEtldmluIiwib2lkIjoiOGQyNzEyMGEtZTAzMC00ZmJmLWJkNDgtNjNmOGNiOWYwYjllIiwicGxhdGYiOiIzIiwicHVpZCI6IjEwMDNCRkZEOUQ3Q0EzODIiLCJzY3AiOiJ1c2VyX2ltcGVyc29uYXRpb24iLCJzdWIiOiJJajR0TjFCLU9VNy0wNlFxR2t4dVVwN1FYVW1YN3hVVS00dXNjWGEzQkhRIiwidGlkIjoiY2VmNzFjMDItNzRjYS00MWY0LTk5NjUtYTRhNDdhMmQ0MTliIiwidW5pcXVlX25hbWUiOiJub2VsQHVuZXJyeS5jby5qcCIsInVwbiI6Im5vZWxAdW5lcnJ5LmNvLmpwIiwidmVyIjoiMS4wIn0.kqeBJlqqzmsznqVJzNV7UTqA5UhcWoT2bqtfA_Xz5Cbd_cGtC0SsqJOmEjXOlLyv3DK5qeVca1t3JRq6fQJOpgvZt4vaGLjrT_EDGnkeS-PxmBvTb7YfkshrVO7c0UlfYSh2o7q52zD3d-Q5RGgAwIawKDjluQqJ1g2BF6joOoeg8T-l2FV96qJb2wjWWIl9PN11Zrk-8WTJuVz-cBlD17AVc2fidyl30yTA2SVJE-fwnKAEHuv_IKwMO5_O5foDrd9s4o51XZFdT0JfCCGFhYZdY57Lq9O5NhOUDb8VxmUjsZQqZwbMnuLTW5HsSMZULRebRJXCCI1UItFJrE7l5w';\n    var powerBIAccessTokenExpiry = '';\n    var baseUrl = window.location.protocol + \"//\" + window.location.host;\n    var powerbi = {\n        session : {\n            userInfo: {\n                name:  '',\n                givenName: '',\n                surname: '',\n                puid: '1003BFFD9D7CA382',\n                uoid: '8d27120a-e030-4fbf-bd48-63f8cb9f0b9e',\n                alternateEmail: ''\n            }\n        }\n    };\n    powerbi.telemetrySamplingRules = { appInsights: [{ purpose: \"CriticalError\", sampleRate: 1 },{ purpose: \"CustomerAction\", sampleRate: 1 },{ purpose: \"Verbose\", sampleRate: 1 }], localytics: [{ purpose: \"CriticalError\", sampleRate: 1 },{ purpose: \"CustomerAction\", sampleRate: 1 },{ purpose: \"Verbose\", sampleRate: 0 }], perf: [{ purpose: \"CriticalError\", sampleRate: 1 },{ purpose: \"CustomerAction\", sampleRate: 1 },{ purpose: \"Verbose\", sampleRate: 1 }] };\n    powerbi.build = '13.0.1700.2008';\n    powerbi.buildDetails = '13.0.1700.2008 ((PowerBI_2017_04_4).170429-2002)';\n    powerbi.common = {};\n    powerbi.common.cultureInfo = 'en-US';\n    powerbi.common.unmappedCultureInfo = 'en-US';\n    powerbi.common.isCurrentContextRtl = 'False';\n    powerbi.common.disableMap = 'False';\n    powerbi.customVisualsUrl = 'https://visuals.azureedge.net/;https://visuals2.azureedge.net/;https://extendcustomvisual.blob.core.windows.net/';\n    powerbi.visualCDNBlobContainerUrl = 'prod';\n    var clusterUri =  'https://wabi-japan-east-redirect.analysis.windows.net';\n    var apiUri =  'https://api.powerbi.com';\n    var tenantId = 'cef71c02-74ca-41f4-9965-a4a47a2d419b';\n    var previousTenantId =  '';\n    var appInsightsV2InstrKey = '00406067-1af3-44c5-a2c1-4a57dd50194c';\n    var localyticsInstrKey = 'd481bcc9809fe5c23d780ae-ff4d768c-3469-11e4-a38d-009c5fda0a25';\n    var telemetrySessionId =  '6e417ef7-1929-4adc-8530-ef0f408869cf';\n    var featureSwitches = (function () { var adminAuditingEnabled = true, adminCapacityEnabled = false, allUpSaveReport = true, altTextForVisual = false, analyzeInExcelEnabled = true, analyzeInExcelSovereignFormatEnabled = false, anonymousEmbeddingEnabled = true, approvedResourcesDisabled = false, appsEnabled = false, appStoreCdnOverride = false, appTemplatesEnabled = false, autoPausedSyncService = false, axisControlImprovements = true, azureUsageAndBillingAppEnabled = false, bingAdsAppEnabled = false, binnedLineSampling = false, breadcrumbNavigationEnabled = true, builtInContentProvidersDisabled = false, clientUsageMetricsEnabled = false, closeAccountEnabled = true, cloudRlsImpersonationEnabled = true, cloudRlsRoleMembershipEnabled = true, clusteringEnabled = false, conceptualModelEnabled = false, csvContentProviderEnabled = true, customFontFamily = true, customVisuals = false, customVisualsUseStaticSandbox = false, dashboardEmailSubscription = false, dashboardEmbedEnabled = true, dataClassificationEnabled = true, datasetDownloadPbix = true, datetimeMinMaxSupported = true, devToolsNewVisual = true, devToolsVisualSettings = true, directQueryScheduledRefresh = false, donutChartLabelPercentEnabled = true, downloadReportEnabled = true, dynamicMessageEnabled = true, emailSubscriptionEnabled = true, embedFullFidelityWorkbooks = true, enableExportDataNewDialog = true, enterpriseGatewayEnabled = true, enterpriseGatewayETLEnabled = true, esriEnabled = true, excelWorkbooksInContentPackEnabled = true, exportReportToPowerPointEnabled = true, exportVisualToExcelEnabled = true, favoritesEnabled = true, ffxlChartSelectionEnabled = true, ffxlLocalFilesEnabled = true, forecastEnabled = true, frontLoadReportEmbedEnabled = true, fullFidelityExcelEnabled = true, granularTenantControls = true, groupCapacity = false, hierarchyAuthoringEnabled = false, hierarchyDragDrop = false, highVolume = false, homeDashboardEnabled = true, importPbiviz = true, inFocusEditEnabled = false, insightsStoreEnabled = true, insightsSyncServiceEnabled = true, isSovereignCloud = false, listViewEnabled = true, matrixWordWrap = true, microsoftOrgAppEnabled = false, multipleODataPredicates = false, negatedTuplesFiltering = false, numericSlicerEnabled = false, o365PowerBIEnabled = true, oneGBUploadFileSizeEnabled = true, orgAppsEnabled = true, paasDynamicRoutingEnabled = true, permissionCenterEnabled = false, pivotTableVisualEnabled = false, preferHigherDataVolume = true, previewConceptualModel = true, previewDefaultOptIn = false, psaAccountManagerAppEnabled = false, psaPracticeManagerAppEnabled = false, psaResourceManagerAppEnabled = false, pseudoLocEnabled = false, qnaSupportOnPremEnabled = true, readOnlyGroupEnabled = true, realTimeASAEnabled = false, realTimePubNubEnabled = true, relativeDateSlicer = false, reportEmbedEditingEnabled = true, reportMeasures = true, responsiveVisualEnabled = false, saasMarketplace = true, salesforceAndOneDriveCredentialsEnabled = true, sandboxVisualsEnabled = true, scriptVisualAnonymousEmbeddingEnabled = false, scriptVisualAuthoringEnabled = false, scriptVisualEnabled = true, scriptVisualLaunchExternalIDEEnabled = false, selectiveModelRefresh = false, selectiveRefreshEnabled = true, serviceAppsEnabled = false, sharePointDocumentLibrariesEnabled = true, sharePointEmbedEnabled = true, showNonUserEntitiesEnabled = false, socialSharingEnabled = true, sparkAppEnabled = true, staticExportReportEnabled = false, stringMinMax = true, tablixWordWrap = true, targetedDataViewParse = false, templateAppUpgradeEnabled = false, templatePublishFlightingWorkAroundEnabled = false, testClientSwitchesForSafeDeployment = false, textboxFontColorEnabled = true, tuplesFiltering = true, useBackendFSEnabled = true, userNotificationsEnabled = true, visualContainerTileConfig = false, vsoVnextAppEnabled = false, withinDXT = false; return { adminAuditingEnabled: function () { return adminAuditingEnabled; }, adminCapacityEnabled: function () { return adminCapacityEnabled; }, allUpSaveReport: function () { return allUpSaveReport; }, altTextForVisual: function () { return altTextForVisual; }, analyzeInExcelEnabled: function () { return analyzeInExcelEnabled; }, analyzeInExcelSovereignFormatEnabled: function () { return analyzeInExcelSovereignFormatEnabled; }, anonymousEmbeddingEnabled: function () { return anonymousEmbeddingEnabled; }, approvedResourcesDisabled: function () { return approvedResourcesDisabled; }, appsEnabled: function () { return appsEnabled; }, appStoreCdnOverride: function () { return appStoreCdnOverride; }, appTemplatesEnabled: function () { return appTemplatesEnabled; }, autoPausedSyncService: function () { return autoPausedSyncService; }, axisControlImprovements: function () { return axisControlImprovements; }, azureUsageAndBillingAppEnabled: function () { return azureUsageAndBillingAppEnabled; }, bingAdsAppEnabled: function () { return bingAdsAppEnabled; }, binnedLineSampling: function () { return binnedLineSampling; }, breadcrumbNavigationEnabled: function () { return breadcrumbNavigationEnabled; }, builtInContentProvidersDisabled: function () { return builtInContentProvidersDisabled; }, clientUsageMetricsEnabled: function () { return clientUsageMetricsEnabled; }, closeAccountEnabled: function () { return closeAccountEnabled; }, cloudRlsImpersonationEnabled: function () { return cloudRlsImpersonationEnabled; }, cloudRlsRoleMembershipEnabled: function () { return cloudRlsRoleMembershipEnabled; }, clusteringEnabled: function () { return clusteringEnabled; }, conceptualModelEnabled: function () { return conceptualModelEnabled; }, csvContentProviderEnabled: function () { return csvContentProviderEnabled; }, customFontFamily: function () { return customFontFamily; }, customVisuals: function () { return customVisuals; }, customVisualsUseStaticSandbox: function () { return customVisualsUseStaticSandbox; }, dashboardEmailSubscription: function () { return dashboardEmailSubscription; }, dashboardEmbedEnabled: function () { return dashboardEmbedEnabled; }, dataClassificationEnabled: function () { return dataClassificationEnabled; }, datasetDownloadPbix: function () { return datasetDownloadPbix; }, datetimeMinMaxSupported: function () { return datetimeMinMaxSupported; }, devToolsNewVisual: function () { return devToolsNewVisual; }, devToolsVisualSettings: function () { return devToolsVisualSettings; }, directQueryScheduledRefresh: function () { return directQueryScheduledRefresh; }, donutChartLabelPercentEnabled: function () { return donutChartLabelPercentEnabled; }, downloadReportEnabled: function () { return downloadReportEnabled; }, dynamicMessageEnabled: function () { return dynamicMessageEnabled; }, emailSubscriptionEnabled: function () { return emailSubscriptionEnabled; }, embedFullFidelityWorkbooks: function () { return embedFullFidelityWorkbooks; }, enableExportDataNewDialog: function () { return enableExportDataNewDialog; }, enterpriseGatewayEnabled: function () { return enterpriseGatewayEnabled; }, enterpriseGatewayETLEnabled: function () { return enterpriseGatewayETLEnabled; }, esriEnabled: function () { return esriEnabled; }, excelWorkbooksInContentPackEnabled: function () { return excelWorkbooksInContentPackEnabled; }, exportReportToPowerPointEnabled: function () { return exportReportToPowerPointEnabled; }, exportVisualToExcelEnabled: function () { return exportVisualToExcelEnabled; }, favoritesEnabled: function () { return favoritesEnabled; }, ffxlChartSelectionEnabled: function () { return ffxlChartSelectionEnabled; }, ffxlLocalFilesEnabled: function () { return ffxlLocalFilesEnabled; }, forecastEnabled: function () { return forecastEnabled; }, frontLoadReportEmbedEnabled: function () { return frontLoadReportEmbedEnabled; }, fullFidelityExcelEnabled: function () { return fullFidelityExcelEnabled; }, granularTenantControls: function () { return granularTenantControls; }, groupCapacity: function () { return groupCapacity; }, hierarchyAuthoringEnabled: function () { return hierarchyAuthoringEnabled; }, hierarchyDragDrop: function () { return hierarchyDragDrop; }, highVolume: function () { return highVolume; }, homeDashboardEnabled: function () { return homeDashboardEnabled; }, importPbiviz: function () { return importPbiviz; }, inFocusEditEnabled: function () { return inFocusEditEnabled; }, insightsStoreEnabled: function () { return insightsStoreEnabled; }, insightsSyncServiceEnabled: function () { return insightsSyncServiceEnabled; }, isSovereignCloud: function () { return isSovereignCloud; }, listViewEnabled: function () { return listViewEnabled; }, matrixWordWrap: function () { return matrixWordWrap; }, microsoftOrgAppEnabled: function () { return microsoftOrgAppEnabled; }, multipleODataPredicates: function () { return multipleODataPredicates; }, negatedTuplesFiltering: function () { return negatedTuplesFiltering; }, numericSlicerEnabled: function () { return numericSlicerEnabled; }, o365PowerBIEnabled: function () { return o365PowerBIEnabled; }, oneGBUploadFileSizeEnabled: function () { return oneGBUploadFileSizeEnabled; }, orgAppsEnabled: function () { return orgAppsEnabled; }, paasDynamicRoutingEnabled: function () { return paasDynamicRoutingEnabled; }, permissionCenterEnabled: function () { return permissionCenterEnabled; }, pivotTableVisualEnabled: function () { return pivotTableVisualEnabled; }, preferHigherDataVolume: function () { return preferHigherDataVolume; }, previewConceptualModel: function () { return previewConceptualModel; }, previewDefaultOptIn: function () { return previewDefaultOptIn; }, psaAccountManagerAppEnabled: function () { return psaAccountManagerAppEnabled; }, psaPracticeManagerAppEnabled: function () { return psaPracticeManagerAppEnabled; }, psaResourceManagerAppEnabled: function () { return psaResourceManagerAppEnabled; }, pseudoLocEnabled: function () { return pseudoLocEnabled; }, qnaSupportOnPremEnabled: function () { return qnaSupportOnPremEnabled; }, readOnlyGroupEnabled: function () { return readOnlyGroupEnabled; }, realTimeASAEnabled: function () { return realTimeASAEnabled; }, realTimePubNubEnabled: function () { return realTimePubNubEnabled; }, relativeDateSlicer: function () { return relativeDateSlicer; }, reportEmbedEditingEnabled: function () { return reportEmbedEditingEnabled; }, reportMeasures: function () { return reportMeasures; }, responsiveVisualEnabled: function () { return responsiveVisualEnabled; }, saasMarketplace: function () { return saasMarketplace; }, salesforceAndOneDriveCredentialsEnabled: function () { return salesforceAndOneDriveCredentialsEnabled; }, sandboxVisualsEnabled: function () { return sandboxVisualsEnabled; }, scriptVisualAnonymousEmbeddingEnabled: function () { return scriptVisualAnonymousEmbeddingEnabled; }, scriptVisualAuthoringEnabled: function () { return scriptVisualAuthoringEnabled; }, scriptVisualEnabled: function () { return scriptVisualEnabled; }, scriptVisualLaunchExternalIDEEnabled: function () { return scriptVisualLaunchExternalIDEEnabled; }, selectiveModelRefresh: function () { return selectiveModelRefresh; }, selectiveRefreshEnabled: function () { return selectiveRefreshEnabled; }, serviceAppsEnabled: function () { return serviceAppsEnabled; }, sharePointDocumentLibrariesEnabled: function () { return sharePointDocumentLibrariesEnabled; }, sharePointEmbedEnabled: function () { return sharePointEmbedEnabled; }, showNonUserEntitiesEnabled: function () { return showNonUserEntitiesEnabled; }, socialSharingEnabled: function () { return socialSharingEnabled; }, sparkAppEnabled: function () { return sparkAppEnabled; }, staticExportReportEnabled: function () { return staticExportReportEnabled; }, stringMinMax: function () { return stringMinMax; }, tablixWordWrap: function () { return tablixWordWrap; }, targetedDataViewParse: function () { return targetedDataViewParse; }, templateAppUpgradeEnabled: function () { return templateAppUpgradeEnabled; }, templatePublishFlightingWorkAroundEnabled: function () { return templatePublishFlightingWorkAroundEnabled; }, testClientSwitchesForSafeDeployment: function () { return testClientSwitchesForSafeDeployment; }, textboxFontColorEnabled: function () { return textboxFontColorEnabled; }, tuplesFiltering: function () { return tuplesFiltering; }, useBackendFSEnabled: function () { return useBackendFSEnabled; }, userNotificationsEnabled: function () { return userNotificationsEnabled; }, visualContainerTileConfig: function () { return visualContainerTileConfig; }, vsoVnextAppEnabled: function () { return vsoVnextAppEnabled; }, withinDXT: function () { return withinDXT; } }; })();\n    var embeddedWebContentIframeSource = 'https://app.pbiwebcontent.com/webcontentsandbox.html';\n    var supportedSaasMarketplaceRedirects = 'https://local.spza.microsoft-int.com;https://appsource.microsoft.com;https://appgallery.spza-staging.net;https://appgallery.spza-internal.net';\n    var saasMarketplaceUrlOrigin = 'https://appsource.microsoft.com';\n    var npsUrlOrigin = 'https://nps.onyx.azure.net';\n    var dynamicMessagingUrl = 'https://dynmsg.modpim.com';\n    var dynamicMessageSurfaceName = 'PowerBI_Notification_Center_Web_APP';\n    var downloadAndroidAppFWlink = 'https://go.microsoft.com/fwlink/?LinkId=544867';\n    var downloadPageFWlink = 'https://go.microsoft.com/fwlink/?linkid=526501';\n    var powerBIOperator = '';\n    var powerBIOperatorLocale = '';\n</script>\n        \n\n\n\n\n\n\n\n\n\n\n##################################################################################\nangular   :    jsfhue\nfusioncharts:  pdgjr5\ntabulator:     a5e4e5z\nangular-fusioncharts:  fsdfsdf5sd5fs5d.min.js\njquery  :  njfiorioze\n\n\"fusioncharts.charts.js\":   ds8f5dsf5s.js\n \"fusioncharts.widgets.js\",  prezjrzej.js  \n\n\n\n    <!-- load library--->\n    <script src=\"../../static/gg/custom_js_css/jquery-3.2.1.js\"></script>\n    <script src=\"../../static/gg/custom_js_css/jquery-ui.js\"></script>\n    <script src=\"../../static/gg/custom_js_css/dbca6ceb2a.js\"></script>\n    <script type=\"text/javascript\" src=\"../../static/gg/custom_js_css/angular/angular.js\"></script>\n     <script type=\"text/javascript\" src=\"../../static/gg/custom_js_css/angular/ui-bootstrap-tpls-2.5.0.min.js\"></script>\n\n    <!--    fusion chart library-->\n    <script type=\"text/javascript\" src=\"../../static/gg/fusioncharts/js/fusioncharts.js\"></script>\n    <script type=\"text/javascript\" src=\"../../static/gg/fusioncharts/wrappers/angularjs/angular-fusioncharts.min.js\"></script>\n    <script src=\"../../static/gg/custom_js_css/ui-grid.min.js\"></script>\n    <link rel=\"stylesheet\" href=\"../../static/gg/custom_js_css/ui-grid.min.css\">\n\n    <!-- Tabulator -->\n    <link href=\"../../static/gg/custom_js_css/tabulator.min.css\" rel=\"stylesheet\">\n    <script type=\"text/javascript\" src=\"../../static/gg/tabulator/tabulator.js\"></script>\n\n    <link rel=\"stylesheet\" href=\"../../static/gg/custom_js_css/bootstrap.min.css\">\n\n\n\n\n\n\n\n##################################################################################################\nUnerry Ip Adrress:\n\n122.208.202.84\n\n\n\n\n\n\n\n\n\n\n\n\n##################################################################################################\n#####  DEBUG= False, Static files are not served \nhttp://whitenoise.evans.io/en/stable/\n\npip install whitenoise\n\n\n1) Open your settings.py and add the following on Bottom\n\nSTATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')\n\n# Simplified static file serving with WhiteNoise adding cachable files and gzip support\nSTATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'\n\n\n2) Run python manage.py collectstatic to put all your static files into STATIC_ROOT. \nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   collectstatic\n\n\n3) Open your wsgi.py and write down :\n\nfrom whitenoise.django import DjangoWhiteNoise\n\napplication = get_wsgi_application()\napplication = DjangoWhiteNoise(application)\n\n\n\n4) Launch Production server\nsudo -E /home/noel/anaconda2/bin/python  /home/noel/project27/geoapp/geoproject/manage.py   runserver 0.0.0.0:8000 \n\n\n\n\n\n\n######### Micro service          #################################################################\nhttps://www.fullstackpython.com/microservices.html\n\n\n\n\n####  IP is not transmitted, this is hashed\nWhat about using this javascript ?\n\n\n\n### Caching Generation :\n  application_id,  start_date,  end_date:\n\n\nIn Django 1.9 and older, make \n\n\n\n### Disk Cache\ndiskcache 2.4.1\nIn [5]: import diskcache as dc\nIn [6]: cache = dc.Cache('tmp')\nIn [7]: cache[b'key'] = b'value'\nIn [8]: %timeit cache[b'key']\n\n\nIn [5]: import diskcache as dc\nIn [6]: cache = dc.Cache('tmp')\nIn [7]: cache[b'key'] = b'value'\nIn [8]: %timeit cache[b'key']\n\n\n\n\n####\n   json_data[\"graph_middle_tab1\"]['daily_graph'][\"xaxis\"][\"data\"]=  dfi1['datestr'].to_dict('record')       # { \"value\": \"2016/01/15\"}, \n\n\n\nこれは　OK ですか? \n--> 新商品販売動向だけではなく、直接的なVM売上UPにつながるための施策を検討にしてほしい。\n      Ex: Webのデ－タで VM購入時の状況や、競合VMの情報を統合するなど。\n\n\n\na011.html :  When Click on TAB, small issue\na027_area.html\tLarge renaming of libraries.\t10 hours ago\na027_cva.html\tLarge renaming of libraries.\t10 hours ago\n\na061_bunseki1.html\tLarge renaming of libraries.\t10 hours ago\n\n\n\n\t\n\n\n\n\nhttps://rawgit.com/arita37/bipage/master/templates/bb/a042_jouhou1.html\n\n\n\n\n\n\n\n\nhttp://www.fusioncharts.com/dev/getting-started/list-of-charts.html\n\n\n\n\nYou can check it here:\nhttp://rawgit.com/\nCopy paste the Github page:\nhttps://github.com/arita37/bipage/blob/master/templates/bb/a027_jouhou1.html\n\n\n\nby Tommorrow ok ?\n\n\n\n\n\nhttps://github.com/arita37/bipage/tree/master/templates/bb\n\n\n\nhttps://docs.google.com/document/d/1VsXa2qcvM7F8pgTwWdx3gBdw1vHoSoPXg9KFmjPArf8/edit\n\n\n\n>>> ss=  \"35.189.149.154_a011_95651_5563_2017042712\"\n>>> hashlib.sha384(ss).hexdigest()\n'0011a0de3b26bd138f12788ba3310ea878eddcdf1a1a95e36722014ea91580186994701316e320454c61a588314cd4c8'\n\n\n'0011a0de3b26bd138f12788ba3310ea878eddcdf1a1a95e36722014ea91580186994701316e320454c61a588314cd4c8'\n\n\n\n\n\n\n#### Configure URL parameter caching\nhttp://djangobook.com/advanced-views-urlconfs/\n\n\n\n\n##### Duplicate logs:\n  1 beacon  ----> many groups  (duplicate logs)\n\n\n\n\n\n\n\n\n########################################################################################\n#### Transfer Image from Project 1 to Project 2  #######################################\nhttp://stackoverflow.com/questions/29585381/google-compute-engine-use-snapshot-from-another-project\n\n\n#### 1) Create Image in origin Project:  #################################################\n  1)  Snapshot the disk\n  2)  Create disk from snapshot\n  3)  Create image from disk\n      Then, we can use this image in another project\n\nIn Image, click on the Image and \"view REST\" :\n{ \"kind\": \"compute#image\",  \"id\": \"7626611612022440063\",  \"creationTimestamp\": \"2017-03-28T03:44:32.182-07:00\",\n  \"name\": \"kkk-image-bengine-0328\", \"description\": \"kkk-image-bengine-0328  pour kevin\",\n  \"sourceType\": \"RAW\",  \"status\": \"READY\",\n  \"archiveSizeBytes\": \"5673890206\",\n  \"diskSizeGb\": \"20\",\n  \"sourceDisk\": \"projects/protean-bus-157207/zones/asia-northeast1-c/disks/disk-bengine-0328\",\n  \"sourceDiskId\": \"4968873866298884268\",\n  \"licenses\": [\"projects/ubuntu-os-cloud/global/licenses/ubuntu-1404-trusty\",\n  #Image URL\n  \"selfLink\": \"projects/protean-bus-157207/global/images/kkk-image-bengine-0328\"\n}\n  \"selfLink\": \"projects/test-beaconbank-biz/global/images/image-kk\",\n\n\n\n#### In the Target project where you want to create the Instance   ############################\n 1) Open Gcloud Shell\n\n 2) Write\n  gcloud compute instances create  NAMEINSTANCE   --image  URI\n  gcloud compute instances create biz-analytic    --image  https://www.googleapis.com/compute/v1/projects/protean-bus-157207/global/images/kkk-image-bengine-0328\n\n  gcloud compute instances create biz-analytic    --image  https://www.googleapis.com/compute/v1/projects/test-beaconbank-biz/global/images/image-kk\n\n  Choose region asia-northeast-a\n\n\n\n3) Check the network access\n\n   Check if Instance VM has the following Tag:  (in Compute VM Instance)\n      http-server\n      https-server\n\n      http-servers  (customize Firewall rule created in Network / Firewall ).\n\n   Create a firewall rule and assign tag to it :       http-servers  \n      tag : http-servers\n      tcp: 8000   Port IP¨Check\n      0.0.0.0  \n\n \n   Check if the Network Firewall Rules are the following :\n     Ip adress\n     Unerry, Portalpoint :   122.208.202.84\n     Unerry-g,           :   \n     Kevin Asus, Obama2 :    220.221.95.138\n\n\nhttp://djangobook.com/deploying-django/\n\n\n\n\n\n#####################################################################################################\n#### Local Cloud :            #######################################################################\n    project: protean-bus-157207\n    Zone :  asia-northeast1-c\n\n#### Beacon Test cloud :      #######################################################################\n    project:   test-beaconbqnk-biz\n    Zone   :   asia-northeast1-a\n    VM Name:   biz-analytics\n    Static External address:  104.198.117.174\t\n    user_login :              noel@biz-analytic\n 　　 biz-analytic.c.test-beaconbqnk-biz.internal\n\n\n\n\n\n###########################################################################################\n#### Setup access Key  using Putty, use Private key to access\nhttps://cloud.google.com/compute/docs/instances/connecting-to-instance\nnoel@104.198.117.174\n\n\n1) Generate your keys using ssh-keygen or PuTTYgen for Windows \n   In the Key comment section, enter your Google username. \n    root  : for root access on the VM\n    noel :  for noel access on the VM\n\n    ssh-rsa [KEY_VALUE] [USERNAME]    \n    Save Private Key in Folder, Save Public Key in Folder.\n\n\n2) In the navigation, Gcloud --> Compute->Compute Engine->Metadata.\n    Click the SSH Keys tab.,     Click the Edit button.\n    Create new SSH Key.\n    Copy Previously generated SSH Public Key here.\n\n  In the empty input box at the bottom of the list, enter the corresponding public key, in the following format: \n   <protocol> <public-key> username@example.com \n    ssh-rsa [KEY_VALUE] [USERNAME]\n\n\n3) This makes your public key automatically available to all of your instances in that project. \n    It can take several minutes before the key is inserted into the instance. \n    If it is successful, your key has been propagated to the instance.\n\n\n4) You can use Private Key in FileZilla for SFTP Access, or Putty for SSH\n\n########################################################################################\n\n\n\n\n###### Cloud SQL access to data  #######################################################\nhttps://cloud.google.com/sql/docs/postgres/extensions\nPOSTGIS extension\n\n\n\n\n\n###########################################################################################\n###### MYSQL   ############################################################################\nmysql+mysqldb://<user>:<password>@<host>[:<port>]/<dbname>  :   3306\n\nmysql-client application on biz-analytics VM.\n\n\n#### Access to Biz-Analytics DB   #########################################################\nmysql -u root -h 104.198.113.129\n\n\n  ### MYsql Command Line :  \n  show database;\n  use biz_analytics;\n  CREATE TABLE test1 (col1 VARCHAR(120), date1 DATETIME);\n  exit;       # exit \n\n\n\n\n############################################################################################\n$ mysql -\n\nSelect database: use [database];\n\nDetermine what database is in use: select database();\n\nShow all tables: show tables;\n\nShow table structure: describe [table];\n\nList all indexes on a table: show index from [table];\n\nCreate new table with columns: CREATE TABLE [table] ([column] VARCHAR(120), [another-column] DATETIME);\n\nAdding a column: ALTER TABLE [table] ADD COLUMN [column] VARCHAR(120);\n\nAdding a column with an unique, auto-incrementing ID: ALTER TABLE [table] ADD COLUMN [column] int NOT NULL AUTO_INCREMENT PRIMARY KEY;\n\nInserting a record: INSERT INTO [table] ([column], [column]) VALUES ('[value]', [value]');\n\nMySQL function for datetime input: NOW()\n\nSelecting records: SELECT * FROM [table];\n\nExplain records: EXPLAIN SELECT * FROM [table];\n\nSelecting parts of records: SELECT [column], [another-column] FROM [table];\n\n\nmysql+mysqldb://<user>:<password>@<host>[:<port>]/<dbname>  :   3306\n   user \n   password\n   104.198.113.129:3306\n   biz_analytics \n\n\nmysql-client application on biz-analytics VM.\n\n\nPlease confirm that there are some databeses (bbcp_* / biz_*) with \"show databases;\".\nAnd you may create tables you need on database \"biz_analytcis\".\n\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| bbcp_logs          |\n| bbcp_main          |\n| biz_analytics      |\n| biz_main           |\n| mysql              |\n| performance_schema |\n+--------------------+\n########################################################################################\n########################################################################################\n\n\n\n########## Test Cloud ######################################################################\nInstance_name : bbizdb\nLogin: User \nPass : bbiz_unerry_123456\nbbizdb\n\n\n\n\n\n\n\n\n########################################################################################\nベストビーコンのAPIの件のまとめです。\n\n-----------\n● input　parameter  :ベストビーコンを選ぶ条件\n\n・アプリケーションID\n・CV beacon group  id（Comma separated)\n・CVビーコンからの距離　(300m, 500m,800m, 1km,3km,5km,10km )　 one of those\n・屋内外区分（0:屋内,1:屋外, 指定しない(0,1, both)）one of those\n・パブリック区分（0:クローズド,1:パブリック,指定しない(0,1, both)）one of those\n・設置場所分類(install_loc_cat2id)  Multiple available\n・最終変更日付　yyyymmdd　以降\n・1グループあたりのビーコン数上限：n個\n\n<screen> \nhttps://drive.google.com/drive/folders/0B6fup0bYMkVURXBFN0VsRk1jdU0\n\n\n●output  parameter　：CV beacon groupごとのベストビーコンのリスト\n\n\ndata: {\n  cv_group.id: {\n    group_type:3,\n    beacon_ids: idのリスト（n個以下）,\n  },\n  ...\n}\n\n\n < screen> \nhttps://drive.google.com/drive/folders/0B6fup0bYMkVURXBFN0VsRk1jdU0\n\n\n＜注意点＞\n\n●（ビーコン設置者が指定している）NG業種、NG企業に該当しているビーコンは選んではいけない\n●他のビーコングループで使っているビーコンは選ばない\n\n＜スケジュール＞\n\n3月31日まで：Determine the specification\n4月7日まで：develop in local\n4月14日まで：Connect with part developed by kray -san\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n######################################################################################\nMain page:\nIP address:   153.156.78.132\nipadress :    153.156.78.132\nipadress :    79.125.215.136\nipadress :    222.124.175.90\n\n\n\nUnerry IP:\n153.156.78.132\n122.208.202.84\n\n\n\n\nKevin:     122.208.202.84\nMakiko :   60.191.38.77\nuhciyama:  79.125.215.136\n\n\n\nipadress 126.147.24.155   Suzuki san\nipadress 139.162.111.98    Suzuki san\n\n\n\n\n\n\n\n#####################################################################################\n##### Google Cloud Linux Instance  #####\n104.198.81.156 \nSSH  \n\n\n\nAPI key 1:\nAIzaSyAmWX0-q2XOLGVD69-MQQSM-r8-9R1CWi4\n\n\nAPI key 2 :\nAIzaSyDd9YLAZxlnGriRJDKWLKOhre8-9drv-nw\n\n\n\n\n\n#### Install Anaconda ##############################################################\nwget https://repo.continuum.io/archive/Anaconda2-4.3.0-Linux-x86_64.sh\n\nbash Anaconda2-4.4.0-Linux-x86_64.sh\n\n/home/noel/anaconda2\n\n\n\n\n##### PostGres Install :   #########################################################\nhttps://cloud.google.com/solutions/set-up-postgres\n\n\nsudo apt-get install postgresql postgresql-contrib\n\nsudo apt-get install -y postgis postgresql-9.3-postgis-2.1\n\nVM Cloud GooglePostres BD:    postgres  elise237\n\nStatic Ip adress of VM:  35.187.217.100 : 5432 / postgres :   postgres\n\n\nUse PGadmin4 to create user/table in DB.   bbank\n  'NAME': 'bbank', #Your database name\n  'USER': 'db_readonly1',\n  'PASSWORD': 'kevinnoel',\n\n#### Install Postgres extension on Db\nsudo -s \n\nsudo -u postgres psql -c \"CREATE EXTENSION postgis; CREATE EXTENSION postgis_topology;\" bbank\n\nexit root mode :    exit\n\n\n\n\n########### SFTP  to Google VM engine  ########################################################\n\n\n4/wUHXSFpZNQwW4al5YU9eAHe59xFcW6oiPobdRG9EiC8  \n\n\n\n\nYou are now logged in as [noel@unerry.co.jp].\nYour current project is [protean-bus-157207].  You can change this setting by running:\n $ gcloud config set project PROJECT_ID\n\n\n\n###In local windows :\ngcloud compute --project  \"protean-bus-157207\"  ssh --zone  \"asia-northeast1-c\"  \"bengine\"\n\n\n####  In Local, Google Cloud SDK Shell :\n\t\tgcloud compute ssh \"bengine\"                [Name_of_Instance]\n\n \t\t\"asia-northeast1-c\"\n\n\n#### We can get local PPK files\n## In Gcloud fixing:   noel@unerry.co.jp\n\n\n\n\n##################################################################################################\nAuthenticating with public key \"KEVIN\\unerry01@kevin\"\n\nThe server's rsa2 key fingerprint is:\nssh-rsa 2048 29:56:dd:48:c1:54:e2:af:ab:4c:24:bf:b3:58:dd:a4\n\n\n##### VM server :\nnoel@\nunerry01@\n\n#################################################################################################\n\n\n\n\n#################################################################################################\nhttps://docs.google.com/document/d/1uuwWsPchzQJIbOij6KKaeZe9CS-UHF4M5b0MdXI0D8o/edit?_redirected\n\n\n\n\n\n###Local IP address of Unerry computer :\n122.208.202.84\n\n\n\n\n\n\n\n\n#################################################################################################\nhttps://gist.github.com/iamatypeofwalrus/5183133\n\nhttp://www.paulshapley.com/2016/04/how-to-install-postgresql-95-and.html\n\n\nhttp://www.saintsjd.com/2014/08/13/howto-install-postgis-on-ubuntu-trusty.html\n\n\n\n\n##################### Re-usable  ####################################################################\nhttps://docs.djangoproject.com/en/1.10/topics/settings/#calling-django-setup-is-required-for-standalone-django-usage\n\n\n\n\n\n\n\n\n\n##### Run VBOX Manage transform VBOX in         ####################################\nhttp://serverfault.com/questions/365423/how-to-run-vboxmanage-exe\n\nYou need to either use the whole path for the command:\n\n\"C:\\Program Files\\Oracle\\VirtualBox\\VBoxManage.exe\" list hdds\n...or cd to C:\\Program Files\\Oracle\\VirtualBox then:\n\nVBoxManage.exe\" list hdds\n...or you can add add the C:\\Program Files\\Oracle\\VirtualBox directory to your PATH :\n\nPATH=%PATH%;C:\\Program Files\\Oracle\\VirtualBox\n...and then you can run VBoxManage from anywhere\n\n\n\n\nHere goes the direct link for 2.2: \nhttp://download.virtualbox.org/virtualbox/2.2.0/VBoxGuestAdditions_2.2.0.iso\n\n\n\n\n\n\n\n\n\n\n#######################################################################################\nconda revision\n\n\n\n#### Conda issues:\nInvoking conda config --add channels r creates a .conda file for me then I modified it manually. Thx\nC:\\Users\\<username>) \n\n\n\n\nconda --version\n\nError: HTTPError: 403  Forbidden  http://repo.continuum.io/pkgs/pro/win-64/\n\n\n\n\n\n\n\n\n\n\n\n\n\n# -*- coding: utf-8 -*-\n#############################################################################################\n######  GOOGLE_APPLICATION_CREDENTIALS \nGo to Console and get API credential\n\nget the json in a folder\n\nSet ENV Var GOOGLE_APPLICATION_CREDENTIALS  to this folder  where json is\n\nand restart the whole.\n\n\n\n\n\n\n\n\n\n\n\n\n\nG:\\_dev\\anaconda27\\pythonw.exe \"G:/_dev/anaconda27/Scripts/spyder-script.py\"\n\n\n\"C:\\Users\\Public\\Documents\\Python Scripts\"\nG:\\_dev\\project27\n\n\n\n\n\nStackoverflow :\n\nhttps://stackoverflow.com/questions/41722003/handle-very-large-data-with-dictionnary-of-dataframes\n\n\n\nhttp://stackoverflow.com/users/7402489/deepmind27\n\n\nParallel processing\nhttps://stackoverflow.com/questions/41724972/use-sub-processes-for-parallel-computing-in-python\n\nhttp://ipyparallel.readthedocs.io/en/latest/multiengine.html?highlight=sync_imports#moving-python-objects-around\n\nhttps://pypi.python.org/pypi/distob\n\n\n\nhttp://www.thebiccountant.com/2016/04/09/hackpowerbi/\n\n\n__________________\n\n\n\n\n\n\nCACHE = {}\ndef distance(v1, v2):\n    k = id(v1), id(v2)\n    if k not in CACHE:\n        d = sum(v1[f] * v2.get(f,0) for f in v1) / (v1.l2 * v2.l2 or 1)\n        CACHE[k] = d\n        CACHE[id(v2), id(v1)] = d # symmetric\n    return CACHE[k]\n\n\n\n\n\n_____________________________________________________________\nstackoverlfow account:\nnoelkev0@gmail.com\ntomoko237.\n\n\nhttps://github.com/settings/admin\ntomoko237.\n\n\n\n\n\n\n\n\n\n\n##### Directly in bat  file   ##############################################################################################\nhttps://stackoverflow.com/questions/41642012/how-to-insert-python-code-in-a-bat-file\nAnother solution is, in one line is :\n\n@echo off & python -x \"%~f0\" %* & goto :eof\n\nprint \"Hello 123\"\nimport time; time.sleep(5)\n\n\n\n\n#### Windows Chrome config    ############################################################################################\nhttp://www.ghacks.net/2010/10/19/how-to-change-google-chromes-cache-location-and-size/\n\n\n########## Version Reference of Chrome : Cache data + Specific User Data\n\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\" --disk-cache-dir=\"F:\\chromecache\"  --user-data-dir=\"E:\\_apps\\chrome\"  --disk-cache-size=10000000\n\n\n########## Change Default Browser in Registry:\nHKEY_CLASSES_ROOT\\ChromeHTML\\shell\\open\\command\n\n\n\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"  --disk-cache-dir=\"F:\\chromecache\"   --user-data-dir=\"G:\\pc\\chromeuser\"  --disk-cache-size=10000000      -- \"%1\"\n\n\n\n\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"  --disk-cache-dir=\"G:\\pc\\chromecache\"   --user-data-dir=\"E:\\_apps\\chrome\"  --disk-cache-size=10000000      -- \"%1\"\n\n\n\n\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\"  --disk-cache-dir=\"G:\\pc\\chromecache\"   --user-data-dir=\"G:\\pc\\chromeuser\"  --disk-cache-size=10000000      -- \"%1\"\n\n\n\nVM (old) Data\n\n\nIt seems sales are until May 2016 (05 2016)\n20161215_OBPPC-2 \\ CCJCデータ\n20161215_OBBJP_2 \\ SKU別HOTCOLD別実績\n\n\n\n\n\n\n\n\n\n\n\n##### Function are slow in Python, use language expression  #####################################################################################\nhttps://doughellmann.com/blog/2012/11/12/the-performance-impact-of-using-dict-instead-of-in-cpython-2-7-2/\n\n\n\nw<.n hikoikiki_iujt_[e>\ta\"#&\t+9/8\n \t\n\n\n##########  Create starting Shortcut in Spyder   #####################################################################################################\nMake the changes in sites-package/spyderlib :\n\nspyderlib/start_app.py\n    def main(file_session=None): \n\n\nspyderlib/spyder.py\n\ndef main(file_session=None):\n    \"\"\"Session manager\"\"\"\n\n    next_session_name = options.startup_session\n    if file_session is not None :    next_session_name= file_session\n    while is_text_string(next_session_name):\n    \n\n\n#Shortcut Details :\n D:\\_devs\\Python01\\Anaconda27\\pythonw.exe \"D:/_devs/Python01/Anaconda27/Scripts/spyder-script.py\" \n \"E:\\documents\\Python Scripts\"\n\n\n#spyder-script.py\nfrom spyderlib import start_app\nmain1= start_app.main('D:/_devs/Python01/project27/test_folio_elvis_test01_.session.tar')\n\n\n\n\n\n##############  PostGres database  ################################################################################################################\n\npostges\n4096\n\n\nhttp://localhost:8050/#/details-pg/pg96\n\n\nlocalhost:5432\n\n\n\n''' Memory test for Pandas Data\n1.7go csv (27764675, 11)--> 2.6Go in RAM dataframe,   \n25Go RAM -->  250mio x 11 col\ncannot use numpy --> Crash !!!\n\n\n1.7go csv (27764675, 11)---> 1.7go in hdf format  (to_hdf )  VERY GOOD,  NO ISSUE\n\n1.7go cs ---> HDFStore  (store but many issues of encoding over 3Go)\n\n1.7go CSV --->  Pandas pkl\n\n\nBAD:\n1.7go csv ---> HDFStore  (store but many issues of encoding over 3Go),\n\n\n\n'''\n\n\n\n\n\n\n\n\n\n\n\n\n\n##############  SVN Repository:   #########################################################################################################################\n\n\n1) Central repository : in Google Drive backup\n   Create repo in Google Drive Backup \n  ---> Create SVN under Server to prevent Deletetion \n\n\n2) Right Click,  Import Folder (where we want to monitor)\n  ...or by project:\n\n/paint/trunk\n/paint/branches\n/paint/tags\n/calc/trunk\n/calc/branches\n/calc/tags\n\n---> Put the folder into the Repository\n\n\n\n3) \nChecking out a Working Copy  (Copy Repository ---->   Local Folder)\n\nNow that we have a project in our repository, we need to create a working copy to use for day-to-day work. Note that the act of importing a folder does not automatically turn that folder into a working copy. The Subversion term for creating a fresh working copy is Checkout. We are going to checkout the Widget1 folder of our repository into a development folder on the PC called C:\\Projects\\Widget1-Dev. Create that folder, then right click on it and select TortoiseSVN ? Checkout.... Then enter the URL to checkout, in this case file:///c:/svn_repos/trunk/Widget1 and click on OK. Our development folder is then populated with files from the repository.\n\n\n\n\n4) Right Click and many options appears :\n   Update :             Means the folder will be erased by new version from the repositor\n   Update to version :  Update up to the version xxxx\n\n   Commit means :  Local Folder to Central Folder\n\n\nViewing the Project History\nOne of the most useful features of TortoiseSVN is the Log dialog. This shows you a list of all the commits you made to a file or folder, and shows those detailed commit messages that you entered (you did enter a commit message as suggested? If not, now you see why this is important).\n\n\n\n5)  Merge\n\nhttp://stackoverflow.com/questions/1057734/tortoisesvn-icons-not-showing-up-under-windows-7\n\n########################################################################################################\n########### Conda\n\nconda list --revisions                See all the packages\n\n\n\n#####################   Local Install :\n\n\n2016-10-17 00:39:34  (rev 16)\n    +mlxtend-0.4.2 (rasbt)\n\n2016-10-23 03:43:33  (rev 17)\n     conda  {4.2.9 -> 4.2.9 (anaconda)}\n     conda-env  {2.6.0 -> 2.6.0 (anaconda)}\n    +orange-2.7.8 (anaconda)\n\n2016-10-31 05:03:20  (rev 18)\n     conda  {4.2.9 (anaconda) -> 4.1.12 (conda-forge)}\n     conda-env  {2.6.0 (anaconda) -> 2.5.2 (conda-forge)}\n     dask  {0.10.0 -> 0.11.1 (conda-forge)}\n    +boto3-1.4.0\n    +botocore-1.4.49 (conda-forge)\n    +distributed-1.12.1 (conda-forge)\n    +jmespath-0.9.0 (conda-forge)\n    +msgpack-python-0.4.7\n    +s3fs-0.0.6 (conda-forge)\n    +s3transfer-0.1.7 (conda-forge)\n    +tblib-1.3.0\n\n2016-10-31 17:27:09  (rev 19)\n     conda  {4.1.12 (conda-forge) -> 4.2.11}\n     conda-env  {2.5.2 (conda-forge) -> 2.6.0}\n    +fastcluster-1.1.20 (omnia)\n\n2016-10-31 17:41:33  (rev 20)\n     fastcluster  {1.1.20 (omnia) -> 1.1.20 (fgregg)}\n\n2016-10-31 18:00:57  (rev 21)\n    -fastcluster-1.1.20 (fgregg)\n\n2016-10-31 18:05:07  (rev 22)\n    +fastcluster-1.1.20 (omnia)\n\n2016-11-01 01:47:16  (rev 23)\n     conda  {4.2.11 -> 4.1.12 (conda-forge)}\n     conda-env  {2.6.0 -> 2.5.2 (conda-forge)}\n    +category_encoders-1.2.2 (conda-forge)\n\n2016-11-02 21:48:52  (rev 24)\n     blaze  {0.10.1 -> 0.11.2 (blaze)}\n     conda  {4.1.12 (conda-forge) -> 4.2.11}\n     conda-env  {2.5.2 (conda-forge) -> 2.6.0}\n     datashape  {0.5.2 -> 0.5.3 (blaze)}\n     psutil  {4.3.0 -> 4.0.0}\n\n2016-11-06 17:15:13  (rev 25)\n     conda  {4.2.11 -> 4.2.12 (anaconda)}\n     conda-env  {2.6.0 -> 2.6.0 (anaconda)}\n    +javabridge-1.0.14 (anaconda)\n\n2016-11-30 11:22:13  (rev 26)\n     conda  {4.2.12 (anaconda) -> 4.2.13 (conda-forge)}\n     conda-env  {2.6.0 (anaconda) -> 2.6.0 (conda-forge)}\n    +hdbscan-0.8.3 (conda-forge)\n\n2016-11-30 11:26:21  (rev 27)\n     conda  {4.2.13 (conda-forge) -> 4.2.13 (anaconda)}\n     conda-env  {2.6.0 (conda-forge) -> 2.6.0 (anaconda)}\n    +seaborn-0.7.1 (anaconda)\n\n2016-11-30 15:54:32  (rev 28)\n     conda  {4.2.13 (anaconda) -> 4.2.13 (conda-forge)}\n     conda-env  {2.6.0 (anaconda) -> 2.6.0 (conda-forge)}\n    +amqp-1.4.9 (conda-forge)\n    +anyjson-0.3.3 (conda-forge)\n    +billiard-3.3.0.23 (conda-forge)\n    +celery-3.1.23 (conda-forge)\n    +kombu-3.0.35 (conda-forge)\n\n2016-12-07 14:38:52  (rev 29)\n     conda  {4.2.13 (conda-forge) -> 4.2.13 (anaconda)}\n     conda-env  {2.6.0 (conda-forge) -> 2.6.0 (anaconda)}\n    +bcolz-1.0.0 (anaconda)\n\n2016-12-11 10:45:56  (rev 30)\n    +pandasql-0.7.3 (anaconda)\n\n2016-12-11 15:01:33  (rev 31)\n     conda  {4.2.13 (anaconda) -> 4.2.13 (conda-forge)}\n     conda-env  {2.6.0 (anaconda) -> 2.6.0 (conda-forge)}\n    +mplleaflet-0.0.5 (conda-forge)\n\n2016-12-20 02:57:44  (rev 32)\n     conda  {4.2.13 (conda-forge) -> 4.2.13}\n     conda-env  {2.6.0 (conda-forge) -> 2.6.0}\n     scikit-learn  {0.17.1 -> 0.18.1}\n\n2016-12-21 11:11:08  (rev 33)\n     qtpy  {1.0.2 -> 1.1.2}\n     spyder  {2.3.9 -> 3.0.2}\n    +astroid-1.4.7\n    +lazy-object-proxy-1.2.1\n    +pylint-1.5.4\n    +qtawesome-0.3.3\n    +wrapt-1.10.8\n\n2016-12-21 13:46:21  (rev 34)\n     qtpy  {1.1.2 -> 1.0.2}\n     spyder  {3.0.2 -> 2.3.9}\n    -astroid-1.4.7\n    -lazy-object-proxy-1.2.1\n    -pylint-1.5.4\n    -qtawesome-0.3.3\n    -wrapt-1.10.8\n\n2016-12-21 17:16:27  (rev 35)\n     conda  {4.2.13 -> 4.2.13 (conda-forge)}\n     conda-env  {2.6.0 -> 2.6.0 (conda-forge)}\n     notebook  {4.2.1 -> 4.2.3 (conda-forge)}\n    +ipyparallel-5.2.0 (conda-forge)\n\n\n\n\n\n\nAnaconda shortcurt\n\n\nD:\\_devs\\Python01\\Anaconda27\\pythonw.exe \"D:/_devs/Python01/Anaconda27/Scripts/spyder-script.py\"\n\n\nD:\\_devs\\Python01\\Anaconda27\\pythonw.exe D:\\_devs\\Python01\\Anaconda27\\cwp.py D:\\_devs\\Python01\\Anaconda27 \"D:/_devs/Python01/Anaconda27/pythonw.exe\" \"D:/_devs/Python01/Anaconda27/Scripts/spyder-script.py\"\n\n\n\nD:\\_devs\\Python01\\Anaconda27\\pythonw.exe D:\\_devs\\Python01\\Anaconda27\\cwp.py D:\\_devs\\Python01\\Anaconda27 \"D:/_devs/Python01/Anaconda27/pythonw.exe\" \"D:/_devs/Python01/Anaconda27/Scripts/spyder-script.py\"\n\n\n\n\n\n\n\n\n\n\n\n\nC:\\Users\\asus1>conda install --revisions 35\nusage: conda-script.py [-h] [-V] command ...\nconda-script.py: error: unrecognized arguments: --revisions\n\nC:\\Users\\asus1>conda install --revision 35\nFetching package metadata .........\n.\n# All requested packages already installed.\n# packages in environment at D:\\_devs\\Python01\\Anaconda27:\n#\n\n\nC:\\Users\\asus1>conda install --revision 34\nFetching package metadata .........\n.\nPackage plan for installation in environment D:\\_devs\\Python01\\Anaconda27:\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    conda-env-2.6.0            |                0          498 B\n    conda-4.2.13               |           py27_0         455 KB\n    ------------------------------------------------------------\n                                           Total:         455 KB\n\nThe following packages will be REMOVED:\n\n    ipyparallel: 5.2.0-py27_1  conda-forge\n\nThe following packages will be SUPERCEDED by a higher-priority channel:\n\n    conda:       4.2.13-py27_0 conda-forge --> 4.2.13-py27_0\n    conda-env:   2.6.0-0       conda-forge --> 2.6.0-0\n    notebook:    4.2.3-py27_0  conda-forge --> 4.2.1-py27_0\n\nProceed ([y]/n)? y\n\nPruning fetched packages from the cache ...\nFetching packages ...\nconda-env-2.6. 100% |###############################| Time: 0:00:00   0.00  B/s\nconda-4.2.13-p 100% |###############################| Time: 0:00:00   1.19 MB/s\nExtracting packages ...\n[      COMPLETE      ]|##################################################| 100%\nUnlinking packages ...\n[      COMPLETE      ]|##################################################| 100%\nLinking packages ...\n[      COMPLETE      ]|##################################################| 100%\n\n\n\n\nWhen runnig this example, we add parallel functions:\n\nVersion 1: this example, works ok.\nhttp://deap.readthedocs.io/en/master/examples/gp_symbreg.html\n\n\nVersion2 Using parallel map in toolbox.register()\n\n```\n#Using Parallell Processing\n# Example of map : http://ipyparallel.readthedocs.io/en/latest/asyncresult.html?highlight=sleep_here\nimport ipyparallel as ipp\nrclient= ipp.Client()\npool = rclient.load_balanced_view()\ntoolbox.register(\"map\", pool.map_sync)\n```\n\nWe get this error message:\n\n\n```\nAttributeErrorTraceback (most recent call last)d:\\_devs\\python01\\anaconda27\\lib\\site-packages\\ipyparallel\\serialize\\serialize.pyc in unpack_apply_message(bufs, g, copy)\n    192     args = []\n    193     for i in range(info['nargs']):\n--> 194         arg, arg_bufs = deserialize_object(arg_bufs, g)\n    195         args.append(arg)\n    196     args = tuple(args)\n\\lib\\site-packages\\ipyparallel\\serialize\\serialize.pyc in deserialize_object(buffers, g)\n    130     bufs = list(buffers)\n    131     pobj = buffer_to_bytes_py2(bufs.pop(0))\n--> 132     canned = pickle.loads(pobj)\n    133     if istype(canned, sequence_types) and len(canned) < MAX_ITEMS:\n    134         for c in canned:\nAttributeError: 'DummyMod' object has no attribute 'evalSymbReg'\n\n```\n\n#### Test code if Clusters are working or Not :\nimport ipyparallel as ipp,  time\nt0 = time.time()\ndef sleep_here(t):\n    time.sleep(t)\n    return id,t\n\n# create client & view\nrclient = ipp.Client()\ndview = rc[:]\n# scatter 'id', so id=0,1,2 on engines 0,1,2\ndview.scatter('idworker', rclient.ids, flatten=True)\nprint(\"Engine IDs: \", dview['idworker'])\n\n\nprint(\"Engine IDs (2n way): \", rc [:]['id'])\nlview = rclient.load_balanced_view()    # Create Load Balancer for the ec2/worker\n\n\nprint(\"running with one call per task\")\ntask_map = lview.map(sleep_here, [.01*t for t in range(10)])\nfor i, taski in enumerate(task_map):\n    print('Task', i, 'Worker', taski[0],'Result',  taski)\n\n\n\n\n##############################  Error Pickle  ######################################\n\n[0:apply]: \nAttributeErrorTraceback (most recent call last)d:\\_devs\\python01\\anaconda27\\lib\\site-packages\\ipyparallel\\serialize\\serialize.pyc in unpack_apply_message(bufs, g, copy)\n    192     args = []\n    193     for i in range(info['nargs']):\n--> 194         arg, arg_bufs = deserialize_object(arg_bufs, g)\n    195         args.append(arg)\n    196     args = tuple(args)\nd:\\_devs\\python01\\anaconda27\\lib\\site-packages\\ipyparallel\\serialize\\serialize.pyc in deserialize_object(buffers, g)\n    130     bufs = list(buffers)\n    131     pobj = buffer_to_bytes_py2(bufs.pop(0))\n--> 132     canned = pickle.loads(pobj)\n    133     if istype(canned, sequence_types) and len(canned) < MAX_ITEMS:\n    134         for c in canned:\nd:\\_devs\\python01\\anaconda27\\lib\\site-packages\\dill\\dill.pyc in loads(str)\n    258     \"\"\"unpickle an object from a string\"\"\"\n    259     file = StringIO(str)\n--> 260     return load(file)\n    261 \n    262 # def dumpzs(obj, protocol=None):\nd:\\_devs\\python01\\anaconda27\\lib\\site-packages\\dill\\dill.pyc in load(file)\n    248     pik = Unpickler(file)\n    249     pik._main = _main_module\n--> 250     obj = pik.load()\n    251     if type(obj).__module__ == _main_module.__name__: # point obj class to main\n    252         try: obj.__class__ == getattr(pik._main, type(obj).__name__)\nd:\\_devs\\python01\\anaconda27\\lib\\pickle.pyc in load(self)\n    862             while 1:\n    863                 key = read(1)\n--> 864                 dispatch[key](self)\n    865         except _Stop, stopinst:\n    866             return stopinst.value\nd:\\_devs\\python01\\anaconda27\\lib\\pickle.pyc in load_global(self)\n   1094         module = self.readline()[:-1]\n   1095         name = self.readline()[:-1]\n-> 1096         klass = self.find_class(module, name)\n   1097         self.append(klass)\n   1098     dispatch[GLOBAL] = load_global\nd:\\_devs\\python01\\anaconda27\\lib\\site-packages\\dill\\dill.pyc in find_class(self, module, name)\n    404         elif (module, name) == ('__builtin__', 'NoneType'):\n    405             return type(None) #XXX: special case: NoneType missing\n--> 406         return StockUnpickler.find_class(self, module, name)\n    407 \n    408     def __init__(self, *args, **kwds):\nd:\\_devs\\python01\\anaconda27\\lib\\pickle.pyc in find_class(self, module, name)\n   1130         __import__(module)\n   1131         mod = sys.modules[module]\n-> 1132         klass = getattr(mod, name)\n   1133         return klass\n   1134 \nAttributeError: 'module' object has no attribute 'Individual'\n\n\n\n\n\n\n####  175\nselect g0.beacon_id, g0.account_id,\n       g0.application_id, app_user_id,  g3.group_id, h2.dau_price, g3.group_type\n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select   g5.id as beacon_id, install_loc_cat1id, install_loc_cat2id,            \n          from   beaconbank.BB_beacon as g5         \n           JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n           JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n           JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id         \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      JOIN \n       (  select  dau_price,  beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\n\n      and g3.group_id > 0 \n   \n\n\n\n\n■ログ反応数　1,276件\nSELECT count(*) FROM `BB_beaconlog` \nWHERE application_id=\"2170005\" and detected_time LIKE \"2017%\" and event=\"0\"\n\n\n\n\n\n\n■ユーザー数　16件\nSELECT count(distinct(app_user_id)) FROM `BB_beaconlog` WHERE application_id=\"2170005\" and detected_time LIKE \"2017%\"\n\n\n■利用ビーコン反応数　約556件\nSELECT count(*) FROM `BB_beaconlog` WHERE application_id=\"2170005\" and detected_time LIKE \"2017%\" and event=\"0\"\nand beacon_id in (SELECT distinct(beacon_id) FROM BB_group_beacon WHERE group_id in (SELECT distinct(id) FROM `BB_group` WHERE application_id=\"2170005\"))\n※ 正確には、BB_group_beaconの有効期間ごとにログを取得する必要あり\n\n■ＣＶビーコン反応数　約289件\nSELECT count(*) FROM `BB_beaconlog` WHERE application_id=\"2170005\" and detected_time LIKE \"2017%\" and event=\"0\"\nand beacon_id in (SELECT distinct(beacon_id) FROM BB_group_beacon WHERE group_id in (SELECT distinct(id) FROM `BB_group` WHERE application_id=\"2170005\" and group_type=\"3\"))\n\n\n\n\n#### Becoming root on GCP\nsudo su\n\n\n\n\n\n\n\nhttps://t-biz-analytics.beaconbank.jp/page_bi/a027_cva/?pagename=a027_cva&account_id=4140002&application_id=2170005&group_id=8140001&startdate=20170531&enddate=20170606&uurid=07d86dd9fdfb02d711078877e01bc48ecf51dfdfca610e070509ac64b7403343275cba8af5b83ea5e760591524e3d41e\n\n\n#### Details\nhttps://t-biz-analytics.beaconbank.jp/page_bi/a027_area_map?pagename=a027_area_map&account_id=4140002&application_id=2170005&group_id=8140001&startdate=20170531&enddate=20170606&uurid=07\n\n\n\n\n\n\n\nimport paramiko\nk = paramiko.RSAKey.from_private_key_file(\"/Users/whatever/Downloads/mykey.pem\")\nc = paramiko.SSHClient()\nc.set_missing_host_key_policy(paramiko.AutoAddPolicy())\nprint \"connecting\"\nc.connect( hostname = \"www.acme.com\", username = \"ubuntu\", pkey = k )\nprint \"connected\"\ncommands = [ \"/home/ubuntu/firstscript.sh\", \"/home/ubuntu/secondscript.sh\" ]\nfor command in commands:\n\tprint \"Executing {}\".format( command )\n\tstdin , stdout, stderr = c.exec_command(command)\n\tprint stdout.read()\n\tprint( \"Errors\")\n\tprint stderr.read()\nc.close()\n\n\n\n\n#### 91\nselect \n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select   g5.id as beacon_id, install_loc_cat1id, install_loc_cat2id,\n                  m5.name as install_loc_cat2id_name, \n                  m6.name as install_loc_cat1id_name\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id         \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\n       \n\n\n\n\nselect g0.beacon_id, g0.account_id,  g0.application_id, app_user_id, g0.app_user_id\n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select  g5.id as beacon_id,  install_loc_cat1id, install_loc_cat2id,\n                  m5.name as install_loc_cat2id_name, \n                  m6.name as install_loc_cat1id_name, min(g5.id)\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id    \n\n         GROUP BY  beacon_id, install_loc_cat1id, install_loc_cat2id,install_loc_cat2id_name,  install_loc_cat1id_name    \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\n       \n   \n\n   \n\n\n#############################################\n#### OK   92 \n\nselect g0.detected_time, g0.beacon_id, g0.account_id,  g0.application_id, app_user_id, g3.group_id, group_type, name,event, \n       h2.dau_price,   min(g0.beacon_id)\n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select  g5.id as beacon_id,  install_loc_cat1id, install_loc_cat2id,\n                  m5.name as install_loc_cat2id_name, \n                  m6.name as install_loc_cat1id_name, min(g5.id)\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id    \n\n         GROUP BY  beacon_id, install_loc_cat1id, install_loc_cat2id,install_loc_cat2id_name,  install_loc_cat1id_name    \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      LEFT JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\nGROUP BY g0.detected_time, g0.beacon_id, g0.account_id,  g0.application_id, app_user_id, g3.group_id, group_type, name,event, h2.dau_price\n   \n\n#########################################\n\n\n\n\n\n\n############# OK: 102\nselect g0.detected_time, g0.beacon_id, g0.account_id,  g0.application_id, app_user_id, g3.group_id, group_type, name,event, \n       h2.dau_price\n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select  g5.id as beacon_id,  install_loc_cat1id, install_loc_cat2id,\n                  m5.name as install_loc_cat2id_name, \n                  m6.name as install_loc_cat1id_name, min(g5.id)\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id    \n\n         GROUP BY  beacon_id, install_loc_cat1id, install_loc_cat2id,install_loc_cat2id_name,  install_loc_cat1id_name    \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\n\n\n\n#############################################################################################################\n#### 294 \n\nselect g0.detected_time, g0.beacon_id, g0.account_id,  g0.application_id, app_user_id, g3.group_id, group_type, name,event, \n       h2.dau_price\n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select  g5.id as beacon_id,  install_loc_cat1id, install_loc_cat2id,\n                  m5.name as install_loc_cat2id_name, \n                  m6.name as install_loc_cat1id_name, min(g5.id)\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id    \n\n         GROUP BY  beacon_id, install_loc_cat1id, install_loc_cat2id,install_loc_cat2id_name,  install_loc_cat1id_name    \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\n\n\n\n\n\n\n\n####### 294\nselect g0.detected_time, g0.beacon_id, g0.account_id,  g0.application_id, app_user_id, g3.group_id, group_type, name,event, \n       h2.dau_price\n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select  g5.id as beacon_id,  install_loc_cat1id, install_loc_cat2id,\n                  m5.name as install_loc_cat2id_name, \n                  m6.name as install_loc_cat1id_name, min(g5.id)\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id    \n\n         GROUP BY  beacon_id, install_loc_cat1id, install_loc_cat2id,install_loc_cat2id_name,  install_loc_cat1id_name    \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\n\n\n\n#####  261 \nselect count(g0.detected_time)\n       from beaconbank.BB_beaconlog20170525 as g0\nwhere g0.application_id=2170005   and event=0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n         \n######################################################################################################\nselect application_id as app_id, account_id,   event, notified,  \n     group_id, group_name, group_type,   valid_from,  valid_to,\n     \n     manufacturer, model, os, sdk, \n     dau_price, daily_price, price_type,\n     \n     beacon_id,  address_prefecture, address_city,\n     address_detail, building_name,\n     install_category,is_fixed, is_outdoor,\n     is_public_area, install_loc_cat1id_name as install_loc_cat1id , install_loc_cat2id_name as install_loc_cat2id,\n     install_loc_name,  latitude as lat, longitude as lng,\n     \n     count(app_user_id) as n_hanoulog,  count(DISTINCT app_user_id) as n_user \n\nfrom \n     (\n    select g0.beacon_id, wildcard_beacon_id, g0.account_id,\n           g0.application_id, app_user_id, event, notified, \n           manufacturer, model, os, sdk,\n           \n           h1.latitude, h1.longitude, \n           h2.group_id, name as group_name, group_type,   valid_from,  valid_to,\n           dau_price, daily_price, price_type,\n           address_prefecture, address_city,\n           address_detail, building_name, \n           install_category, is_fixed, is_outdoor,\n           is_public_area, install_loc_cat1id, install_loc_cat2id,\n           install_loc_name, install_loc_detail, install_memo, install_loc_cat2id_name,\n            install_loc_cat1id_name\n       \n       from beaconbank.BB_beaconlog20170525 as g0\n     LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             CAST(FORMAT_DATE(\"%Y%m%d\", valid_form) as INT64 ) as valid_from, CAST(FORMAT_DATE(\"%Y%m%d\", valid_to) as INT64 ) as valid_to\n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id, valid_from, valid_to \n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n     \n     JOIN\n        ( select   g5.id as beacon_id, latitude, longitude, \n              address_prefecture, address_city,\n              address_detail, building_name, building_floor,\n              install_category, power_supply, is_fixed, is_outdoor,\n              is_public_area, install_loc_cat1id, install_loc_cat2id,\n              install_loc_name, install_loc_detail, install_memo,\n              m5.name as install_loc_cat2id_name, \n              m6.name as install_loc_cat1id_name\n         \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id  \n        \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n     \n     JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2      ON   g0.beacon_id= h2.beacon_id\n\n)\n\nwhere valid_to >= 20170301 and valid_from <= 20210501\n      and event=0\ngroup by  application_id, account_id,\n     event, notified,  \n     group_id, group_name, group_type,   valid_from,  valid_to,\n     \n     manufacturer, model, os, sdk,\n     dau_price, daily_price, price_type,\n     beacon_id,        \n     address_prefecture, address_city,\n     address_detail, building_name,\n     install_category,  is_fixed, is_outdoor,\n     is_public_area, install_loc_cat1id, install_loc_cat2id,\n     install_loc_name,  lat, lng\norder by application_id asc, group_type, group_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nselect g0.beacon_id, g0.account_id,\n           g0.application_id, app_user_id\n\n       from beaconbank.BB_beaconlog20170525 as g0\n       \n       LEFT JOIN  \n       (  select account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON  g1.group_id= g2.id \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n      JOIN\n        ( select   g5.id as beacon_id, install_loc_cat1id, install_loc_cat2id,\n                  m5.name as install_loc_cat2id_name, \n                  m6.name as install_loc_cat1id_name\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id         \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n      JOIN \n       (  select  dau_price, daily_price, price_type, beacon_id, group_id, max(group_id)\n          from   beaconbank.beacon_prices\n          GROUP BY  dau_price, daily_price, price_type, beacon_id, group_id \n       )  as h2     ON   g0.beacon_id= h2.beacon_id\n\nwhere g0.application_id=2170005   and event=0\n       \n   \n\n   \n\n##### SQL for page Area with Map\n\n       \nselect application_id as app_id, account_id,   event, app_use_detected, \n       group_id, group_name, group_type, \n     \n       beacon_id,  address_prefecture, address_city,\n       address_detail, \n       install_loc_cat1id_name as install_loc_cat1id , install_loc_cat2id_name as install_loc_cat2id,\n       latitude as lat, longitude as lng,\n     \n       count(app_user_id) as n_hanoulog,  count(DISTINCT app_user_id) as n_user \n\nfrom \n     (\n     select g0.beacon_id, g0.account_id,\n           g0.application_id, app_user_id, event, app_use_detected, \n           h1.latitude, h1.longitude, \n           h2.group_id, name as group_name, group_type, \n           address_prefecture, address_city,  address_detail, \n           install_loc_cat2id_name, install_loc_cat1id_name, min(g0.beacon_id)\n     \n       from  `beaconbank.BB_beaconlog*`     as g0\n     LEFT JOIN  \n       (  select account_id, application_id, name, group_type,  group_id, beacon_id\n\n          from (\n           select  account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n                   CAST(FORMAT_DATE(\"%Y%m%d\", valid_form) as INT64 ) as valid_from, CAST(FORMAT_DATE(\"%Y%m%d\", valid_to) as INT64 ) as valid_to1\n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON     g1.group_id= g2.id \n             wHERE  g1.is_latest=1   AND  g1.group_type= 1   \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id, valid_from, valid_to1\n            )\n          where valid_to1 >= \"\"\"+ t0 + \"\"\" and valid_from <= \"\"\" + t0 + \"\"\"\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n     LEFT JOIN\n        ( select   g5.id as beacon_id, latitude, longitude, \n              address_prefecture, address_city,   address_detail, \n              install_loc_cat1id, install_loc_cat2id,\n              m5.name as install_loc_cat2id_name, \n              m6.name as install_loc_cat1id_name, min(is_fixed)\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id  \n                \n         GROUP BY beacon_id, latitude, longitude, \n              address_prefecture, address_city,   address_detail, \n              install_loc_cat1id, install_loc_cat2id,\n              install_loc_cat2id_name,    install_loc_cat1id_name           \n        )     as h1    ON  g0.beacon_id= h1.beacon_id\n               \nWHERE _TABLE_SUFFIX BETWEEN '\"\"\"+ta+\"\"\"' AND '\"\"\"+tb+\"\"\"'\nAND  CAST(FORMAT_DATETIME(\"%Y%m%d\", local_time) as INT64 )= \"\"\"+t0+\"\"\"\n\n\nGROUP BY g0.beacon_id, g0.account_id, detected_time,\n           g0.application_id, app_user_id, event, app_use_detected, \n           \n           h1.latitude, h1.longitude, \n           h2.group_id, group_name, group_type,\n\n           address_prefecture, address_city,\n           address_detail,  install_loc_cat1id, install_loc_cat2id,\n           install_loc_cat2id_name, install_loc_cat1id_name\n)\n \nwhere  event=0\n\ngroup by  application_id, account_id,\n     event, app_use_detected,  \n     group_id, group_name, group_type, \n     beacon_id, address_prefecture, address_city,\n     address_detail,  install_loc_cat1id, install_loc_cat2id,\n     lat, lng\norder by application_id asc, group_type, group_id   \n\n\n\n\n\nPython \nType \"copyright\", \"credits\" or \"license\" for more information.\n\nIPython  -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\n\nIn [1]: \n\nIn [1]: dfstat= util.load(   HOME + '/data/count_table/20170702_2/dfstat.pkl' )\n   ...: \nTraceback (most recent call last):\n\n  File \"<ipython-input-1-d9ac68688699>\", line 1, in <module>\n    dfstat= util.load(   HOME + '/data/count_table/20170702_2/dfstat.pkl' )\n\nNameError: name 'util' is not defined\n\n\nIn [2]: import util\n   ...: dfstat= util.load(   HOME + '/data/count_table/20170702_2/dfstat.pkl' )\n   ...: \n   ...: \nTraceback (most recent call last):\n\n  File \"<ipython-input-2-eb37bb0cc5e6>\", line 2, in <module>\n    dfstat= util.load(   HOME + '/data/count_table/20170702_2/dfstat.pkl' )\n\nNameError: name 'HOME' is not defined\n\n\nIn [3]: \"\"\"  bbank  \"\"\"\n   ...: # %load_ext autoreload\n   ...: # %autoreload\n   ...: import os, sys\n   ...: DIRCWD=  'D:/_devs/project27/'  if  os.environ['COMPUTERNAME']=='ASUS1-PC' and sys.platform.find('win')> -1 else  'G:/_devs/project27/' if sys.platform.find('win')> -1   and  os.environ['COMPUTERNAME']=='KEVIN'   else  '/home/noel/project27/' if os.environ['HOME'].find('ubuntu')>-1 else '/home/noel/project27/'\n   ...: \n   ...: # DIRCWD=   '/home/noel/project27/'\n   ...: \n   ...: \n   ...: DIRCWD= r\"G:/_devs/google_cloud/home/noel/project27/\"\n   ...: \n   ...: \n   ...: os.chdir(DIRCWD); sys.path.append(DIRCWD + '/aapackage'); # sys.path.append(DIRCWD + '/linux/aapackage')\n   ...: # execfile( DIRCWD + '/aapackage/allmodule.py')\n   ...: import  pandas as pd, sqlalchemy as sql,  numpy as np, gc,  arrow\n   ...: from attrdict import AttrDict as dict2; from collections import defaultdict  as dict1\n   ...: \n   ...: import util\n   ...: ############################################################################################\n   ...: HOME= 'D:/_devs/google_cloud/home/noel/project27/' if  os.environ['COMPUTERNAME']=='ASUS1-PC' else  '/home/noel/project27/'  if sys.platform.find('lin')> -1  else 'G:/_devs/google_cloud/home/noel/project27/'\n   ...: # HOME=   '/home/noel/project27/'\n   ...: \n   ...: \n   ...: HOME= r\"G:/_devs/google_cloud/home/noel/project27/\"\n   ...: \n   ...: \n\nIn [4]: import util\n   ...: dfstat= util.load(   HOME + '/data/count_table/20170702_2/dfstat.pkl' )\n   ...: \nTraceback (most recent call last):\n\n  File \"<ipython-input-4-8d0fe8fd90ed>\", line 2, in <module>\n    dfstat= util.load(   HOME + '/data/count_table/20170702_2/dfstat.pkl' )\n\n  File \"G:\\_devs\\project27\\aapackage\\util.py\", line 1093, in load\n    return py_load_obj(folder=folder, isabsolutpath=isabsolutpath)\n\n  File \"G:\\_devs\\project27\\aapackage\\util.py\", line 1127, in py_load_obj\n    with open(dir1, 'rb') as f:\n\nIOError: [Errno 2] No such file or directory: 'G:/_devs/google_cloud/home/noel/project27//data/count_table/20170702_2/dfstat.pkl'\n\n\nIn [5]: import util\n   ...: dfstat= util.load(   HOME + '/data/count_table/dfstat.pkl' )\n   ...: \n   ...: \n\nIn [6]: ss= \"\"\"       \n   ...: select application_id as app_id, account_id,   event, app_use_detected, \n   ...:        group_id, group_name, group_type, \n   ...:        \n   ...:        beacon_id,  address_prefecture, address_city,\n   ...:        address_detail, \n   ...:        install_loc_cat1id_name as install_loc_cat1id , install_loc_cat2id_name as install_loc_cat2id,\n   ...:        latitude as lat, longitude as lng,\n   ...:        \n   ...:        count(app_user_id) as n_hanoulog,  count(DISTINCT app_user_id) as n_user \n   ...: \n   ...: from \n   ...:      (\n   ...:      select g0.beacon_id, g0.account_id,\n   ...:            g0.application_id, app_user_id, event, app_use_detected, \n   ...:            h1.latitude, h1.longitude, \n   ...:            h2.group_id, name as group_name, group_type, \n   ...:            address_prefecture, address_city,  address_detail, \n   ...:            install_loc_cat2id_name, install_loc_cat1id_name, min(g0.beacon_id)\n   ...:        \n   ...:        from  `beaconbank.BB_beaconlog*`     as g0\n   ...:      LEFT JOIN  \n   ...:        (  select account_id, application_id, name, group_type,  group_id, beacon_id\n   ...:           \n   ...:           from (\n   ...:            select  account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n   ...:                    CAST(FORMAT_DATE(\"%Y%m%d\", valid_form) as INT64 ) as valid_from, CAST(FORMAT_DATE(\"%Y%m%d\", valid_to) as INT64 ) as valid_to1\n   ...:              from beaconbank.BB_group_beacon as g1  \n   ...:              JOIN beaconbank.BB_group as g2 \n   ...:              ON     g1.group_id= g2.id \n   ...:              wHERE  g1.is_latest=1   AND  g1.group_type= 1   \n   ...:              GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id, valid_from, valid_to1\n   ...:             )\n   ...:           where valid_to1 >= \"\"\"+ t0 + \"\"\" and valid_from <= \"\"\" + t0 + \"\"\"\n   ...:        )  as g3   ON    g0.beacon_id=       g3.beacon_id\n   ...:                   AND   g0.application_id=  g3.application_id\n   ...:                   AND   g0.account_id=      g3.account_id\n   ...:      \n   ...:      LEFT JOIN\n   ...:         ( select   g5.id as beacon_id, latitude, longitude, \n   ...:               address_prefecture, address_city,   address_detail, \n   ...:               install_loc_cat1id, install_loc_cat2id,\n   ...:               m5.name as install_loc_cat2id_name, \n   ...:               m6.name as install_loc_cat1id_name, min(is_fixed)\n   ...:          \n   ...:          from   beaconbank.BB_beacon as g5         \n   ...:          JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n   ...:          JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n   ...:          JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id  \n   ...:          \n   ...:          GROUP BY beacon_id, latitude, longitude, \n   ...:               address_prefecture, address_city,   address_detail, \n   ...:               install_loc_cat1id, install_loc_cat2id,\n   ...:               install_loc_cat2id_name,    install_loc_cat1id_name           \n   ...:         )     as h1    ON  g0.beacon_id= h1.beacon_id\n   ...: \n   ...: WHERE _TABLE_SUFFIX BETWEEN '\"\"\"+ta+\"\"\"' AND '\"\"\"+tb+\"\"\"'\n   ...: AND  CAST(FORMAT_DATETIME(\"%Y%m%d\", local_time) as INT64 )= \"\"\"+t0+\"\"\"\n   ...: \n   ...: \n   ...: GROUP BY g0.beacon_id, g0.account_id, detected_time,\n   ...:            g0.application_id, app_user_id, event, app_use_detected, \n   ...:            \n   ...:            h1.latitude, h1.longitude, \n   ...:            h2.group_id, group_name, group_type,\n   ...:            \n   ...:            address_prefecture, address_city,\n   ...:            address_detail,  install_loc_cat1id, install_loc_cat2id,\n   ...:            install_loc_cat2id_name, install_loc_cat1id_name\n   ...: )\n   ...: \n   ...: where  event=0\n   ...: \n   ...: group by  application_id, account_id,\n   ...:      event, app_use_detected,  \n   ...:      group_id, group_name, group_type, \n   ...:      beacon_id, address_prefecture, address_city,\n   ...:      address_detail,  install_loc_cat1id, install_loc_cat2id,\n   ...:      lat, lng\n   ...: order by application_id asc, group_type, group_id   \n   ...: \"\"\"\n   ...: \n   ...: \nTraceback (most recent call last):\n\n  File \"<ipython-input-6-6e3154b765c8>\", line 58, in <module>\n    AND  CAST(FORMAT_DATETIME(\"%Y%m%d\", local_time) as INT64 )= \"\"\"+t0+\"\"\"\n\nNameError: name 't0' is not defined\n\n\nIn [7]: t0=\"20170715\"\n   ...: ta=\"20170714\"\n   ...: tb=\"20170715\"\n\nIn [8]: ss= \"\"\"       \n   ...: select application_id as app_id, account_id,   event, app_use_detected, \n   ...:        group_id, group_name, group_type, \n   ...:        \n   ...:        beacon_id,  address_prefecture, address_city,\n   ...:        address_detail, \n   ...:        install_loc_cat1id_name as install_loc_cat1id , install_loc_cat2id_name as install_loc_cat2id,\n   ...:        latitude as lat, longitude as lng,\n   ...:        \n   ...:        count(app_user_id) as n_hanoulog,  count(DISTINCT app_user_id) as n_user \n   ...: \n   ...: from \n   ...:      (\n   ...:      select g0.beacon_id, g0.account_id,\n   ...:            g0.application_id, app_user_id, event, app_use_detected, \n   ...:            h1.latitude, h1.longitude, \n   ...:            h2.group_id, name as group_name, group_type, \n   ...:            address_prefecture, address_city,  address_detail, \n   ...:            install_loc_cat2id_name, install_loc_cat1id_name, min(g0.beacon_id)\n   ...:        \n   ...:        from  `beaconbank.BB_beaconlog*`     as g0\n   ...:      LEFT JOIN  \n   ...:        (  select account_id, application_id, name, group_type,  group_id, beacon_id\n   ...:           \n   ...:           from (\n   ...:            select  account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n   ...:                    CAST(FORMAT_DATE(\"%Y%m%d\", valid_form) as INT64 ) as valid_from, CAST(FORMAT_DATE(\"%Y%m%d\", valid_to) as INT64 ) as valid_to1\n   ...:              from beaconbank.BB_group_beacon as g1  \n   ...:              JOIN beaconbank.BB_group as g2 \n   ...:              ON     g1.group_id= g2.id \n   ...:              wHERE  g1.is_latest=1   AND  g1.group_type= 1   \n   ...:              GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id, valid_from, valid_to1\n   ...:             )\n   ...:           where valid_to1 >= \"\"\"+ t0 + \"\"\" and valid_from <= \"\"\" + t0 + \"\"\"\n   ...:        )  as g3   ON    g0.beacon_id=       g3.beacon_id\n   ...:                   AND   g0.application_id=  g3.application_id\n   ...:                   AND   g0.account_id=      g3.account_id\n   ...:      \n   ...:      LEFT JOIN\n   ...:         ( select   g5.id as beacon_id, latitude, longitude, \n   ...:               address_prefecture, address_city,   address_detail, \n   ...:               install_loc_cat1id, install_loc_cat2id,\n   ...:               m5.name as install_loc_cat2id_name, \n   ...:               m6.name as install_loc_cat1id_name, min(is_fixed)\n   ...:          \n   ...:          from   beaconbank.BB_beacon as g5         \n   ...:          JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n   ...:          JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n   ...:          JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id  \n   ...:          \n   ...:          GROUP BY beacon_id, latitude, longitude, \n   ...:               address_prefecture, address_city,   address_detail, \n   ...:               install_loc_cat1id, install_loc_cat2id,\n   ...:               install_loc_cat2id_name,    install_loc_cat1id_name           \n   ...:         )     as h1    ON  g0.beacon_id= h1.beacon_id\n   ...: \n   ...: WHERE _TABLE_SUFFIX BETWEEN '\"\"\"+ta+\"\"\"' AND '\"\"\"+tb+\"\"\"'\n   ...: AND  CAST(FORMAT_DATETIME(\"%Y%m%d\", local_time) as INT64 )= \"\"\"+t0+\"\"\"\n   ...: \n   ...: \n   ...: GROUP BY g0.beacon_id, g0.account_id, detected_time,\n   ...:            g0.application_id, app_user_id, event, app_use_detected, \n   ...:            \n   ...:            h1.latitude, h1.longitude, \n   ...:            h2.group_id, group_name, group_type,\n   ...:            \n   ...:            address_prefecture, address_city,\n   ...:            address_detail,  install_loc_cat1id, install_loc_cat2id,\n   ...:            install_loc_cat2id_name, install_loc_cat1id_name\n   ...: )\n   ...: \n   ...: where  event=0\n   ...: \n   ...: group by  application_id, account_id,\n   ...:      event, app_use_detected,  \n   ...:      group_id, group_name, group_type, \n   ...:      beacon_id, address_prefecture, address_city,\n   ...:      address_detail,  install_loc_cat1id, install_loc_cat2id,\n   ...:      lat, lng\n   ...: order by application_id asc, group_type, group_id   \n   ...: \"\"\"\n   ...: \n   ...: \n   ...: \n\nIn [9]: print ss\n       \nselect application_id as app_id, account_id,   event, app_use_detected, \n       group_id, group_name, group_type, \n       \n       beacon_id,  address_prefecture, address_city,\n       address_detail, \n       install_loc_cat1id_name as install_loc_cat1id , install_loc_cat2id_name as install_loc_cat2id,\n       latitude as lat, longitude as lng,\n       \n       count(app_user_id) as n_hanoulog,  count(DISTINCT app_user_id) as n_user \n\nfrom \n     (\n     select g0.beacon_id, g0.account_id,\n           g0.application_id, app_user_id, event, app_use_detected, \n           h1.latitude, h1.longitude, \n           h2.group_id, name as group_name, group_type, \n           address_prefecture, address_city,  address_detail, \n           install_loc_cat2id_name, install_loc_cat1id_name, min(g0.beacon_id)\n       \n       from  `beaconbank.BB_beaconlog*`     as g0\n     LEFT JOIN  \n       (  select account_id, application_id, name, group_type,  group_id, beacon_id\n          \n          from (\n           select  account_id, g1.application_id, name, group_type,  group_id, beacon_id, max(group_id), \n                   CAST(FORMAT_DATE(\"%Y%m%d\", valid_form) as INT64 ) as valid_from, CAST(FORMAT_DATE(\"%Y%m%d\", valid_to) as INT64 ) as valid_to1\n             from beaconbank.BB_group_beacon as g1  \n             JOIN beaconbank.BB_group as g2 \n             ON     g1.group_id= g2.id \n             wHERE  g1.is_latest=1   AND  g1.group_type= 1   \n             GROUP BY beacon_id, account_id,  g1.application_id, name, group_type, group_id, valid_from, valid_to1\n            )\n          where valid_to1 >= 20170715 and valid_from <= 20170715\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n     \n     LEFT JOIN\n        ( select   g5.id as beacon_id, latitude, longitude, \n              address_prefecture, address_city,   address_detail, \n              install_loc_cat1id, install_loc_cat2id,\n              m5.name as install_loc_cat2id_name, \n              m6.name as install_loc_cat1id_name, min(is_fixed)\n         \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id  \n         \n         GROUP BY beacon_id, latitude, longitude, \n              address_prefecture, address_city,   address_detail, \n              install_loc_cat1id, install_loc_cat2id,\n              install_loc_cat2id_name,    install_loc_cat1id_name           \n        )     as h1    ON  g0.beacon_id= h1.beacon_id\n\nWHERE _TABLE_SUFFIX BETWEEN '20170714' AND '20170715'\nAND  CAST(FORMAT_DATETIME(\"%Y%m%d\", local_time) as INT64 )= 20170715\n\n\nGROUP BY g0.beacon_id, g0.account_id, detected_time,\n           g0.application_id, app_user_id, event, app_use_detected, \n           \n           h1.latitude, h1.longitude, \n           h2.group_id, group_name, group_type,\n           \n           address_prefecture, address_city,\n           address_detail,  install_loc_cat1id, install_loc_cat2id,\n           install_loc_cat2id_name, install_loc_cat1id_name\n)\n\nwhere  event=0\n\ngroup by  application_id, account_id,\n     event, app_use_detected,  \n     group_id, group_name, group_type, \n     beacon_id, address_prefecture, address_city,\n     address_detail,  install_loc_cat1id, install_loc_cat2id,\n     lat, lng\norder by application_id asc, group_type, group_id   \n\n\n\n\n\nPySCIPOpt   Mixed Integer Optimization\n\nPySCIPOpt\n\n\n\n\n\n\n\n\n##################################################################################################################\nI have trained two models here namely Naive Bayes classifier and Support Vector Machines (SVM). \n\nNaive Bayes classifier is a conventional and very popular method for document classification problem. \n\nIt is a supervised probabilistic classifier based on Bayes theorem assuming independence between every pair of features. \nSVMs are supervised binary classifiers which are very effective when you have higher number of features. \nThe goal of SVM is to separate some subset of training data from rest called the support vectors (boundary of separating hyper-plane). \n\nThe decision function of SVM model that predicts the class of the test data is based on support vectors and makes use of a kernel trick.\n\n\nPre-Process the Text : remove frequent words,\nInput :\n    Samples row: 15,000 emails\n    Columns     : words   with frequency.\n\nwordID = i\nfeatures_matrix[docID,wordID] = all_words.count(word)\n\n\nhttps://github.com/abhijeet3922/Mail-Spam-Filtering/blob/master/enron-spamfilter.py\n\n\n\nConfusion Matrix\n\n\n\n\n\n\n\n\n\n##################################################################################################################\n\n1) How will you handle missing data ?\n\n  Identify nature of the column data:\n      numerical, category, IDentifier\n      How many data are missing : 1% or 50% or 90%\n\n    1) Easy/fast: remove rows / columns with missing.\n\n    2) Interpolation\n       numerical :  time series: interpolate with mean / model\n                   No time series, cluster the rows together and interpolate from cluster.\n                   Model to interpolate the missing:\n                        Maximum likehood based on cluster or time series like\n\n\n    3) category : same, from cluster,   or median from column\n    4) Identifier: Find a way to identify.\n\n\n2) reduce bias\n   Increase complexity of model   (vs OVerfit)\n  Error**2=  Bias**2 + Variance + Epsilon**2\n      bias=  E[ Y - Yest    ]**2\n      var=   E[  (Y-Yest)**2 ]\n\n\n       \n\nOverfitting : \n    Put regularizer L1, L2  fit on Validation dataset.\n    Cross Validation with K-Fold.\n    Bag methodology: separate dataset in K-Fold\n\n\n    A model is trained using k-1 of the folds as training data;\n    the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n\n    http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n\n\nReduce Variance :\n     OVerfit : too complex model.\n     Noise in Input\n\n     Averaging: reduce variance :  use different uncorrelated estimators\n\n     Bagging: split dataset in separate (no effect on bias).\n     Boosting: Loss function, weight training equally.\n               After training : overweight sample on the highest error\n               ---> Issue with overfitting noisy data.\n\n     More training sample\n\n\nIOT :   Kalman filter\n\n\nP value :  Check if hypothesis is True or Not\n    Pvalue = Proba( Result is more extreme than observed / H0    )\n   Null hypothesis:  \n         if P > alpha :  H0 is True\n         If P is small: H0 is False --->  Our method is Ok.\n   \n\nANOVA : Analysis of Variance : Feature importance.\n\n\n\nConfusion Matrix:\n Disease  TP: 100%\n          TN : 5%  (5% are said \"sick\", but not sick)\n\nTotal 1/1000 are sick:\n     \n\n\n\n\n\n3) K Fold Cross Validation\n   5 datasets ---> 1 for validation, train on 4 and loop\n   Accuracy is on average.\n\n\n\n\n4) K Means Clustering:\n  Fixed K nb of clsuter, random initlization, and put point to closest clusters.\n    Recalculate the new center ---> convergence\n\n   KNN: nearest neightbor\n\n\n   Category: Hamming distance: nb of values needed for change\n             K modes for category\n             distance(sydney, melbourne)=1-similarity(sydney,melbourne).\n             pb is the mearning level : vector space\n\n\n\n\n\n\n\n\n\n\n3) What is logistic regression\nUsed for classification :  \n  Binary output  1 or 0  (or probability between 1 and 0)  :Probability of passing exam /event\n  Proba(Y= yi / X,theta) = 1/ (1 + exp(-t.Theta * X))\n\n  Input can be continous variable\n\n  Fit with Maximum Likehood\n\n  Likehood = Product ( P(y=yi / X,theta)  )   conditionnal probability\n  log-likehood   ==  Cross Entropy   -y*ln(y)\n\n  Confusion Matrix / AUC\n\n\n\n4)\n\n\n19)       A test has a true positive rate of 100% and false positive rate of 5%. \nThere is a population with a 1/1000 rate of having the condition the test identifies.\n Considering a positive test, what is the probability of having that condition?\n\nPb= 1/1000\n\nP( sick / test positive) =  P(test positive / sick ) * P(sick) / P( test positive)\n                         = 100% * 1/1000  / P( test positive)\n\nP(positive) = Proba(sick) * Proba(Positive/sick) +   Proba(nosick) * Proba(positive/nosick)\n            =  1/1000 * 1   +  (1- 1/1000) *5%\n\n\np(sick /tes positve)=   1 /  ( 1 + (1000-1)*5%) = 1 / ( 1 +    999*5%) = 1/51= 2%\n\n\n\nP( A / True) =  P(Ttrue / A) * P(A)  /  P(True)\n             =   TP * P(A)\n\nP(true) =  P(A) * P(True/A)  +   ( 1- P(A)) * P(False/A)\n        = P(A) * TF    + (1-PA) * TN\n\n\n\n1000 --> 1 is sick\n     999 are not sick ---->  5% of them will say positive == 50 persons    \n51 persons positive test -->  ony 1 has sickness\n\n (1-p) * TN  + p*TF\n\n\n\n\n5) Box Cox Transformation for Linear regression\n   Residual are not normal\n\n   Z= (Y + delta) ** lambda\n\n   Fit Z by maximum likehood\n\n   ---> Improve residual to normality\n\n\nSupervised :     Input and Output\nUn-supervised :  Only Output ---> Find Structure (ie Auto Encoder)\n\n\n\n\n5) Metrics ROC Curve\n\n\n\n\n6) K Means Algorithm\n\n   Set of data with feature (ie columns)\n   Distance euclidian\n\n\n\nARIMA : integrated mixture autoregressive moving average model\np d q :   p:lag in Xt,  d delta  and q lag in error\nPhi(L) *D * Xt =   mu +  theta(L)* et\n\n\f\n\n\n\n\n\n\n###################################################################################################\n## Question 11: About 30% of human twins are identical, and the rest are fraternal.\nIdentical twins are necessarily the same sex, half are males and the other half are females.\nOne-quarter of fraternal twins are both male,\none-quarter both female,\nand one-half are mixes: one male, one female.\n\nYou have just become a parent of twins and are told they are both girls.\nGiven this information, what is the probability that they are identical?\n\nP(iden) = 0.3\nP(   m, m  / id twins   ) =0.5\nP(   f, f  / id twins  ) =0.5\n\nP(fraternal)\nP(  m, m / fra) = 0.25\nP(  f, f/ fra) =  0.25\nP(  m, f/ fra) =  0.5\n\n\nP(  Identical /   twin +(f,f)) =   P(  twin+(f,f) / Identical) * P(Identical) / P( twin + f,f )\n                               =   p(  Iden & twin(f,f) ) / P(twin (f,f) )\n                               =\n\n\n\n\n\n\n2)  What will you do if removing missing values from a dataset cause bias?\n\n3)  How can you reduce bias in a given data set?\n\n4) How will you impute missing information in a dataset?\n\n\n\n\n\n###########################################################\nAutomobile Predictive Maintenance: implemented a nearly real-time predictive system \nto identify automobile failure causes with over 90% accuracy; Teradata, R, Random Forest, KNN, DNN\n\nDangerous Driving Analytics: analyze dangerous driving behaviors \nand find dangerous driving spots from historical automobile sensor data; Aster, Python, PCA, Logistic Regression\n\nAnalytics-Driven Automotive Navigation Improvement: \nidentify on-road obstacle avoidance timing, \npredict traffic jam, recommend optimal driving routes to drivers by applying path \nand association analysis from historical automobile sensor data; Aster, R, Python\n\n\n\n\nTatsuru Kikuchi works as a Data Scientist / Business Strategy Consultant in the team of Data Science, \nAnalytics Business Consulting at Teradata in the Tokyo practice. As a member of Analytics Business Consulting, \nhe works in collaboration with other members of the team to research, design, prototype, test and document novel algorithms and predictive analytics\nfor automated, near real time decision making on telemetry data. On a daily basis, he works with massive data sets,\nwhich includes data integration, data cleansing, exploratory analysis, predictive modeling, and rapid prototyping. He presents findings to teammates and guide the transition of new algorithms into operational code.\n\nAccomplishments:\n- Digital Channel Transformation (Finance/Retail Bank): \nlead key initiative strategy related to the retail banks' overall omni-channel strategy; delivered a know-your-customer (KYC) type analysis \nfor drive the retail bank strategy to reduce the number of branches; managed the digital channel transformation of retail bank\n by providing forecast of the number of transactions in each channels (Branch/ATM/IB).\n\n\n- Failure Diagnosis (Automobile): implemented a nearly real-time predictive system to identify automobile failure causes \nwith over 90% accuracy based on Deep Learning technique.\n\n- Demand Forecasting (Telecommunications): predicted future demand forecasting for the metal line\n considering the growth of the optical line based on ARIMA model.\n\n\nTechnical skills:\n- SQL\n- R, Python\n- Machine Learning\n- Deep Learning\n\n\n\n•Demand forecast analysis of electric power. Key methods: machine learning (SVM, random forest) to find key variables of consumer's activities.\n•Consumer segmentation analysis of retailer's customers. Key method: k-means to segment consumer into categories.\n•Web stream analysis of customer shopping behavior. Key method: machine learning (SVM, random forest) to find their major web stream categories.\n•Building optimal target selection model for retailer. Key method: k-means for customer segmentation, logistic regression for optimization.\n•Deep analysis of customer's behavior. Key method: Principal component analysis for building powerful customer related variables, Logistic regression to classify customer into groups.\n\n\n\n\n\n\n\n##################################################################################################################\nQuestions from Data Science Interviews at Top Tech Companies\nData Scientist Interview Questions for Top Tech Companies\n\n\n\n\n\nData Science Interview Questions Asked at Other Top Tech Companies\n\n1) R programming language cannot handle large amounts of data. What are the other ways of handling it without using Hadoop infrastructure? (Asked at Pyro Networks)\n\n2) Explain the working of a Random Forest Machine Learning Algorithm (Asked at Cyient)\n\n3) Describe K-Means Clustering.(Asked at Symphony Teleca)\n\n4) What is the difference between logistic and linear regression? (Asked at Symphony Teleca)\n\n5) What kind of distribution does logistic regression follow? (Asked at Symphony Teleca)\n\n6) How do you parallelize machine learning algorithms? (Asked at Vodafone)\n\n7) When required data is not available for analysis, how do you go about collecting it? (Asked at Vodafone)\n\n8) What do you understand by heteroscadisticity (Asked at Vodafone)\n\n9) What do you understand by confidence interval? (Asked at Vodafone)\n\n10) Difference between adjusted r and r square. (Asked at Vodafone)\n\n11) How Facebook recommends items to newsfeed? (Asked at Finomena)\n\n12)  What do you understand by ROC curve and how is it used? (Asked at MachinePulse)\n\n13) How will you identify the top K queries from a file? (Asked at BloomReach)\n\n14) Given a set of webpages and changes on the website, how will you test the new website feature to determine if the change works positively? (Asked at BloomReach)\n\n15) There are N pieces of rope in a bucket. You put your hand into the bucket, take one end piece of the rope .Again you put your hand into the bucket and take another end piece of a rope. You tie both the end pieces together. What is the expected value of the number of loops within the bucket? (Asked at Natera)\n\n16) How will you test if a chosen credit scoring model works or not? What data will you look at? (Asked at Square)\n\n17) There are 10 bottles where each contains coins of 1 gram each. There is one bottle of that contains 1.1 gram coins. How will you identify that bottle after only one measurement? (Data Science Puzzle asked at Latent View Analytics)\n\n18) How will you measure a cylindrical glass filled with water whether it is exactly half filled or not? You cannot measure the water, you cannot measure the height of the glass nor can you dip anything into the glass. (Data Science Puzzle asked at Latent View Analytics)\n\n19) What would you do if you were a traffic sign? (Data Science Interview Question asked at Latent View Analytics)\n\n20)  If you could get the dataset on any topic of interest, irespective of the collection methods or resources then how would the dataset look like and what will you do with it. (Data Scientist Interview Question asked at CKM Advisors)\n\n21) Given n samples from a uniform distribution [0,d], how will you estimate the value of d? (Data Scientist Interview Question asked at Spotify)\n\n22) How will you tune a Random Forest? (Data Science Interview Question asked at Instacart).\n\n23) Tell us about a project where you have extracted useful information from a large dataset. Which machine learning algorithm did you use for this and why? (Data Scientist Interview Question asked at Greenplum)\n\n24) What is the difference between Z test and T test ? (Data Scientist Interview Questions asked at Antuit)\n\n25) What are the different models you have used for analysis and what were your inferences? (Data Scientist Interview Questions asked at Cognizant)\n\n26) Given the title of a product, identify the category and sub-category of the product. (Data Scientist interview question asked at Delhivery)\n\n27) What is the difference between machine learning and deep learning? ( Data Scientist Interview Question asked at InfoObjects)\n\n28) What are the different parameters in ARIMA models ? (Data Science Interview Question asked at Morgan Stanley)\n\n29) What are the optimisations you would consider when computing the similarity matrix for a large dataset? (Data Science Interview questions asked at MakeMyTrip)\n\n30) Use Python programming language to implement a toolbox with specific image processing tasks.(Data Science Interview Question asked at Intuitive Surgical)\n\n31) Why do you use Random Forest instead of a simple classifier for one of the classification problems ? (Data Science Interview Question asked at Audi)\n\n32) What is an n-gram? (Data Science Interview Question asked at Yelp)\n\n33) What are the problems related to Overfitting and Underfitting  and how will you deal with these ? (Data Science Interview Question asked at Tiger Analytics)\n\n34) Given a MxN dimension matrix with each cell containing an alphabet, find if a string is contained in it or not.(Data Science Interview Question asked at Tiger Analytics)\n\n35) How do you \"Group By\" in R programming language without making use of any package ? (Data Scientist Interview Question asked at OLX)\n\n36) List 15 features that you will make use of to build a classifier for OLX website.(Data Scientist Interview Question asked at OLX)\n\n37) How will you build a caching system using an advanced data structure like hashmap ? (Data Scientist Interview Question asked at OLX)\n\n38) How to reverse strings that have changing positions ? (Data Scientist Interview Question asked at Tiger Analytics)\n\n\n\n\n\n\n\n\nThese questions listed here are after a thorough research of the companies’ sites and high quality discussion forums. This is not a guarantee that these very questions will be asked in data science interviews, but this is just to give the readers an idea of what can be expected when they apply for the position of Data Scientists in these tech companies.\n\nLearn Data Science in Python to Land a Top Gig as a Data Scientist at Top Tech Companies!\n\nFacebook Data Science Interview Questions\n1)         A building has 100 floors. Given 2 identical eggs, how can you use them to find the threshold floor? The egg will break from any particular floor above floor N, including floor N itself.\n\n2)         In a given day, how many birthday posts occur on Facebook?\n\n3)         You are at a Casino. You have two dices to play with. You win $10 every time you roll a 5. If you play till you win and then stop, what is the expected pay-out?\n\n4)         How many big Macs does McDonald sell every year in US?\n\n5)         You are about to get on a plane to Seattle, you want to know whether you have to bring an umbrella or not. You call three of your random friends and as each one of them if it’s raining. The probability that your friend is telling the truth is 2/3 and the probability that they are playing a prank on you by lying is 1/3. If all 3 of them tell that it is raining, then what is the probability that it is actually raining in Seattle.\n\n6)         You can roll a dice three times. You will be given $X where X is the highest roll you get. You can choose to stop rolling at any time (example, if you roll a 6 on the first roll, you can stop). What is your expected pay-out?\n\n7)         How can bogus Facebook accounts be detected?\n\n8)       You have been given the data on Facebook user’s friending or defriending each other. How will you determine whether a given pair of Facebook users are friends or not?\n\n9)         How many dentists are there in US?\n\n10)         You have 2 dices. What is the probability of getting at least one 4? Also find out the probability of getting at least one 4 if you have n dices.\n\n11)       Pick up a coin C1 given C1+C2 with probability of trials p (h1) =.7, p (h2) =.6 and doing 10 trials. And what is the probability that the given coin you picked is C1 given you have 7 heads and 3 tails? \n\n12)     You are given two tables- friend_request and request_accepted. Friend_request contains requester_id, time and sent_to_id and request_accepted table contains time, acceptor_id and requestor_id. How will you determine the overall acceptance rate of requests?\n\n13)       How would add new Facebook members to the database of members, and code their relationships to others in the database? \n\n14)       What would you add to Facebook and how would you pitch it and measure its success?\n\n15)  How will you test that there is increased probability of a user to stay active after 6 months given that a user has more friends now?\n\n16) You have two tables-the first table has data about the users and their friends, the second table has data about the users and the pages they have liked. Write an SQL query to make recommendations using pages that your friends liked. The query result should not recommend the pages that have already been liked by a user.\n\n17) What is the probability of pulling a different shape or a different colour card from a deck of 52 cards?\n\n18) Which technique will you use to compare the performance of two back-end engines that generate automatic friend recommendations on Facebook?\n\n19) Implement a sorting algorithm for a numerical dataset in Python.\n\n20) How many people are using Facebook in California at 1.30 PM on Monday?\n\n21) You are given 50 cards with five different colors- 10 Green cards, 10 Red Cards, 10 Orange Cards, 10 Blue cards, and 10 Yellow cards. The cards of each colors are numbered from one to ten. Two cards are picked at random. Find out the probability that the cards picked are not of same number and same color.\n\n22) What approach will you follow to develop the love,like, sad feature on Facebook?\n\n\n\n\nInsight Data Science Interview Questions\n\n1)         Which companies participating in Insight would you be interested in working for? \n\n2)         Create a program in a language of your choice to read a text file with various tweets. The output should be 2 text files-one that contains the list of all unique words among all tweets along with the count for repeated words and the second file should contain the medium number of unique words for all tweets.\n\n3)         What motivates you to transition from academia to data science?\n\n\n\n\nTwitter Data Scientist Interview Questions                       \n\n1)    How can you measure engagement with given Twitter data?\n\n2)    Give a large dataset, find the median.\n\n3)    What is the good measure of influence of a Twitter user?\n\n\n\n\nAirBnB Data Science Interview Questions\n\n1)  Do you have some knowledge of R - analyse a given dataset in R?\n\n2)  What will you do if removing missing values from a dataset cause bias?\n\n3)  How can you reduce bias in a given data set?\n\n4) How will you impute missing information in a dataset?\n\n\n\n\nGoogle Data Science Interview Questions\n\n1)  Explain about string parsing in R language\n\n2) A disc is spinning on a spindle and you don’t know the direction in which way the disc is spinning. You are provided with a set of pins.How will you use the pins to describe in which way the disc is spinning?\n\n3)  Describe the data analysis process.\n\n4) How will you cut a circular cake into 8 equal pieces?\n\n\n\n\nLinkedIn Data Science Interview Questions\n\n1)  Find out K most frequent numbers from a given stream of numbers on the fly.\n\n2)  Given 2 vectors, how will you generate a sorted vector?\n\n3)  Implementing pow function\n\n4)  What kind of product you want to build at LinkedIn?\n\n5)  How will you design a recommendation engine for jobs?\n\n6)  Write a program to segment a long string into a group of valid words using Dictionary. The result should return false if the string cannot be segmented. Also explain about the complexity of the devised solution.\n\n7) Define an algorithm to discover when a person is starting to search for new job.\n\n8) What are the factors used to produce “People You May Know” data product on LinkedIn?\n\n9)  How will you find the second largest element in a Binary Search tree ? (Asked for a Data Scientist Intern job role)\n\n\n\n\nMu Sigma Data Science Interview Questions\n\n1)   Explain the difference between Supervised and Unsupervised Learning through examples.\n\n2)   How would you add value to the company through your projects?\n\n3)   Case Study based questions – Cars are implanted with speed tracker so that the insurance companies can track about our driving state. Based on this new scheme what kind of business questions can be answered?\n\n4)  Define standard deviation, mean, mode and median.\n\n5) What is a joke that people say about you and how would you rate the joke on a scale of 1 to 10?\n\n6) You own a clothing enterprise and want to improve your place in the market. How will you do it from the ground level ?\n\n\n\n\nAmazon Data Science Interview Questions\n\n1) Estimate the probability of a disease in a particular city given that the probability of the disease on a national level is low.\n\n2) How will inspect missing data and when are they important for your analysis?\n\n3) How will you decide whether a customer will buy a product today or not given the income of the customer, location where the customer lives, profession and gender? Define a machine learning algorithm for this.\n\n4) From a long sorted list and a short 4 element sorted list, which algorithm will you use to search the long sorted list for 4 elements.\n\n5) How can you compare a neural network that has one layer, one input and output to a logistic regression model?\n\n6) How do you treat colinearity?\n\n7) How will you deal with unbalanced data where the ratio of negative and positive is huge?\n\n8) What is the difference between -\n\ni) Stack and Queue\n\nii) Linkedin and Array\n\n\n\n\nUber Data Science Interview Questions\n\n1) Will Uber cause city congestion?\n\n2) What are the metrics you will use to track if Uber’s paid advertising strategies to acquire customers work? How will you figure out the acceptable cost of customer acquisition?\n\n3) Explain principal components analysis with equations.\n\n4) Explain about the various time series forecasting technqiues.\n\n5) Which machine learning algorithm will you use to solve a Uber driver accepting  request?\n\n6)How will you compare the results of various machine learning algorithms?\n\n7) How to solve multi-collinearity?\n\n8) How will you design the heatmap for Uber drivers to provide recommendation on where to wait for passengers? How would you approach this?\n\n9) If we added one rider to the current SF market, how would that affect the existing riders and drivers?  \n\n10) What are the different performance metrics for evaluating Uber services?\n\n11) How will you decide which version (Version 1 or Version 2) of the Surge Pricing Algorithms is working better for Uber ?\n\n12) How will you explain JOIN function in SQL to a 10 year old ?\n\n\n\n\nNetflix Data Science Interview Questions\n\n1) How can you build and test a metric to compare ranked list of TV shows or Movies for two Netflix users?\n\n2) How can you decide if one algorithm is better than the other?\n\n\n\n\nMicrosoft Data Science Interview Questions\n\n1) Write a function to check whether a particular word is a palindrome or not.\n\n2) How can you compute an inverse matrix faster by playing with some computation tricks?\n\n3) You have a bag with 6 marbles. One marble is white.  You reach the bag 100 times. After taking out a marble, it is placed back in the bag. What is the probability of drawing a white marble at least once?\n\n\nApple Data Science Interview Questions\n\n1) How do you take millions of users with 100's of transactions each, amongst 10000's of products and group the users together in a meaningful segments?\n\n\n\nAdobe Data Scientist Interview Questions\n\n1) Check whether a given integer is a palindrome or not without converting it to a string.\n\n2) What is the degree of freedom for lasso?\n\n3) You have two sorted array of integers, write a program to find a number from each array such that the sum of the two numbers is closest to an integer i.\n\n\n\n\nAmerican Express Data Scientist Interview Questions\n\n1) Suppose that American Express has 1 million card members along with their transaction details. They also have 10,000 restaurants and 1000 food coupons. Suggest a method which can be used to pass the food coupons to users given that some users have already received the food coupons so far.\n\n2) You are given a training dataset of users that contain their demographic details, the pages on Facebook they have liked so far and results of psychology test  based on their personality i.e. their openness to like FB pages or not. How will you predict the age, gender and other demographics of unseen data?\n\n\n\nQuora Data Scientist Interview Questions\n\n1) How will you test a machine learning model for accuracy?\n\n2) Print the elements of a matrix in zig-zag manner.\n\n3) How will you overcome overfitting in predictive models?\n\n4) Develop an algorithm to sort two lists of sorted integers into a single list.\n\n\n\nGoldman Sachs Data Scientist Interview Questions\n\n1) Count the total number of trees in United States.\n\n2) Estimate the number of square feet pizza’s eaten in US each year.\n\n3) A box has 12 red cards and 12 black cards. Another box has 24 red cards and 24 black cards. You want to draw two cards at random from one of the two boxes, which box has a higher probability of getting cards of same colour and why?\n\n4) How will you prove that the square root of 2 is irrational?\n\n5) What is the probability of getting a HTT combination before getting a TTH combination?\n\n6) There are 8 identical balls and only one of the ball is slightly heavier than the others. You are given a balance scale to find the heavier ball. What is the least number of times you have to use the balance scale to find the heavier ball?\n\n\n\nWalmart Data Science Interview Questions\n\n1) Write the code to reverse a Linked list.\n\n2) What assumptions does linear regression machine learning algorithm make?\n\n3) A stranger uses a search engine to find something and you do not know anything about the person. How will you design an algorithm to determine what the stranger is looking for just after he/she types few characters in the search box?\n\n4) How will you fix multi-colinearity in a regression model?\n\n5) What data structures are available in the Pandas package in Python programming language?\n\n6) State some use cases where Hadoop MapReduce works well and where it does not.\n\n7) What is the difference between an iterator, generator and list comprehension in Python?\n\n8) What is the difference between a bagged model and a boosted model?\n\n9) What do you understand by parametric and non-parametric methods? Explain with examples.\n\n10) Have you used sampling? What are the various types of sampling have you worked with?\n\n\n\n\nIBM Data Science Interview Questions\n\n1) How will you handle missing data ?\n\n\n\n\n\nYammer Data Science Interview Questions\n\nHow can you solve a problem that has no solution?\nOn rolling a dice if you get $1 per dot on the upturned face,what are your expected earnings from rolling a dice?\nIn continuation with question #2, if you have 2 chances to roll the dice and you are given the opportunity to decide when to stop rolling the dice (in the first roll or in the second roll). What will be your rolling strategy to get maximum earnings?\n What will be your expected earnings with the two roll strategy?\nYou are creating a report for user content uploads every month and observe a sudden increase in the number of upload for the month of November. The increase in uploads is particularly in image uploads. What do you think will be the cause for this and how will you test this sudden spike?\nCiti Bank Data Science Interview Questions\n\n1) A dice is rolled twice, what is the probability that on the second chance it will be a 6?\n\n2) What are Type 1 and Type 2 errors ?\n\n3) Burn two ropes, one needs 60 minutes of time to burn and the other needs 30 minutes of time. How will you achieve this in 45 minutes of time ?\n\nData Science Interview Questions Asked at Other Top Tech Companies\n\n1) R programming language cannot handle large amounts of data. What are the other ways of handling it without using Hadoop infrastructure? (Asked at Pyro Networks)\n\n2) Explain the working of a Random Forest Machine Learning Algorithm (Asked at Cyient)\n\n3) Describe K-Means Clustering.(Asked at Symphony Teleca)\n\n4) What is the difference between logistic and linear regression? (Asked at Symphony Teleca)\n\n5) What kind of distribution does logistic regression follow? (Asked at Symphony Teleca)\n\n6) How do you parallelize machine learning algorithms? (Asked at Vodafone)\n\n7) When required data is not available for analysis, how do you go about collecting it? (Asked at Vodafone)\n\n8) What do you understand by heteroscadisticity (Asked at Vodafone)\n\n9) What do you understand by confidence interval? (Asked at Vodafone)\n\n10) Difference between adjusted r and r square. (Asked at Vodafone)\n\n11) How Facebook recommends items to newsfeed? (Asked at Finomena)\n\n12)  What do you understand by ROC curve and how is it used? (Asked at MachinePulse)\n\n13) How will you identify the top K queries from a file? (Asked at BloomReach)\n\n14) Given a set of webpages and changes on the website, how will you test the new website feature to determine if the change works positively? (Asked at BloomReach)\n\n15) There are N pieces of rope in a bucket. You put your hand into the bucket, take one end piece of the rope .Again you put your hand into the bucket and take another end piece of a rope. You tie both the end pieces together. What is the expected value of the number of loops within the bucket? (Asked at Natera)\n\n16) How will you test if a chosen credit scoring model works or not? What data will you look at? (Asked at Square)\n\n17) There are 10 bottles where each contains coins of 1 gram each. There is one bottle of that contains 1.1 gram coins. How will you identify that bottle after only one measurement? (Data Science Puzzle asked at Latent View Analytics)\n\n18) How will you measure a cylindrical glass filled with water whether it is exactly half filled or not? You cannot measure the water, you cannot measure the height of the glass nor can you dip anything into the glass. (Data Science Puzzle asked at Latent View Analytics)\n\n19) What would you do if you were a traffic sign? (Data Science Interview Question asked at Latent View Analytics)\n\n20)  If you could get the dataset on any topic of interest, irespective of the collection methods or resources then how would the dataset look like and what will you do with it. (Data Scientist Interview Question asked at CKM Advisors)\n\n21) Given n samples from a uniform distribution [0,d], how will you estimate the value of d? (Data Scientist Interview Question asked at Spotify)\n\n22) How will you tune a Random Forest? (Data Science Interview Question asked at Instacart).\n\n23) Tell us about a project where you have extracted useful information from a large dataset. Which machine learning algorithm did you use for this and why? (Data Scientist Interview Question asked at Greenplum)\n\n24) What is the difference between Z test and T test ? (Data Scientist Interview Questions asked at Antuit)\n\n25) What are the different models you have used for analysis and what were your inferences? (Data Scientist Interview Questions asked at Cognizant)\n\n26) Given the title of a product, identify the category and sub-category of the product. (Data Scientist interview question asked at Delhivery)\n\n27) What is the difference between machine learning and deep learning? ( Data Scientist Interview Question asked at InfoObjects)\n\n28) What are the different parameters in ARIMA models ? (Data Science Interview Question asked at Morgan Stanley)\n\n29) What are the optimisations you would consider when computing the similarity matrix for a large dataset? (Data Science Interview questions asked at MakeMyTrip)\n\n30) Use Python programming language to implement a toolbox with specific image processing tasks.(Data Science Interview Question asked at Intuitive Surgical)\n\n31) Why do you use Random Forest instead of a simple classifier for one of the classification problems ? (Data Science Interview Question asked at Audi)\n\n32) What is an n-gram? (Data Science Interview Question asked at Yelp)\n\n33) What are the problems related to Overfitting and Underfitting  and how will you deal with these ? (Data Science Interview Question asked at Tiger Analytics)\n\n34) Given a MxN dimension matrix with each cell containing an alphabet, find if a string is contained in it or not.(Data Science Interview Question asked at Tiger Analytics)\n\n35) How do you \"Group By\" in R programming language without making use of any package ? (Data Scientist Interview Question asked at OLX)\n\n36) List 15 features that you will make use of to build a classifier for OLX website.(Data Scientist Interview Question asked at OLX)\n\n37) How will you build a caching system using an advanced data structure like hashmap ? (Data Scientist Interview Question asked at OLX)\n\n38) How to reverse strings that have changing positions ? (Data Scientist Interview Question asked at Tiger Analytics)\n\nIf you are asked questions like what is your favourite leisure activity? Or something like what is that you like to do for fun?  Most of the people often tend to answer that they like to read programming books or do coding thinking that this is what they are supposed to say in a technical interview. Is this something you really do it for fun? A key point to bear in mind that the interviewer is also a person and interact with them as a person naturally. This will help the interviewer see you as an all-rounder who can visualize the company’s whole vision and not just view business problems from an academic viewpoint.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##################################################################################################################\nbinary search :\n  if n==1 : return val\n\n\n  while true :\n   if left=mid : \n      return val\n   else :\n      left= FF(left, mid)\n      right= FF(mid+1, right)\n      final = Merge( left, right)\n\n\n\n\n##################\n#### Equilibrium point   \nint equi(int arr[], int n) {\n    if (n==0) return -1; \n    long long sum = 0;\n    int i; \n    for(i=0;i<n;i++) sum+=(long long) arr[i]; \n\n    long long sum_left = 0;    \n    for(i=0;i<n;i++) {\n        long long sum_right = sum - sum_left - (long long) arr[i];\n        if (sum_left == sum_right) return i;\n        sum_left += (long long) arr[i];\n    } \n    return -1; \n} \n\n\n\n\n##### Find Leader :  Sorting the table  in N*long_N\ndef solution(A): \n    n = len(A)\n    L = [-1] + A\n    L.sort()\n    count = 0\n    pos = (n + 1) // 2\n    candidate = L[pos]\n    for i in xrange(1, n + 1):\n        if (L[i] == candidate):\n            count = count + 1\n    if (2*count > n):\n        return candidate\n    return -1\n\n\n\n\n\n\n\n#########################################################################################################################\n#########################################################################################################################\nhttps://codility.com/programmers/lessons/3-time_complexity/frog_jmp/\n\n\nCodility ‘Tape Equilibrium’ Solution\nPosted on July 22, 2014 by Martin\nShort Problem Definition:\nMinimize the value |(A[0] + … + A[P-1]) – (A[P] + … + A[N-1])|.\n\nLink\nTapeEquilibrium\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nIn the first run I compute the left part up to the point i and the overall sum last. Then I compute the minimal difference between 0..i and i+1..n.\n\nSolution:\nimport sys\n \ndef solution(A):\n    #1st pass\n    parts = [0] * len(A)\n    parts[0] = A[0]\n  \n    for idx in xrange(1, len(A)):\n        parts[idx] = A[idx] + parts[idx-1]\n  \n    #2nd pass\n    solution = sys.maxint\n    for idx in xrange(0, len(parts)-1):\n        solution = min(solution, abs(parts[-1] - 2 * parts[idx]));  \n  \n    return solution\n\n\n\n#########################################################################################################################\nShort Problem Definition:\nCount minimal number of jumps from position X to Y.\n\nLink\nFrogJmp\n\nComplexity:\nexpected worst-case time complexity is O(1);\n\nexpected worst-case space complexity is O(1).\n\nExecution:\nDo not use float division if possible!\n\nSolution:\ndef solution(X, Y, D):\n    if Y < X or D <= 0:\n        raise Exception(\"Invalid arguments\")\n         \n    if (Y- X) % D == 0:\n        return (Y- X) // D\n    else:\n        return ((Y- X) // D) + 1\n\n\n\n\n######################################################################################################\nShort Problem Definition:\nFind the missing element in a given permutation.\n\nLink\nPermMissingElem\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(1)\n\nExecution:\nSum all elements that should be in the list and sum all elements that actually are in the list. The sum is 0 based, so +1 is required. The first solution using the + operator can cause int overflow in not-python languages. Therefore the use of a binary XOR is adequate.\n\nSolution:\ndef solution(A):\n    should_be = len(A) # you never see N+1 in the iteration\n    sum_is = 0\n \n    for idx in xrange(len(A)):\n        sum_is += A[idx]\n        should_be += idx+1\n \n    return should_be - sum_is +1\n\n\n\n\n###################################################################################################\nShort Problem Definition:\nCheck whether array N is a permutation.\n\nLink\nPermCheck\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nMark elements as seen in a boolean array. Elements seen twice or out of bounds of the size indicate that the list is no permutation. The check if the boolean array only contains true elements is not required. This solution only works with permutations starting from 1.\n\nSolution:\n\ndef solution(A):\n    seen = [False] * len(A)\n \n    for value in A:\n        if 0 <= value > len(A):\n            return 0\n        if seen[value-1] == True:\n            return 0\n        seen[value-1] = True\n \n    return 1\n\n\n\n\n###################################################################################################\nShort Problem Definition:\nFind the earliest time when a frog can jump to the other side of a river.\n\nLink\nFrogRiverOne\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(X)\n\nExecution:\nMark seen elements as such in a boolean array. I do not like the idea of returning the first second as 0. But specifications are specifications ????\n\nSolution:\ndef solution(X, A):\n    passable = [False] * X\n    uncovered = X\n \n    for idx in xrange(len(A)):\n        if A[idx] <= 0 or A[idx] > X:\n            raise Exception(\"Invalid value\", A[idx])\n        if passable[A[idx]-1] == False:\n            passable[A[idx]-1] = True\n            uncovered -= 1\n            if uncovered == 0:\n                return idx\n \n    return -1\n\n\n\n###################################################################################################\nShort Problem Definition:\nCalculate the values of counters after applying all alternating operations: increase counter by 1; set value of all counters to current maximum.\n\nLink\nMaxCounters\n\nComplexity:\nexpected worst-case time complexity is O(N+M);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nThe idea is to perform the specified operation as stated. It is not required to iterate over the whole array if a new value is set for all the values. Just save the value and check it when an increase on that position is performed.\n\nSolution:\n#include <algorithm>\n \nvector<int> solution(int N, vector<int> &A) {\n    vector<int> sol;\n    int current_max = 0;\n    int last_increase = 0;\n \n    for(int i=0; i<N;i++){\n        sol.push_back(0);\n    }\n \n    for(unsigned int i=0; i<A.size();i++){\n        if (A[i] > N) {\n            last_increase = current_max;\n        } else {\n            sol[A[i]-1] = max(sol[A[i]-1], last_increase);\n            sol[A[i]-1]++;\n            current_max = max(current_max, sol[A[i]-1]);\n        }\n    }\n \n    for(int i=0; i<N;i++){\n        sol[i] = max(sol[i], last_increase);\n    }\n \n    return sol;\n}\n\n\n\n###################################################################################################\nShort Problem Definition:\nFind the minimal positive integer not occurring in a given sequence.\n\nLink\nMissingInteger\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nYou only need to consider the first (N) positive integers. In this specification 0 does not count as a valid candidate! Any value that is below 1 or above N can be ignored.\n\nSolution:\ndef solution(A):\n    seen = [False] * len(A)\n    for value in A:\n        if 0 < value <= len(A):\n            seen[value-1] = True\n \n    for idx in xrange(len(seen)):\n        if seen[idx] == False:\n            return idx + 1\n \n    return len(A)+1\n\n\n\n\n\n\n\n###################################################################################################\nShort Problem Definition:\nCount the number of passing cars on the road.\n\nLink\nPassingCars\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(1)\n\nExecution:\nCount all cars heading in one direction (west). Each car heading the other direction (east) passes all cars that went west so far. Note that east cars at the beginning of the list pass no cars! Also do not forget the upper limit!\n\nSolution:\ndef solution(A):\n    west_cars = 0\n    cnt_passings = 0\n \n    for idx in xrange(len(A)-1, -1, -1):\n        if A[idx] == 0:\n            cnt_passings += west_cars\n            if cnt_passings > 1000000000:\n                return -1\n        else:\n            west_cars += 1\n \n    return cnt_passings\n\n\n\n\n######################################################################################################\nShort Problem Definition:\nFind the minimal nucleotide from a range of sequence DNA.\n\nLink\nGenomicRangeQuery\n\nComplexity:\nexpected worst-case time complexity is O(N+M);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nRemember the last position on which was the genome (A, C, G, T) was seen. If the distance between Q and P is lower than the distance to the last seen genome, we have found the right candidate.\n\nSolution:\ndef writeCharToList(S, last_seen, c, idx):\n    if S[idx] == c:\n        last_seen[idx] = idx\n    elif idx > 0:\n        last_seen[idx] = last_seen[idx -1]\n \ndef solution(S, P, Q):\n     \n    if len(P) != len(Q):\n        raise Exception(\"Invalid input\")\n     \n    last_seen_A = [-1] * len(S)\n    last_seen_C = [-1] * len(S)\n    last_seen_G = [-1] * len(S)\n    last_seen_T = [-1] * len(S)\n         \n    for idx in xrange(len(S)):\n        writeCharToList(S, last_seen_A, 'A', idx)\n        writeCharToList(S, last_seen_C, 'C', idx)\n        writeCharToList(S, last_seen_G, 'G', idx)\n        writeCharToList(S, last_seen_T, 'T', idx)\n     \n     \n    solution = [0] * len(Q)\n     \n    for idx in xrange(len(Q)):\n        if last_seen_A[Q[idx]] >= P[idx]:\n            solution[idx] = 1\n        elif last_seen_C[Q[idx]] >= P[idx]:\n            solution[idx] = 2\n        elif last_seen_G[Q[idx]] >= P[idx]:\n            solution[idx] = 3\n        elif last_seen_T[Q[idx]] >= P[idx]:\n            solution[idx] = 4\n        else:    \n            raise Exception(\"Should never happen\")\n         \n    return solution\n\n\n\n######################################################################################################\nShort Problem Definition:\nCompute number of integers divisible by k in range [a..b].\n\nLink\nCountDiv\n\nComplexity:\nexpected worst-case time complexity is O(1);\n\nexpected worst-case space complexity is O(1)\n\nExecution:\nThis little check required a bit of experimentation. One needs to start from the first valid value that is bigger than A and a multiply of K.\n\nSolution:\ndef solution(A, B, K):\n    if B < A or K <= 0:\n        raise Exception(\"Invalid Input\")\n \n    min_value =  ((A + K -1) // K) * K\n \n    if min_value > B:\n      return 0\n \n    return ((B - min_value) // K) + 1\n\n\n\n\n\n\n\n\n\n######################################################################################################\nShort Problem Definition:\nDetermine whether a given string of parentheses is properly nested.\n\nLink\nBrackets\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nPut every opening bracket on a stack. If a closing bracket is not the same as the top stack bracket, the string is not properly nested.\n\nSolution:\ndef isValidPair(left, right):\n    if left == '(' and right == ')':\n        return True\n    if left == '[' and right == ']':\n        return True \n    if left == '{' and right == '}':\n        return True   \n    return False\n \ndef solution(S):\n    stack = []\n     \n    for symbol in S:\n        if symbol == '[' or symbol == '{' or symbol == '(':\n            stack.append(symbol)\n        else:\n            if len(stack) == 0:\n                return 0\n            last = stack.pop()\n            if not isValidPair(last, symbol):\n                return 0\n     \n    if len(stack) != 0:\n        return 0\n             \n    return 1\n\n\n######################################################################################################\n\n\n\n\n######################################################################################################\nCodility ‘MaxSliceSum’ Solution\nPosted on January 6, 2015 by Martin\nShort Problem Definition:\nFind a maximum sum of a compact subsequence of array elements.\n\nLink\nMaxSliceSum\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nThe only difference to the example given by Codility is the minimal slice length, which is 1.\n\n\nSolution:\ndef solution(A):\n    max_ending = max_slice = -1000000\n    for a in A:\n        max_ending = max(a, max_ending +a)\n        max_slice = max(max_slice, max_ending)\n         \n    return max_slice\n\n\n\n######################################################################################################\nCodility ‘MaxDoubleSliceSum’ Solution\nPosted on December 30, 2014 by Martin\nShort Problem Definition:\nFind the maximal sum of any double slice.\nA non-empty zero-indexed array A consisting of N integers is given.\n\nA triplet (X, Y, Z), such that 0 ≤ X < Y < Z < N, is called a double slice.\nThe sum of double slice (X, Y, Z) is the total of A[X + 1] + A[X + 2] + ... + A[Y − 1] + A[Y + 1] + A[Y + 2] + ... + A[Z − 1].\nFor example, array A such that:\n\n    A[0] = 3\n    A[1] = 2\n    A[2] = 6\n    A[3] = -1\n    A[4] = 4\n    A[5] = 5\n    A[6] = -1\n    A[7] = 2\ncontains the following example double slices:\n\ndouble slice (0, 3, 6), sum is 2 + 6 + 4 + 5 = 17,\ndouble slice (0, 3, 7), sum is 2 + 6 + 4 + 5 − 1 = 16,\ndouble slice (3, 4, 5), sum is 0.\n\n\n\nLink\nMaxDoubleSliceSum\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nTo solve this task, you need to keep track of two slice arrays. The optimal double slice can be found at an index that has the maximal sum of those two arrays. It can not be the 0th or the last index.\n\n### Solution:\ndef solution(A):\n    ending_here = [0] * len(A)\n    starting_here = [0] * len(A)\n     \n    for idx in xrange(1, len(A)):\n        ending_here[idx] = max(0, ending_here[idx-1] + A[idx])\n     \n    for idx in reversed(xrange(len(A)-1)):\n        starting_here[idx] = max(0, starting_here[idx+1] + A[idx])\n     \n    max_double_slice = 0\n     \n    for idx in xrange(1, len(A)-1):\n        max_double_slice = max(max_double_slice, starting_here[idx+1] + ending_here[idx-1])\n         \n         \n    return max_double_slice\n\n\n\n\n######################################################################################################\ndef sieve(N):\n    semi = set()\n    sieve = [True]* (N+1)\n    sieve[0] = sieve[1] = False\n \n    i = 2\n    while (i*i <= N):\n        if sieve[i] == True:\n            for j in xrange(i*i, N+1, i):\n                sieve[j] = False\n        i += 1\n \n    i = 2\n    while (i*i <= N):\n        if sieve[i] == True:\n            for j in xrange(i*i, N+1, i):\n                if (j % i == 0 and sieve[j/i] == True):\n                    semi.add(j)\n        i += 1\n \n    return semi\n \ndef solution(N, P, Q):\n \n    semi_set = sieve(N)\n \n    prefix = []\n \n    prefix.append(0) # 0\n    prefix.append(0) # 1\n    prefix.append(0) # 2\n    prefix.append(0) # 3\n    prefix.append(1) # 4\n \n    for idx in xrange(5, max(Q)+1):\n        if idx in semi_set:\n            prefix.append(prefix[-1]+1)\n        else:\n            prefix.append(prefix[-1])\n \n    solution = []\n \n    for idx in xrange(len(Q)):\n        solution.append(prefix[Q[idx]] - prefix[P[idx]-1])\n \n    return solution\n\n\n\n\n######################################################################################################\nCodility ‘FibFrog’ Solution\nPosted on December 28, 2014 by Martin\nShort Problem Definition:\nCount the minimum number of jumps required for a frog to get to the other side of a river.\n\nThe Fibonacci sequence is defined using the following recursive formula:\n\n    F(0) = 0\n    F(1) = 1\n    F(M) = F(M - 1) + F(M - 2) if M >= 2\nA small frog wants to get to the other side of a river. The frog is initially located at one bank of the river (position −1) and wants to get to the other bank (position N). The frog can jump over any distance F(K), where F(K) is the K-th Fibonacci number. Luckily, there are many leaves on the river, and the frog can jump between the leaves, but only in the direction of the bank at position N.\n\nThe leaves on the river are represented in a zero-indexed array A consisting of N integers. Consecutive elements of array A represent consecutive positions from 0 to N − 1 on the river. Array A contains only 0s and/or 1s:\n\n0 represents a position without a leaf;\n1 represents a position containing a leaf.\nThe goal is to count the minimum number of jumps in which the frog can get to the other side of the river (from position −1 to position N). The frog can jump between positions −1 and N (the banks of the river) and every position containing a leaf.\n\nFor example, consider array A such that:\n\n    A[0] = 0\n    A[1] = 0\n    A[2] = 0\n    A[3] = 1\n    A[4] = 1\n    A[5] = 0\n    A[6] = 1\n    A[7] = 0\n    A[8] = 0\n    A[9] = 0\n    A[10] = 0\nThe frog can make three jumps of length F(5) = 5, F(3) = 2 and F(5) = 5.\n\n\n\nLink\nFibFrog\n\nComplexity:\nexpected worst-case time complexity is O(N*log(N))\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nThis problem can be solved by in a Dynamic Programming way. You need to know the optimal count of jumps that can reach a given leaf. You get those by either reaching the leaf from the first shore or by reaching it from another leaf.\n\nThe N*log(N) time complexity is given by the fact, that there are approximately log(N) Fibonacci numbers up to N and you visit each position once.\n\nAs for the sequence hack: there are 26 Fibonacci numbers smaller than 100k, so I just preallocate an array of this size.\n\nSolution:\ndef get_fib_seq_up_to_n(N):\n    # there are 26 numbers smaller than 100k\n    fib = [0] * (27)\n    fib[1] = 1\n    for i in xrange(2, 27):\n        fib[i] = fib[i - 1] + fib[i - 2]\n        if fib[i] > N:\n            return fib[2:i]\n        else:\n            last_valid = i\n     \n     \n     \ndef solution(A):\n    # you can always step on the other shore, this simplifies the algorithm\n    A.append(1)\n \n    fib_set = get_fib_seq_up_to_n(len(A))\n     \n    # this array will hold the optimal jump count that reaches this index\n    reachable = [-1] * (len(A))\n     \n    # get the leafs that can be reached from the starting shore\n    for jump in fib_set:\n        if A[jump-1] == 1:\n            reachable[jump-1] = 1\n     \n    # iterate all the positions until you reach the other shore\n    for idx in xrange(len(A)):\n        # ignore non-leafs and already found paths\n        if A[idx] == 0 or reachable[idx] > 0:\n            continue\n \n        # get the optimal jump count to reach this leaf\n        min_idx = -1\n        min_value = 100000\n        for jump in fib_set:\n            previous_idx = idx - jump\n            if previous_idx < 0:\n                break\n            if reachable[previous_idx] > 0 and min_value > reachable[previous_idx]:\n                min_value = reachable[previous_idx]\n                min_idx = previous_idx\n        if min_idx != -1:\n            reachable[idx] = min_value +1\n \n    return reachable[len(A)-1]\n\n\n\n\n######################################################################################################\nCodility ‘AbsDistinct’ Solution\nPosted on August 14, 2014 by Martin\nShort Problem Definition:\nCompute number of distinct absolute values of sorted array elements.\n\nLink\nAbsDistinct\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nAdditional storage is allowed. Therefore a simple python solution will suffice.\n\nSolution:\ndef solution(A):\n    return len(set([abs(x) for x in A]))\n\n\n\n\n######################################################################################################\nCodility ‘TieRopes’ Solution\nPosted on August 25, 2014 by Martin\nShort Problem Definition:\nTie adjacent ropes to achieve the maximum number of ropes of length >= K.\n\nThere are N ropes numbered from 0 to N − 1, whose lengths are given in a zero-indexed array A, lying on the floor in a line. For each I (0 ≤ I < N), the length of rope I on the line is A[I].\n\nWe say that two ropes I and I + 1 are adjacent. Two adjacent ropes can be tied together with a knot, and the length of the tied rope is the sum of lengths of both ropes. The resulting new rope can then be tied again.\n\nFor a given integer K, the goal is to tie the ropes in such a way that the number of ropes whose length is greater than or equal to K is maximal.\n\nFor example, consider K = 4 and array A such that:\n\n    A[0] = 1\n    A[1] = 2\n    A[2] = 3\n    A[3] = 4\n    A[4] = 1\n    A[5] = 1\n    A[6] = 3\n\nWe can tie:\n\nrope 1 with rope 2 to produce a rope of length A[1] + A[2] = 5;\nrope 4 with rope 5 with rope 6 to produce a rope of length A[4] + A[5] + A[6] = 5.\nAfter that, there will be three ropes whose lengths are greater than or equal to K = 4. It is not possible to produce four such ropes.\nFor example, given K = 4 and array A such that:\n\n    A[0] = 1\n    A[1] = 2\n    A[2] = 3\n    A[3] = 4\n    A[4] = 1\n    A[5] = 1\n    A[6] = 3\nthe function should return 3, as explained above.\n\n\nLink\nTieRopes\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nI am a bit skeptical about the correctness of my solution. It gets 100/100 through…\n\nSolution:\ndef solution(K, A):\n    cnt = 0\n    current = 0\n    for part in A:\n        current += part\n        if current >= K:\n            cnt +=1\n            current = 0\n \n    return cnt\n######################################################################################################\n\n\n\n######################################################################################################\nCodility ‘Max Nonoverlapping Segments’ Solution\nShort Problem Definition:\nFind a maximal set of non((-))overlapping segments.\n\n\nLocated on a line are N segments, numbered from 0 to N − 1, whose positions are given in zero-indexed arrays A and B. For each I (0 ≤ I < N) the position of segment I is from A[I] to B[I] (inclusive). The segments are sorted by their ends, which means that B[K] ≤ B[K + 1] for K such that 0 ≤ K < N − 1.\n\nTwo segments I and J, such that I ≠ J, are overlapping if they share at least one common point. In other words, A[I] ≤ A[J] ≤ B[I] or A[J] ≤ A[I] ≤ B[J].\n\nWe say that the set of segments is non-overlapping if it contains no two overlapping segments. The goal is to find the size of a non-overlapping set containing the maximal number of segments.\n\nFor example, consider arrays A, B such that:\n\n    A[0] = 1    B[0] = 5\n    A[1] = 3    B[1] = 6\n    A[2] = 7    B[2] = 8\n    A[3] = 9    B[3] = 9\n    A[4] = 9    B[4] = 10\nThe segments are shown in the figure below.\n\n\n\nThe size of a non-overlapping set containing a maximal number of segments is 3. For example, possible sets are {0, 2, 3}, {0, 2, 4}, {1, 2, 3} or {1, 2, 4}. There is no non-overlapping set with four segments.\n\nWrite a function:\n\ndef solution(A, B)\n\nthat, given two zero-indexed arrays A and B consisting of N integers, returns the size of a non-overlapping set containing a maximal number of segments.\n\nFor example, given arrays A, B shown above, the function should return 3, as explained above.\n\nAssume that:\n\nN is an integer within the range [0..30,000];\neach element of arrays A, B is an integer within the range [0..1,000,000,000];\nA[I] ≤ B[I], for each I (0 ≤ I < N);\nB[K] ≤ B[K + 1], for each K (0 ≤ K < N − 1).\nComplexity:\n\nexpected worst-case time complexity is O(N);\nexpected worst-case space complexity is O(N), beyond input storage (not counting the storage required for input arguments).\nElements of input arrays can be modified.\n\n\nLink\nMaxNonoverlappingSegments\n\nComplexity:\nexpected worst-case time complexity is O(N)\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nThis can be solved by using greedy search. The beginning of the next segment must come strictly after its predecessor.\n\nSolution:\ndef solution(A, B):\n    if len(A) < 1:\n        return 0\n     \n    cnt = 1\n    prev_end = B[0]\n     \n    for idx in xrange(1, len(A)):\n        if A[idx] > prev_end:\n            cnt += 1\n            prev_end = B[idx]\n     \n    return cnt\n\n\n\n\n######################################################################################################\nCodility ‘BinaryGap’ Solution\nPosted on August 2, 2014 by Martin\nShort Problem Definition:\nFind longest sequence of zeros in binary representation of an integer.\n\nLink\nBinaryGap\n\nComplexity:\nexpected worst-case time complexity is O(log(N));\n\nexpected worst-case space complexity is O(1)\n\nExecution:\nThe solution is straight-forward! Use of binary shift.\n\nSolution:\ndef solution(N):\n    cnt = 0\n    result = 0\n    found_one = False\n \n    i = N    \n         \n    while i:\n        if i & 1 == 1:\n            if (found_one == False):\n                found_one = True\n            else:\n                result = max(result,cnt)\n            cnt = 0\n        else:\n            cnt += 1\n        i >>= 1\n    \n    return result\n\n\n\n######################################################################################################\nShort Problem Definition:\nFind a symmetry point of a string, if any.\n\nLink\nStrSymmetryPoint\n\nComplexity:\nexpected worst-case time complexity is O(length(S));\n\nexpected worst-case space complexity is O(1) (not counting the storage required for input arguments).\n\nExecution:\nThis problem gave me a lot of headache. It is so trivial I that over-complicated it. I thought that you should find a symmetry point at any possible position, ignoring the residual characters. You would obviously try to maximize the length of this symmetrical sub-array. I was not able to come with any O(S) algorithm for this problem derivation. So just to remind you, this problem is a simple palindrome check. Additionally, you drop all evenly sized strings as their symmetry point is between the indexes.\n\nSolution:\ndef solution(S):\n    l = len(S)\n \n    if l % 2 == 0:\n        return -1\n \n    mid_point = l // 2\n \n    for idx in xrange(0, mid_point):\n        if S[idx] != S[l - idx - 1]:\n            return -1\n \n    return mid_point\n\n\n\n\n\n######################################################################################################\nCodility ‘OddOccurrencesInArray’ Solution\nFind value that occurs in odd number of elements.\n\nLink\nOddOccurrencesInArray\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(1)\n\nExecution:\nThis problem can be found in many algorithm books. A xor A cancels itself and B xor 0 is B. Therefore A xor A xor B xor C xor C is B.\n\nSolution:\ndef solution(A):\n    missing_int = 0\n    for value in A:\n        missing_int ^= value\n    return missing_int\n\n\n\n\n\n######################################################################################################\nCodility ‘TreeHeight’ Solution\nShort Problem Definition:\nCompute the height of a binary link-tree.\n\nLink\nTreeHeight\n\nComplexity:\nexpected worst-case time complexity is O(N);\n\nexpected worst-case space complexity is O(N)\n\nExecution:\nThe height of a tree is the maximal height +1 of its subtrees. In this specification a tree with just the root node has a height of 0.\n\nSolution:\n'''\nclass Tree(object):\n  x = 0\n  l = None\n  r = None\n'''\n \ndef getHeight(sub_T):\n    if sub_T == None:\n        return 0\n    return max(getHeight(sub_T.l), getHeight(sub_T.r))+1\n \ndef solution(T):\n    return max(getHeight(T.l), getHeight(T.r))\n\n\n\n\n\n######################################################################################################\nCodility ‘CyclicRotation’ Solution\nPosted on January 19, 2016 by Martin\nShort Problem Definition:\nRotate an array to the right by a given number of steps.\n\nLink\nCyclic Rotation\n\nComplexity:\nexpected worst-case time complexity is O(N)\n\nExecution:\nThere are multiple solutions to this problem. I picked the one that does not create a copy of the array.\n\nSolution:\ndef reverse(arr, i, j):\n    for idx in xrange((j - i + 1) / 2):\n        arr[i+idx], arr[j-idx] = arr[j-idx], arr[i+idx]\n \ndef solution(A, K):\n    l = len(A)\n    if l == 0:\n        return []\n         \n    K = K%l\n     \n    reverse(A, l - K, l -1)\n    reverse(A, 0, l - K -1)\n    reverse(A, 0, l - 1)\n \n    return A\n\n\n\n\n######################################################################\ndef solution(A) :\n  n= len(A)\n  \n  def isequi(k) :\n    if k==0  : \n      if 0==sum(A[k+1:]) : return 1\n   \n    if k== n-1 :\n      if sum(A[0:k-1])==0 : return 1\n      \n    \n    if sum(A[:k-1+1]) ==  sum(A[k+1:]) : return 1\n    else : return 0\n\n  l1= []\n  for k in xrange(0, n) :\n     if isequi(k) : \n         l1.append(k)\n\n  if len(l1) ==0 : return -1\n  else : return l1[0]\n  \n  \n\nFor example, consider the following array A consisting of N = 8 elements:\n\n  A[0] = -1\n  A[1] =  3\n  A[2] = -4\n  A[3] =  5\n  A[4] =  1\n  A[5] = -6\n  A[6] =  2\n  A[7] =  1\nP = 1 is an equilibrium index of this array, because:\n\nA[0] = −1 = A[2] + A[3] + A[4] + A[5] + A[6] + A[7]\nP = 3 is an equilibrium index of this array, because:\n\nA[0] + A[1] + A[2] = −2 = A[4] + A[5] + A[6] + A[7]\nP = 7 is also an equilibrium index, because:\n\nA[0] + A[1] + A[2] + A[3] + A[4] + A[5] + A[6] = 0\n\n\n\nWrite a function:\n\ndef solution(A)\n\nthat, given a zero-indexed array A consisting of N integers, returns any of its equilibrium indices. \nThe function should return −1 if no equilibrium index exists.\n\n\n##########################################################################################################\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest cases\n\nSo far, their test cases follow a predictable methodology :\n\nthe examples provided are explicitly tested\nCorrectness tests\n\nan empty or zero test case is devised and tested - the expected result is often not explicit or obvious, but will actually be there implicitly, such that you may not realise it is indeed specified until you work out what the answer should be.\na minimal test case - using just one input, or whatever is the absolute minimal conceiveable input - again, probably not explicitly described, but there implicitly nonetheless\nedge cases - test cases written to root out those awkward -off-by-one- scenarios that inevitably suck up 80% of the time required to devise a solution\na simple, or 'small' test case or two - just some basic, as you might reasonably anticipate, examples\nPerformance tests\n\nworst case scenario is tested - the biggest possible numbers in the biggest resultsets - with the intent to test the speed and space restraints\nnot always, as the problem dictates, some medium sized test cases eg: ~100 - ~5000 length arrays\nalways some 'extreme' test cases typically involving generating maximal random datasets\nOther notes\n\nyou're safe to assume they won't test, mark you down for, failing to guard against the explicit assumptions described. So if it says N is 0..1000, they won't feed in an N=1001 just to see if you protected against it.\nthe \"Open reading material\", currently at the top of each lesson, is worth reading before attempting the exercises as they are short and focus exactly on what you'll need to solve the following puzzles\nduring the actual interview testing/exam, the report sent to the candidate is much more sparesly detailed than the one sent to the company?!\nif you use the browser to actually build your solution - every edit and run is recorded and presented to the client\nif you are given multiple tasks, you are permitted to read them, and commence them, and submit them in any order.\n\n\nif there seems to be a lack of specificity in every puzzle around what is the correct response to error conditions; \n\nlook, read, look again, as after seeing the solution that apparent lack always seems like a debateably reasonable assumption implied by the specs. For example:\n\"MissingInteger\" (Lesson 4) does not specify the correct response if the input sequence is [-1,-2,-3]: there are no positive integers so what is the correct response? The 'minimal positive integer' is 1.\nSimilarly, it does not explicitly state that if the input set is full (no integers are missing) then return the largest value—plus one. Again, seems perfectly reasonable in hindsight, but a source of uncertainty in the moment\nbefore submitting your solution, there is no feedback regarding it's efficiency; but it does affect your score and report\nUnderstanding the O factors reveals the nature of the optimal solution:\nO(1) there is a formulaic solution\nO(n) the solution has no nested loops and all happens in a single pass\nO(n+m) the solution has no nested loops, and passes over n and m only once\nO(n+n) the solution has no nested loops, but you can pass over the sequence twice\nO(n*n) the solution has a loop through n nested inside a loop through n\nthe python in operator is a list loop and could contribute an O(N) all on it's own. ie:\nfoo in bar is ok if bar is a dictionary, but a potential problem if bar is a list\nfoo in bar.keys() is a nested loop (sequentially visiting every item in the list of keys)\ncoming up with reasonable test cases, and determining the correct answers, is half the puzzle! Passing the example is not enough. Every puzzle is subjected to 'simple' tests, not unlike the one provided, and 'medium' tests- which involve arrays of significant length, to be sure you pass these tests you need to work out some way to generate a sizeable test sequence but still know the correct answer. Then there are the 'maximal' tests which seek to max-out the size and complexity so, to be certain of 100%, you need to devise tests-and the correct answers-for that too.\n\n\n\n##########################################################################################################\n\nhttps://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys\n\nhttps://cloud.google.com/compute/docs/api/how-tos/authorization\n\n\n\n\n\n\n\n\n\n\n\n#############################################################\nサーバ構成\n\nPROD環境\nPROD(STG)環境 ← PRODと全く同じ構成 Hot Standby的なイメージ\nTEST環境\nPROD環境はどちらもRAILS_ENVはPRODで動かします。\nTEST環境はSTGで動かします。\n\n※PROD, PROD(STG)の場合分けはENVではできないので、\n　.envにマッピングされるインスタンスメタデータで設定値を分ける形になります。\n\nSDK設置場所情報\n\n- プロジェクト名：beaconbank-core\n- バケット名：beaconbank-sdk\n- フォルダ構成： (OS種別)/(バージョン)/sdkファイル(zip形式)\n    - Android SDK -> Android/v1.0.0/BeaconBank_AndroidSDK.zip\n    - iOS SDK -> iOS/v1.0.0/BeaconBank_iOSSDK.zip\nGCSアクセス用証明書\njson beaconbank-core-ffaada3635a8.json\n\nTEST環境情報\n\nプロジェクトID: test-beaconbank-biz\nドメイン: t-biz.beaconbank.jp\n\nサービスアカウント用 JSON ファイル\n Key_JSON_for_GCE_test-biz\n\nLB: 130.211.24.62\nWEB01(Redis): エフェメラルIP(可変)\nWEB02: エフェメラルIP(可変)\n\nDBユーザ: beaconbank\nDBパスワード: 8gRs1T4H\n\nCORE_DBユーザ: bizbb\nCORE_DBパスワード: KPhgZtW7sUpX\n\nログの退避先: gs://test-biz-documents\n※サーバ内部ではlogrotateさせていて、\n※cronで毎朝2時にnginx, application, redisのログをCloudStorageに投げるようになっています。\n※applicationログの詳細はアプリケーションログ を参照\n\n※Cloud SQLに接続方法はsystemdに登録されたCloud SQL Proxyを使用しています。\n　各インスタンス内からmysql -u [USER] -p -h 127.0.0.1で接続できます。\n\n※GCPは同N/W上であればインスタンス名で接続できるので、redisはインスタンス名をhostとして接続する\n\nサーバ構成\n\n※全てGCP内リソース\n\nロードバランサ * 1台\nWEBサーバ + Redisサーバ * 1台\nWEBサーバ * 1台\nCloud SQL * 1台\n構成管理\n\nリソース管理（インスタンスの作成等）\nGoogle Cloud Deployment Manager\nミドル管理（nginx, ruby等のインストール）\nitamae\nデプロイ（railsアプリケーションのデプロイ）\nCapistrano（ローカル実行）\n外部連携も多いとのことで任意のタイミングでデプロイできるのがのぞましいとのことだったのでCircle CIに組み込まずローカルからcap deployで実行を想定\n構築手順\n\nGCPリソースのデプロイ\n\ndown\n[インフラ] Deployment Managerの使い方\nlink https://unerry.docbase.io/posts/215812\n\nプロビジョニング\n\ndown\n[インフラ] itamaeの使い方\nlink https://unerry.docbase.io/posts/216009\n\n秘匿情報の配置\n\n秘匿情報はbiz-envリポジトリで管理します。\n\n事前準備\n\n配置スクリプトでjqコマンドを使用するので事前にインストールしてください。\nbiz-envリポジトリルートに移動してデプロイ先の秘密鍵をssh-agentに登録します。\n\ncd biz-env/\nssh-add <xxx.biz-deploy-key> # xxx は dev|test|staging|production\n.envの配置\n\n./upload_env.sh <env> # envはdev|test|staging|production\nGCEアクセス証明書の配置\n\n./upload_key_json.sh <env> # envはdev|test|staging|production\nhtaccessの配置\n\n./upload_htaccess.sh <env> # envはdev|test|staging\nデプロイ\n\ndeployユーザのデプロイキー\n deploy_stg\nリモートからgithubへの接続必要なのでssh-agentに登録してから実行してください。\n\n注意事項\nサーバIPが変わる可能性があるのでgcloudコマンドからbiz-webNNというインスタンス名のインスタンスIPを取得してデプロイするようになっています。\ngcloudコマンドこちらからインストール・設定してください。\nソースデプロイとDBマイグレーションはtaskを分けています。マイグレーションが必要な際はメンテ画面と合わせて適宜実行してください。\n以下TEST環境へのデプロイの例\n\n// マイグレーション無しのソースのみのデプロイ\nUSE_RBENV=true bundle exec cap test deploy\n// マイグレーション有りのデプロイ\n// ※メンテにいれなくていい場合はメンテ部分はスルーで大丈夫です。\n\n// メンテナンスに入れる\nbundle exec cap test maintenance:on\n\n// ソース反映\nUSE_RBENV=true bundle exec cap test deploy\n\n// マイグレーション\nbundle exec cap test db:migrate\n\n// 許可IPのみメンテナンス解除\nbundle exec cap test maintenance:allow_ips\n\n// 確認後メンテナンス解除\nbundle exec cap test maintenance:off\nメンテナンス\n\nnginxでメンテ判定をするように設定しています。\nshared/maintenance.htmlというファイルが存在するかどうかで判定しています。\n存在する場合はshared/maintenance.htmlが表示されます。\ncapistranoからcurrent/maintenance/maintenance.htmlからコピーするタスクがあるのでそちらからon/off切り替えられます。\n\nbundle exec cap staging maintenance:on\nbundle exec cap staging maintenance:off\nshared/maintenance_allow_ipsというファイルが存在する場合は、許可IPのみメンテナンス中にアクセス可能になります。\ncapistranoからtouchで作成するタスクがあるのでそちらから切り替えます。\n\nbundle exec cap staging maintenance:allow_ips\nメンテ中に貫通できるIPは、nginx.confで管理していて、\nbeaconbank/cookbooks/itamae/nodes/*で設定するようになっています。\n追加等ある場合はこちらを編集してitamaeをdeployしてください。\n\nアプリケーションログ\n\n過去のログについては退避先のGCSバケット(※)に入っています。\n※TEST環境なら test-biz-documents\n\nログの場所\n\n各サーバーの /var/www/bizBB/shared/log/ 以下に出力されています。\n\nRailsのログ\n\nPROD環境/PROD(STG)環境: production.log\nTEST環境: staging.log\n\n※開発環境のdevelopment.logもファイルは作られますが無視してOKです\n\nSidekiqのログ\n\n※メール送信やCSVダウンロードなどのバックグラウンドワーカー\nsidekiq.log\n\nWhenever\n\n※バッチ処理\nwhenever.log\n\nその他\n\nアプリサーバーミドルウェアのログ\n\nunicorn-stdout.log\nunicorn-error.log\n\n\n\n\nkevin [9:45 AM] \n\n\nlogin: test5@example.com        pass: testtest\n\n\n\n\n\n\ncache={}\ndef fib(n) :\n   if n==0 or n==1 : return 1\n\n   try :\n     return cache[n]\n   except :\n      fb= fib(n-1) + fib(n-2)\n      cache[n]= fb\n      return fb\n\n\ndef fib2(n) :\n   if n==0 or n==1 : return 1\n\n   try :\n     return cache[n]\n   except :\n\n      fb1= fb2= 1\n      for i in xrange(2, n) :\n         fb3= fb1 + fb2\n         fb1= fb3  # n --> n-1\n         fb2= fb1  # n-1 --> n-2\n\n      cache[n]= fb3\n      return fb3\n\n\n\n\nfib(25)\n\nfib2(3700)\n\n\n\n 5/6月ともリピータは一貫して効果が高い\n\n 6月はa,bで効果が上昇\n\n\n###########################################################\na.職域（ホワイト）\nb.職域（ブルー）\nc.大規模小売店\nd.交通\ne.学校\nf.娯楽施設\ng.パチンコ\nh.スポーツ施設\ni.病院\nj.宿泊施設\nk.その他\nl.アウトドア\n\n\n\n\ne.学校\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n moto.*,saki.*\nfrom\n\n\n (select\n  ggb.group_id,log.beacon_id,log.adid,log.detected_time\n from\n  BB_beaconlog log\n  inner join\n  (select\n    gb.group_id,\n    gb.beacon_id\n   from\n    BB_group_beacon gb\n    inner join\n    (select id from BB_group where group_type=\"2\" and deleted=\"0\") g\n    on gb.group_id = g.id) ggb\n  on log.beacon_id = ggb.beacon_id\n where log.event=\"0\"\n and log.application_id=\"2170005\"\n group by ggb.group_id,log.beacon_id,log.adid,log.detected_time) moto,\n\n\n\n (select\n  ggb.group_id,log.beacon_id,log.adid,log.detected_time\n from\n  BB_beaconlog log\n  inner join\n  (select\n    gb.group_id,\n    gb.beacon_id\n   from\n    BB_group_beacon gb\n    inner join\n    (select id from BB_group where group_type=\"2\" and deleted=\"0\") g\n    on gb.group_id = g.id) ggb\n  on log.beacon_id = ggb.beacon_id\n where log.event=\"0\"\n and log.application_id=\"2170005\"\n group by ggb.group_id,log.beacon_id,log.adid,log.detected_time) saki\n\nwhere moto.adid = saki.adid\nand   moto.group_id <> saki.group_id\nand   moto.detected_time < saki.detected_time\n\n\n\n\n\n\nhttp://sinhrks.hatenablog.com/entry/2014/12/12/081841\n\n\n\n\nselect application_id as app_id, account_id,   event, app_use_detected, \n     group_id, group_name, group_type, \n     \n     manufacturer, model, os, sdk, \n     \n     beacon_id,  address_prefecture, address_city,\n     address_detail, is_fixed, is_outdoor,\n     is_public_area, install_loc_cat1id_name as install_loc_cat1id , install_loc_cat2id_name as install_loc_cat2id,\n     latitude as lat, longitude as lng,\n     \n     count(app_user_id) as n_hanoulog,  count(DISTINCT app_user_id) as n_user \n\nfrom \n   (\n  select g0.beacon_id, g0.account_id, detected_time,\n         g0.application_id, app_user_id, event, app_use_detected, \n         manufacturer, model, os, sdk,\n         \n         h1.latitude, h1.longitude, \n         group_id, name as group_name, group_type, \n         \n         address_prefecture, address_city,  address_detail, \n         is_fixed, is_outdoor,\n         is_public_area, install_loc_cat1id, install_loc_cat2id,\n         install_loc_cat2id_name, install_loc_cat1id_name, min(g0.beacon_id)\n     \n     from  `beaconbank.BB_beaconlog*`     as g0\n   LEFT JOIN  \n     (  select account_id, application_id, name, group_type,  group_id, beacon_id, latest_version\n        \n        from (\n            select  account_id, u1.application_id, name, group_type,  group_id, beacon_id, max(group_id),\n                  latest_version, \n                  CAST(FORMAT_DATE(\"%Y%m%d\", valid_form) as INT64 ) as valid_from, CAST(FORMAT_DATE(\"%Y%m%d\", valid_to) as INT64 ) as valid_to1\n            \n            FROM      beaconbank.BB_group as u1\n              LEFT JOIN  beaconbank.BB_group_beacon as u2  \n              ON         u1.id= u2.group_id AND u1.latest_version= u2.version\n            WHERE     u1.deleted=0 AND u2.is_latest=1\n            \n            GROUP BY beacon_id, account_id,  u1.application_id, name, group_type, group_id, valid_from, valid_to1, latest_version       \n            )\n        where valid_to1 >= 20170719 and valid_from <= 20170719\n     )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                AND   g0.application_id=  g3.application_id\n                AND   g0.account_id=      g3.account_id\n   \n   JOIN\n      ( select   g5.id as beacon_id, latitude, longitude, \n            address_prefecture, address_city,   address_detail, \n            is_fixed, is_outdoor,\n            is_public_area, install_loc_cat1id, install_loc_cat2id,\n            m5.name as install_loc_cat2id_name, \n            m6.name as install_loc_cat1id_name, min(is_fixed)\n       \n       from   beaconbank.BB_beacon as g5         \n       JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n       JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n       JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id  \n       \n       GROUP BY beacon_id, latitude, longitude, \n            address_prefecture, address_city,   address_detail, \n            is_fixed, is_outdoor, is_public_area, install_loc_cat1id, install_loc_cat2id,\n            install_loc_cat2id_name,    install_loc_cat1id_name           \n      )  as h1      ON  g0.beacon_id= h1.beacon_id\n\n\nWHERE _TABLE_SUFFIX BETWEEN '20170718' AND '20170719'\n    AND  CAST(FORMAT_DATETIME(\"%Y%m%d\", local_time) as INT64 )= 20170719\n\n\nGROUP BY g0.beacon_id, g0.account_id, detected_time,\n         g0.application_id, app_user_id, event, app_use_detected, \n         manufacturer, model, os, sdk,\n         \n         h1.latitude, h1.longitude, \n         group_id, group_name, group_type,\n         \n         address_prefecture, address_city,\n         address_detail,  is_fixed, is_outdoor,\n         is_public_area, install_loc_cat1id, install_loc_cat2id,\n         install_loc_cat2id_name, install_loc_cat1id_name\n)\n\nwhere  event=0\n\ngroup by  application_id, account_id,\n   event, app_use_detected,  \n   group_id, group_name, group_type, \n   manufacturer, model, os, sdk,\n   \n   beacon_id, address_prefecture, address_city,\n   address_detail,  is_fixed, is_outdoor,\n   is_public_area, install_loc_cat1id, install_loc_cat2id,\n   lat, lng\norder by application_id asc, group_type, group_id      \n\n\nss2= \"\"\"\n    select g0.beacon_id, g0.account_id, detected_time,\n           g0.application_id, app_user_id, event, app_use_detected, \n           manufacturer, model, os, sdk,\n           \n           h1.latitude, h1.longitude, \n           group_id, name as group_name, group_type, \n\n           address_prefecture, address_city,  address_detail, \n           is_fixed, is_outdoor,\n           is_public_area, install_loc_cat1id, install_loc_cat2id,\n           install_loc_cat2id_name, install_loc_cat1id_name, min(g0.beacon_id)\n     \n       from  `beaconbank.BB_beaconlog*`     as g0\n     LEFT JOIN  \n       (  select account_id, application_id, name, group_type,  group_id, beacon_id, latest_version\n\n          from (\n              select  account_id, u1.application_id, name, group_type,  group_id, beacon_id, max(group_id),\n                    latest_version, \n                    CAST(FORMAT_DATE(\"%Y%m%d\", valid_form) as INT64 ) as valid_from, CAST(FORMAT_DATE(\"%Y%m%d\", valid_to) as INT64 ) as valid_to1\n\n              FROM      beaconbank.BB_group as u1\n                LEFT JOIN  beaconbank.BB_group_beacon as u2  \n                ON         u1.id= u2.group_id AND u1.latest_version= u2.version\n              WHERE     u1.deleted=0 AND u2.is_latest=1\n\n              GROUP BY beacon_id, account_id,  u1.application_id, name, group_type, group_id, valid_from, valid_to1, latest_version       \n              )\n          where valid_to1 >= \"\"\"+ t0 + \"\"\" and valid_from <= \"\"\" + t0 + \"\"\"\n       )  as g3   ON    g0.beacon_id=       g3.beacon_id\n                  AND   g0.application_id=  g3.application_id\n                  AND   g0.account_id=      g3.account_id\n\n     JOIN\n        ( select   g5.id as beacon_id, latitude, longitude, \n              address_prefecture, address_city,   address_detail, \n              is_fixed, is_outdoor,\n              is_public_area, install_loc_cat1id, install_loc_cat2id,\n              m5.name as install_loc_cat2id_name, \n              m6.name as install_loc_cat1id_name, min(is_fixed)\n            \n         from   beaconbank.BB_beacon as g5         \n         JOIN   beaconbank.BB_beacon_attribute as g6    ON  g5.attr_estimate_id   =  g6.id\n         JOIN   beaconbank.BB_loc_category2    as m5    ON  g6.install_loc_cat2id =  m5.id  \n         JOIN   beaconbank.BB_loc_category1    as m6    ON  g6.install_loc_cat1id =  m6.id  \n                \n         GROUP BY beacon_id, latitude, longitude, \n              address_prefecture, address_city,   address_detail, \n              is_fixed, is_outdoor, is_public_area, install_loc_cat1id, install_loc_cat2id,\n              install_loc_cat2id_name,    install_loc_cat1id_name           \n        )  as h1      ON  g0.beacon_id= h1.beacon_id\n               \n\nWHERE _TABLE_SUFFIX BETWEEN '\"\"\"+ta+\"\"\"' AND '\"\"\"+tb+\"\"\"'\n      AND  CAST(FORMAT_DATETIME(\"%Y%m%d\", local_time) as INT64 )= \"\"\"+t0+\"\"\"\n\n\nGROUP BY g0.beacon_id, g0.account_id, detected_time,\n           g0.application_id, app_user_id, event, app_use_detected, \n           manufacturer, model, os, sdk,\n           \n           h1.latitude, h1.longitude, \n           group_id, group_name, group_type,\n\n           address_prefecture, address_city,\n           address_detail,  is_fixed, is_outdoor,\n           is_public_area, install_loc_cat1id, install_loc_cat2id,\n           install_loc_cat2id_name, install_loc_cat1id_name\n\n \n\n\"\"\"\n\n以下のロケごとに, HOT-COLDプロファイル:\n a.職域（ホワイト）\t\n b.職域（ブルー）\n c.大規模小売店\t\t\n d.交通\n e.学校\n g.パチンコ\t\t\t \n l.アウトドア \n\n\n\n\nCOFFEE-BLACK-CAN-190\nCOFFEE-BLACK-BOTTLECAN-300\nCOFFEE-CAN-190\nCOFFEE-BOTTLECAN-300\nCOFFEE-CAFEOLAIT-CAN-190\nCOFFEE-CAFEOLAIT-PET-280\nCOFFEE-SUGAR-CAN-190\nTEA-PET-280\nBLACK TEA-CAN-280\n\n\n\n\n\ndf2  = bq_sql(  ss2  )\n  \n\n\n\n\n######################################################################\n######\nPackages Ubuntu \n\n\napache2-bin (2.4.7-1ubuntu4.17 [amd64, i386], 2.4.7-1ubuntu4 [arm64, armhf, powerpc, ppc64el]) [security]\nApache HTTP Server (binary files and modules)\napache2-data (2.4.7-1ubuntu4.17) [security]\nApache HTTP Server (common files)\napache2-dev (2.4.7-1ubuntu4.17 [amd64, i386], 2.4.7-1ubuntu4 [arm64, armhf, powerpc, ppc64el]) [security]\nApache HTTP Server (development headers)\napache2-suexec-pristine (2.4.7-1ubuntu4.17 [amd64, i386], 2.4.7-1ubuntu4 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nApache HTTP Server standard suexec program for mod_suexec\napache2.2-bin (2.4.7-1ubuntu4.17 [amd64, i386], 2.4.7-1ubuntu4 [arm64, armhf, powerpc, ppc64el]) [security]\nTransitional package for apache2-bin\nerlang-yaws (1.98-2) [universe]\nErlang application which implements HTTP webserver\nlibapache2-mod-auth-pgsql (2.0.3-6)\nModule for Apache2 which provides PostgreSQL authentication\nlibapache2-mod-auth-pubtkt (0.8-3) [universe]\nkey-based single-sign-on authentication module for Apache\nlibapache2-mod-auth-tkt (2.1.0-8) [universe]\nlightweight single-sign-on authentication module for Apache\nlibapache2-mod-authn-webid (0~20110301-2) [universe]\nWebID FOAF+SSL authentication module for Apache\nlibapache2-mod-dacs (1.4.28b-3ubuntu1) [universe]\nDistributed Access Control System (DACS) - Apache Module\nlibapache2-mod-musicindex (1.4.1-1) [universe]\nBrowse, stream, download and search through MP3/Ogg/FLAC/MP4 files\nlibapache2-mod-netcgi-apache (3.7.3-3build2) [universe]\nOCaml application-level Internet libraries - netcgi2 Apache2 connector\nlibapache2-mod-php5 (5.5.9+dfsg-1ubuntu4.21 [amd64, i386], 5.5.9+dfsg-1ubuntu4 [arm64, armhf, powerpc, ppc64el]) [security]\nserver-side, HTML-embedded scripting language (Apache 2 module)\nlibapache2-mod-php5filter (5.5.9+dfsg-1ubuntu4.21 [amd64, i386], 5.5.9+dfsg-1ubuntu4 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nserver-side, HTML-embedded scripting language (apache 2 filter module)\nlibapache2-mod-proxy-msrpc (0.4-1ubuntu1) [universe]\nApache module for Outlook Anywhere support in reverse proxy setups\nlibapache2-mod-qos (10.28-1) [universe]\nquality of service module for the apache2\nlibapache2-mod-security2 (2.7.7-2) [universe]\nTighten web applications security for Apache\nlibapache2-mod-svn (1.8.8-1ubuntu3.2 [amd64, i386], 1.8.8-1ubuntu3 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nApache Subversion server modules for Apache httpd\nlibapache2-mod-webauth (4.5.5-2) [universe]\nApache module for WebAuth authentication\nlibapache2-mod-webauthldap (4.5.5-2) [universe]\nApache module for WebAuth LDAP lookup and authorization\nlibapache2-mod-webkdc (4.5.5-2) [universe]\nApache modules for a WebAuth authentication KDC\nlibapache2-mod-wsgi-py3 (3.4-4ubuntu2.1.14.04.2 [amd64, i386], 3.4-4ubuntu2 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nPython 3 WSGI adapter module for Apache\nlibapache2-modsecurity (2.7.7-2) [universe]\nDummy transitional package\nmodsecurity-crs (2.2.8-1) [universe]\nmodsecurity's Core Rule Set\nnginx-common (1.4.6-1ubuntu3.8) [security]\nsmall, powerful, scalable web/proxy server - common files\nnginx-core (1.4.6-1ubuntu3.8 [amd64, i386], 1.4.6-1ubuntu3 [arm64, armhf, powerpc, ppc64el]) [security]\nnginx web/proxy server (core version)\nnginx-extras (1.4.6-1ubuntu3.8 [amd64, i386], 1.4.6-1ubuntu3 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nnginx web/proxy server (extended version)\nnginx-full (1.4.6-1ubuntu3.8 [amd64, i386], 1.4.6-1ubuntu3 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nnginx web/proxy server (standard version)\nnginx-light (1.4.6-1ubuntu3.8 [amd64, i386], 1.4.6-1ubuntu3 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nnginx web/proxy server (basic version)\nnginx-naxsi (1.4.6-1ubuntu3.8 [amd64, i386], 1.4.6-1ubuntu3 [arm64, armhf, powerpc, ppc64el]) [universe] [security]\nnginx web/proxy server (version with naxsi)\nnginx-naxsi-ui (1.4.6-1ubuntu3.8) [universe] [security]\nnginx web/proxy server - naxsi configuration front-end\nocsigenserver (2.2.0-3) [universe]\nweb server of the Ocsigen project\nqweborf (0.13-3) [universe]\nShares files using the HTTP protocol\nweborf-daemon (0.13-3) [universe]\ninit script for weborf\n\n\n\n\nhttps://packages.ubuntu.com/trusty/devel/\n\n\nsudo apt-get install default-jdk\nsudo apt-get install gzip\n\nsudo apt-get update\napt-cache dump\n\nsudo apt-get install dos2unix\nsudo software-properties-gtk\nsudo apt-get update\nsudo apt-get install rar\n\n\n\nZsh\n\nTmux\n\nVim\n\nRVM\n\nBashrc \n\n\n\n\n\n\n\n\n\n\n\nJulia Install Instructions :\nGeneric Linux Binaries\n https://julialang.org/downloads/platform.html\n\n The generic Linux binaries do not require any special installation steps, \n but you will need to ensure that your system can find the julia executable. \n First, extract the .tar.gz file downloaded from the downloads page to a folder on your computer. \n To run Julia, you can do any of the following:\n\n  Create a symbolic link to julia inside a folder which is on your system PATH\n  Add Julia’s bin folder to your system PATH environment variable\n  Invoke the julia executable by using its full path, as in <where you extracted Julia>/bin/julia\n  For example, to create a symbolic link to julia inside the /usr/local/bin folder, you can do the following:\n\n  sudo ln -s <where you extracted the julia archive>/bin/julia /usr/local/bin/julia\n\n\n\n\nsudo apt-get remove scala-library scala\nsudo wget www.scala-lang.org/files/archive/scala-2.10.4.deb\nsudo dpkg -i scala-2.10.4.deb\nsudo apt-get update\nsudo apt-get install scala\nwget http://scalasbt.artifactoryonline.com/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.12.4/sbt.deb\nsudo dpkg -i sbt.deb\nsudo apt-get update\nsudo apt-get install sbt\nsudo add-apt-repository ppa:webupd8team/sublime-text-3\nsudo apt-get update\nsudo apt-get install sublime-text-installer\n\n\n\nsudo add-apt-repository ppa:ubuntu-elisp/ppa\nsudo apt-get update\nsudo apt-get install emacs-snapshot emacs-snapshot-el\nsudo apt-get install virtualbox\n\n\nsudo apt-get install cue2toc - converts CUE files to cdrdao's TOC format\nsudo apt-get install easytag - viewing, editing and writing ID3 tags\nsudo apt-get install id3v2 - A command line id3v2 tag editor\n\n\n\nこれはあなたが提案している通りです。\n\n\n\n\ncd\nwget -c http://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh\nchmod +x Miniconda-latest-Linux-x86_64.sh\n./Miniconda-latest-Linux-x86_64.sh\n\n\n#Install into :\n/home/noel/anaconda27\n\n\nconda install numba\n\n\nsudo apt-get install r-base-dev\n\nsudo dpkg -i cuda-repo-ubuntu1404_7.5-18_amd64.deb \nsudo apt-get install cuda\n\n\n\n\n\napache2-bin \napache2-data \napache2-dev \napache2-suexec-pristine \napache2.2-bin \nerlang-yaws \nlibapache2-mod-auth-pubtkt \nlibapache2-mod-auth-tkt \nlibapache2-mod-authn-webid \nlibapache2-mod-dacs \nlibapache2-mod-musicindex \nlibapache2-mod-netcgi-apache \nlibapache2-mod-php5 \nlibapache2-mod-php5filter \nlibapache2-mod-proxy-msrpc \nlibapache2-mod-qos \nlibapache2-mod-security2 \nlibapache2-mod-svn \nlibapache2-mod-webauth \nlibapache2-mod-webauthldap \nlibapache2-mod-webkdc \nlibapache2-mod-wsgi-py3 \nlibapache2-modsecurity \nmodsecurity-crs \nnginx-common \nnginx-core \nnginx-extras \nnginx-full \nnginx-light \nnginx-naxsi \nnginx-naxsi-ui \nocsigenserver \nqweborf \nweborf-daemon \n\n\n\n\n\n\n\n Package Name\tAccess\tSummary\t Updated\n numba\tpublic\ta just-in-time Python function compiler based on LLVM\t2017-07-17\n conda-build\tpublic\tCommands and tools for building conda packages\t2017-07-14\n scipy\tpublic\tScientific Library for Python\t2017-07-14\n six\tpublic\tNo Summary\t2017-07-13\n glob2\tpublic\tEnhanced glob that can capture patterns and supports recursive wildcards\t2017-07-13\n cx_oracle\tpublic\tPython interface to Oracle\t2017-07-12\n openjdk\tpublic\tNo Summary\t2017-07-11\n cudatoolkit\tpublic\tNo Summary\t2017-07-11\n fastparquet\tpublic\tPython interface to the parquet format\t2017-07-11\n cudnn\tpublic\tNo Summary\t2017-07-11\n pyutilib\tpublic\tPyUtilib: A collection of Python utilities\t2017-07-11\n maven\tpublic\tA software project management and comprehension tool.\t2017-07-11\n nose-parameterized\tpublic\tParameterized testing with any Python test framework\t2017-07-11\n leveldb\tpublic\tA fast key-value storage library providing ordered mappings.\t2017-07-11\n gtest\tpublic\tGoogle's C++ test framework\t2017-07-11\n pandas-profiling\tpublic\tGenerate profile report for pandas DataFrame\t2017-07-11\n neo4j-python-driver\tpublic\tDatabase connector for Neo4j graph database\t2017-07-11\n llvmlite\tpublic\tA lightweight LLVM python binding for writing JIT compilers\t2017-07-11\n entrypoints\tpublic\tNo Summary\t2017-07-10\n sympy\tpublic\tPython library for symbolic mathematics\t2017-07-10\n dbus\tpublic\tmessage bus system, a simple way for applications to talk to one another\t2017-07-10\n pillow\tpublic\tThe friendly Python Imaging Library(PIL) fork\t2017-07-08\n numpy\tpublic\tarray processing for numbers, strings, records, and objects\t2017-07-07\n astropy\tpublic\tCommunity-developed Python Library for Astronomy\t2017-07-07\n gflags\tpublic\tA C++ library that implements commandline flags processing.\t2017-07-07\n service_identity\tpublic\tService identity verification for pyOpenSSL\t2017-07-06\n holoviews\tpublic\tStop plotting your data - annotate your data and let it visualize itself.\t2017-07-06\n lz4\tpublic\tBindings for the lz4 compression library\t2017-07-05\n s3fs\tpublic\tconvenient Filesystem interface over S3\t2017-07-05\n hdf5\tpublic\tNo Summary\t2017-07-05\n django\tpublic\tWeb framework that encourages rapid development\t2017-07-05\n nccl\tpublic\tOptimized primitives for collective multi-GPU communication\t2017-07-05\n icu\tpublic\tNo Summary\t2017-07-05\n caffe-gpu\tpublic\tA deep learning framework made with expression, speed, and modularity in mind.\t2017-07-05\n caffe\tpublic\tA deep learning framework made with expression, speed, and modularity in mind.\t2017-07-05\n boost\tpublic\tNo Summary\t2017-07-05\n libtorch-gpu\tpublic\tTorch libraries for use in PyTorch, GPU enabled version.\t2017-07-05\n libtorch\tpublic\tTorch libraries for use in PyTorch.\t2017-07-05\n pyodbc\tpublic\tDB API Module for ODBC\t2017-07-03\n thinc\tpublic\tLearn sparse linear models\t2017-06-30\n spacy\tpublic\tIndustrial-strength Natural Language Processing\t2017-06-30\n ftfy\tpublic\tfixes some problems with Unicode text after the fact\t2017-06-30\n regex\tpublic\talternative regular expression module, to replace re\t2017-06-30\n preshed\tpublic\tCython hash table that trusts the keys are pre-hashed\t2017-06-30\n murmurhash\tpublic\tA non-cryptographic hash function\t2017-06-30\n\n\n Package Name\tAccess\tSummary\t Updated\n pymc3\tpublic\tprobabilistic Programming in Python\t2017-06-30\n python-blosc\tpublic\tA Python wrapper for the extremely fast Blosc compression library\t2017-06-30\n pytest-xdist\tpublic\tpy.test xdist plugin for distributed testing and loop-on-failing modes\t2017-06-30\n pydotplus\tpublic\tPython interface to the Graphviz Dot language\t2017-06-30\n lmdb\tpublic\tA high-performance embedded transactional key-value store database.\t2017-06-30\n glog\tpublic\tSimple Google-style logging wrapper for Python.\t2017-06-30\n curl\tpublic\tTool and library for transferring data with URL syntax\t2017-06-30\n c-blosc\tpublic\tNo Summary\t2017-06-30\n protobuf\tpublic\tNo Summary\t2017-06-29\n termcolor\tpublic\tANSII Color formatting for output in terminal\t2017-06-29\n pathlib\tpublic\tobject-oriented filesystem paths\t2017-06-29\n chainer\tpublic\tflexible framework of neural networks for deep learning\t2017-06-29\n yarl\tpublic\tYet another URL library\t2017-06-29\n continuum-docs\tpublic\tNo Summary\t2017-06-29\n mkl\tpublic\tMath library for Intel and compatible processors\t2017-06-29\n urllib3\tpublic\tHTTP library with thread-safe connection pooling, file post, and more.\t2017-06-29\n typing\tpublic\tbackport of the standard library typing module to Python versions older than 3.6\t2017-06-29\n the-silver-searcher\tpublic\tA code searching tool similar to ack, with a focus on speed.\t2017-06-29\n sphinxcontrib-websupport\tpublic\tSphinx API for Web Apps\t2017-06-29\n sphinxcontrib\tpublic\tPython namespace for sphinxcontrib\t2017-06-29\n scikit-learn\tpublic\tset of python modules for machine learning and data mining\t2017-06-29\n rapidjson\tpublic\tA fast JSON parser/generator for C++ with both SAX/DOM style API\t2017-06-29\n qgrid\tpublic\tPandas DataFrame viewer for IPython Notebook\t2017-06-29\n python-rapidjson\tpublic\tPython wrapper around rapidjson\t2017-06-29\n pytest-asyncio\tpublic\tPytest support for asyncio\t2017-06-29\n pygpu\tpublic\tNo Summary\t2017-06-29\n ninja\tpublic\tA small build system with a focus on speed\t2017-06-29\n libssh2\tpublic\tthe SSH library\t2017-06-29\n libgpuarray\tpublic\tNo Summary\t2017-06-29\n libconda\tpublic\tconda 4.0 based library\t2017-06-29\n jupyter_client\tpublic\tJupyter protocol implementation and client libraries\t2017-06-29\n isort\tpublic\tPython utility / library to sort Python imports\t2017-06-29\n hyperlink\tpublic\tImmutable, Pythonic, correct URLs\t2017-06-29\n cram\tpublic\tA simple testing framework for command line applications\t2017-06-29\n functools_lru_cache\tpublic\tbackport of functools.lru_cache from Python 3.3\t2017-06-29\n coverage\tpublic\tCode coverage measurement for Python\t2017-06-29\n aiofiles\tpublic\tPython library for handling local disk files in asyncio applications\t2017-06-29\n word2vec\tpublic\tPython interface to Google word2vec\t2017-06-29\n portpicker\tpublic\tA library to choose unique available network ports.\t2017-06-29\n pattern\tpublic\tVisual analysis and diagnostic tools to facilitate ML model selection\t2017-06-29\n tensorflow\tpublic\tNo Summary\t2017-06-23\n funcsigs\tpublic\tNo Summary\t2017-06-23\n gensim\tpublic\tNo Summary\t2017-06-23\n netcdf4\tpublic\tNo Summary\t2017-06-22\n libnetcdf\tpublic\tNo Summary\t2017-06-22\n param\tpublic\tNo Summary\t2017-06-22\n wget\tpublic\tNo Summary\t2017-06-22\n vs2015_runtime\tpublic\tNo Summary\t2017-06-21\n libprotobuf\tpublic\tNo Summary\t2017-06-21\n numexpr\tpublic\tNo Summary\t2017-06-20\n\n\n Package Name\tAccess\tSummary\t Updated\n cvxopt\tpublic\tNo Summary\t2017-06-20\n sqlalchemy\tpublic\tNo Summary\t2017-06-20\n _nb_ext_conf\tpublic\tNo Summary\t2017-06-20\n lazy-object-proxy\tpublic\tNo Summary\t2017-06-20\n astroid\tpublic\tNo Summary\t2017-06-20\n pytest\tpublic\tNo Summary\t2017-06-19\n nb_conda_kernels\tpublic\tNo Summary\t2017-06-19\n nb_conda\tpublic\tNo Summary\t2017-06-19\n nb_anacondacloud\tpublic\tNo Summary\t2017-06-19\n zope.interface\tpublic\tNo Summary\t2017-06-18\n twisted\tpublic\tNo Summary\t2017-06-18\n sphinx\tpublic\tNo Summary\t2017-06-18\n smart_open\tpublic\tNo Summary\t2017-06-18\n pomegranate\tpublic\tNo Summary\t2017-06-18\n more-itertools\tpublic\tNo Summary\t2017-06-18\n distributed\tpublic\tNo Summary\t2017-06-18\n dask\tpublic\tNo Summary\t2017-06-18\n conda\tpublic\tNo Summary\t2017-06-18\n chardet\tpublic\tNo Summary\t2017-06-18\n botocore\tpublic\tNo Summary\t2017-06-18\n bokeh\tpublic\tNo Summary\t2017-06-18\n bkcharts\tpublic\tNo Summary\t2017-06-18\n anaconda-verify\tpublic\tNo Summary\t2017-06-18\n anaconda-navigator\tpublic\tNo Summary\t2017-06-18\n bazel\tpublic\tNo Summary\t2017-06-18\n xarray\tpublic\tNo Summary\t2017-06-09\n aiohttp\tpublic\tNo Summary\t2017-06-08\n pyopengl-accelerate\tpublic\tNo Summary\t2017-06-08\n pyopengl\tpublic\tNo Summary\t2017-06-08\n pyamg\tpublic\tNo Summary\t2017-06-08\n scikit-bio\tpublic\tNo Summary\t2017-06-08\n pywavelets\tpublic\tNo Summary\t2017-06-08\n bottleneck\tpublic\tNo Summary\t2017-06-08\n basemap\tpublic\tNo Summary\t2017-06-08\n biopython\tpublic\tNo Summary\t2017-06-08\n scikit-image\tpublic\tNo Summary\t2017-06-08\n pytables\tpublic\tNo Summary\t2017-06-08\n matplotlib\tpublic\tNo Summary\t2017-06-08\n h5py\tpublic\tNo Summary\t2017-06-08\n statsmodels\tpublic\tNo Summary\t2017-06-08\n pandas\tpublic\tNo Summary\t2017-06-08\n pytest-mock\tpublic\tNo Summary\t2017-06-07\n plotly\tpublic\tNo Summary\t2017-06-07\n gevent\tpublic\tNo Summary\t2017-06-07\n multidict\tpublic\tNo Summary\t2017-06-07\n async-timeout\tpublic\tNo Summary\t2017-06-07\n javabridge\tpublic\tNo Summary\t2017-06-07\n glueviz\tpublic\tNo Summary\t2017-06-07\n glue-vispy-viewers\tpublic\tNo Summary\t2017-06-07\n glue-core\tpublic\tNo Summary\t2017-06-07\n\n graphviz\tpublic\tNo Summary\t2017-06-06\n py\tpublic\tNo Summary\t2017-06-05\n lxml\tpublic\tNo Summary\t2017-06-05\n parsel\tpublic\tNo Summary\t2017-06-02\n nodejs\tpublic\tNo Summary\t2017-06-02\n ipython\tpublic\tNo Summary\t2017-06-02\n bcolz\tpublic\tNo Summary\t2017-06-02\n requests-ftp\tpublic\tNo Summary\t2017-06-01\n pandas-datareader\tpublic\tNo Summary\t2017-06-01\n hdf4\tpublic\tNo Summary\t2017-06-01\n boto\tpublic\tNo Summary\t2017-06-01\n testpath\tpublic\tNo Summary\t2017-06-01\n nltk\tpublic\tNo Summary\t2017-06-01\n fabric\tpublic\tNo Summary\t2017-06-01\n tqdm\tpublic\tNo Summary\t2017-06-01\n semver\tpublic\tNo Summary\t2017-06-01\n nbconvert\tpublic\tNo Summary\t2017-06-01\n navigator-updater\tpublic\tNo Summary\t2017-05-30\n anaconda\tpublic\tNo Summary\t2017-05-30\n anaconda-project\tpublic\tNo Summary\t2017-05-26\n openssl\tpublic\tNo Summary\t2017-05-25\n munch\tpublic\tNo Summary\t2017-05-24\n pycrypto\tpublic\tNo Summary\t2017-05-24\n caffe-nv-gpu\tpublic\tNo Summary\t2017-05-23\n python-graphviz\tpublic\tNo Summary\t2017-05-22\n menuinst\tpublic\tNo Summary\t2017-05-19\n libgfortran\tpublic\tNo Summary\t2017-05-19\n qt\tpublic\tNo Summary\t2017-05-17\n contextlib2\tpublic\tNo Summary\t2017-05-17\n flask\tpublic\tNo Summary\t2017-05-17\n futures\tpublic\tNo Summary\t2017-05-16\n colorama\tpublic\tNo Summary\t2017-05-16\n werkzeug\tpublic\tNo Summary\t2017-05-16\n pyopenssl\tpublic\tNo Summary\t2017-05-16\n packaging\tpublic\tNo Summary\t2017-05-16\n openpyxl\tpublic\tNo Summary\t2017-05-16\n idna\tpublic\tNo Summary\t2017-05-16\n cryptography\tpublic\tNo Summary\t2017-05-16\n cffi\tpublic\tNo Summary\t2017-05-16\n asn1crypto\tpublic\tNo Summary\t2017-05-16\n jedi\tpublic\tNo Summary\t2017-05-15\n python\tpublic\tNo Summary\t2017-05-15\n nsis\tpublic\tNo Summary\t2017-05-15\n requests\tpublic\tNo Summary\t2017-05-12\n beautifulsoup4\tpublic\tNo Summary\t2017-05-11\n natsort\tpublic\tNo Summary\t2017-05-10\n markdown2\tpublic\tNo Summary\t2017-05-10\n zict\tpublic\tNo Summary\t2017-05-09\n tornado\tpublic\tNo Summary\t2017-05-09\n snappy\tpublic\tNo Summary\t2017-05-09\n\n\npartd\tpublic\tNo Summary\t2017-05-09\n anaconda-client\tpublic\tNo Summary\t2017-05-09\n theano\tpublic\tNo Summary\t2017-05-08\n scikit-fmm\tpublic\tNo Summary\t2017-05-08\n python-socketio\tpublic\tNo Summary\t2017-05-08\n python-lmdb\tpublic\tNo Summary\t2017-05-08\n python-leveldb\tpublic\tNo Summary\t2017-05-08\n python-gflags\tpublic\tNo Summary\t2017-05-08\n python-engineio\tpublic\tNo Summary\t2017-05-08\n opencv\tpublic\tNo Summary\t2017-05-08\n flask-socketio\tpublic\tNo Summary\t2017-05-08\n eigen\tpublic\tNo Summary\t2017-05-08\n caffe-nv\tpublic\tNo Summary\t2017-05-08\n spyder\tpublic\tNo Summary\t2017-05-02\n datashader\tpublic\tNo Summary\t2017-04-25\n msinttypes\tpublic\tNo Summary\t2017-04-25\n progressbar2\tpublic\tNo Summary\t2017-04-18\n path.py\tpublic\tNo Summary\t2017-04-18\n jsonschema\tpublic\tNo Summary\t2017-04-18\n cssselect\tpublic\tNo Summary\t2017-04-18\n joblib\tpublic\tNo Summary\t2017-04-17\n ipykernel\tpublic\tNo Summary\t2017-04-17\n humanize\tpublic\tNo Summary\t2017-04-17\n keras-gpu\tpublic\tNo Summary\t2017-04-13\n keras\tpublic\tNo Summary\t2017-04-13\n setuptools_scm\tpublic\tNo Summary\t2017-04-13\n pytest-runner\tpublic\tNo Summary\t2017-04-13\n tblib\tpublic\tNo Summary\t2017-04-12\n kealib\tpublic\tNo Summary\t2017-04-12\n python-utils\tpublic\tNo Summary\t2017-04-12\n psutil\tpublic\tNo Summary\t2017-04-12\n picklable-itertools\tpublic\tNo Summary\t2017-04-12\n gevent-websocket\tpublic\tNo Summary\t2017-04-12\n fuel\tpublic\tNo Summary\t2017-04-12\n dill\tpublic\tNo Summary\t2017-04-12\n pyqtgraph\tpublic\tNo Summary\t2017-04-09\n orange3\tpublic\tNo Summary\t2017-04-09\n anyqt\tpublic\tNo Summary\t2017-04-09\n pathlib2\tpublic\tNo Summary\t2017-04-07\n tornado-json\tpublic\tNo Summary\t2017-04-07\n tabpy-server\tpublic\tNo Summary\t2017-04-07\n tabpy-client\tpublic\tNo Summary\t2017-04-07\n jinja2\tpublic\tNo Summary\t2017-04-07\n gitpython\tpublic\tNo Summary\t2017-04-07\n genson\tpublic\tNo Summary\t2017-04-07\n wrapt\tpublic\tNo Summary\t2017-04-05\n sortedcollections\tpublic\tNo Summary\t2017-04-05\n prompt_toolkit\tpublic\tNo Summary\t2017-04-05\n notebook\tpublic\tNo Summary\t2017-04-05\n sphinx_rtd_theme\tpublic\tNo Summary\t2017-04-04\n\n\n\n\n\n\n\n####  Packages #########################################\nBabel\t2.3.3\t2.4.0\nBottleneck\t1.1.0\t1.2.1\nCython\t0.24\t0.26\nDjango\t1.10.5\t1.11.3\nFlask\t0.11.1\t0.12.2\nFlask-Cors\t2.1.2\t3.0.3\nGDAL\t2.2.0\t2.2.1\nHeapDict\t1.0.0\t1.0.0\nJinja2\t2.8\t2.9.6\nMarkupSafe\t0.23\t1.0\nOrange\t2.7.8\t2.7.8\nPattern\t2.6\t2.6\nPillow\t3.2.0\t4.2.1\nPyDrive\t1.2.1\t1.3.1\nPyMySQL\t0.7.9\t0.7.11\nPyYAML\t3.11\t3.12\nPygments\t2.1.3\t2.2.0\nQtPy\t1.0.2\t1.2.1\nQuandl\t2.8.9\t3.2.0\nSQLAlchemy\t1.0.13\t1.2.0b1\nStarCluster\t0.95.6\t0.95.6\nTPOT\t0.6.4\t0.8.3\nWerkzeug\t0.11.10\t0.12.2\nXlsxWriter\t0.9.2\t0.9.8\nYahoo-ticker-downloader\t0.8.1\t2.1.0\n_nb_ext_conf\t0.2.0\t\nalabaster\t0.7.8\t0.7.10\namqp\t1.4.9\t2.2.1\nanaconda-client\t1.4.0\t1.2.2\nanaconda-navigator\t1.2.1\t\nanyjson\t0.3.3\t0.3.3\nargcomplete\t1.0.0\t1.8.2\nargs\t0.1.0\t0.1.0\narrow\t0.10.0\t0.10.0\nattrdict\t2.0.0\t2.0.0\nbabel\t2.3.3\t\nbackports\t1.0\t1.0\nbackports-abc\t0.4\t\nbackports.shutil-get-terminal-size\t1.0.0\t\nbackports.ssl-match-hostname\t3.4.0.2\t\nbackports_abc\t0.4\t0.5\nbcolz\t1.0.0\t1.1.2\nbeautifulsoup4\t4.4.1\t4.6.0\nbilliard\t3.3.0.23\t3.5.0.3\nbitarray\t0.8.1\t0.8.1\nblaze\t0.11.2\t0.10.1\nbokeh\t0.12.3\t0.12.6\nboost\t1.57.0\t0.1\nboto\t2.40.0\t2.48.0\nboto3\t1.4.1\t1.4.4\nboto3\t1.4.0\t1.4.4\nbotocore\t1.4.68\t1.5.85\nbotocore\t1.4.49\t1.5.85\nbottleneck\t1.1.0\t\nbqplot\t0.8.4\t0.10.0a2\nbrewer2mpl\t1.4.1\t1.4.1\nbzip2\t1.0.6\t1.0.6\ncategory-encoders\t1.2.2\t\ncategory_encoders\t1.2.2\t1.2.4\ncdecimal\t2.3\t2.3\ncelery\t3.1.23\t4.0.2\ncertifi\t2017.4.17\t2017.4.17\ncffi\t1.6.0\t1.10.0\nchardet\t3.0.4\t3.0.4\nchest\t0.2.3\t0.2.3\nclick\t6.7\t6.7\nclick\t6.6\t6.7\nclint\t0.5.1\t0.5.1\ncloudpickle\t0.2.2\t0.3.1\nclyent\t1.2.2\t1.2.1\ncolorama\t0.3.7\t0.3.9\ncomtypes\t1.1.2\t1.1.3\nconda\t4.3.22\t4.3.16\nconda-build\t1.21.14\t2.1.5\nconda-build\t1.21.14+0.g4dfebe9.dirty\t2.1.5\nconda-env\t2.6.0\t2.4.2\nconfigobj\t5.0.6\t5.0.6\nconfigparser\t3.5.0b2\t3.5.0b2\nconsole_shortcut\t0.1.1\t\ncontextlib2\t0.5.3\t0.5.5\nconvertdate\t2.1.0\t\ncryptography\t1.4\t1.8.1\ncurl\t7.49.0\t\ncycler\t0.10.0\t\ncython\t0.24\t\ncytoolz\t0.8.2\t0.8.2\ndask\t0.13.0\t0.15.0\ndatacleaner\t0.1.4\t\ndatashape\t0.5.3\t0.5.4\ndateparser\t0.5.1\t\ndeap\t1.0.2\t\ndeap\t1.0.2.post2\t\ndecorator\t4.0.10\t4.0.11\ndill\t0.2.5\t0.2.6\ndiskcache\t2.4.1\t\ndistributed\t1.15.0\t1.17.1\ndjango\t1.10.5\t1.10.5\ndjango-ipware\t1.1.6\t\ndjango-registration-redux\t1.4\t\ndocutils\t0.12\t0.13.1\nentrypoints\t0.2.2\t0.2.2\nenum34\t1.1.6\t1.1.6\nephem\t3.7.6.0\t3.7.5.3\net-xmlfile\t1.0.1\t\net_xmlfile\t1.0.1\t1.0.1\nexpat\t2.1.0\t\nfastcache\t1.0.2\t1.0.2\nfastcluster\t1.1.20\t\nfastcsv\t0.1.2\t\nfastnumbers\t1.0.0\t\nfilterpy\t0.1.4\t\nflask\t0.11.1\t0.12.2\nflask-cors\t2.1.2\t3.0.2\nfolium\t0.2.1\t\nfreetype\t2.5.5\t2.5.5\nfreexl\t1.0.2\t\nfuncsigs\t1.0.2\t1.0.2\nfunctools32\t3.2.3.post2\t3.2.3.2\nfunctools32\t3.2.3.2\t3.2.3.2\nfuture\t0.15.2\t0.16.0\nfutures\t3.0.5\t3.1.1\ngdal\t2.2.0\t2.1.0\ngeopy\t1.11.0\t\ngeos\t3.5.0\t3.5.0\nget_terminal_size\t1.0.0\t1.0.0\ngevent\t1.1.1\t1.2.2\nggplot\t0.11.5\t\ngoogle-api-python-client\t1.6.2\t\ngoogle-api-python-client\t1.5.3\t\ngplearn\t0.1.0\t\ngreenlet\t0.4.10\t0.4.12\ngrin\t1.2.1\t1.2.1\ngspread\t0.4.1\t\nguidata\t1.7.6\t\nh2o\t3.10.0.8\t3.10.0.9\nh5py\t2.7.0\t2.7.0\nhdbscan\t0.8.3\t\nhdf4\t4.2.12\t4.2.12\nhdf5\t1.8.17\t1.8.17\nheapdict\t1.0.0\t1.0.0\nhttplib2\t0.10.3\t\nidna\t2.1\t2.5\nidna\t2.5\t2.5\nimagesize\t0.7.1\t0.7.1\nipaddress\t1.0.16\t1.0.18\niptools\t0.6.1\t\nipykernel\t4.3.1\t4.6.1\nipyleaflet\t0.2.1\t\nipyparallel\t5.2.0\t6.0.2\nipython\t4.2.0\t6.1.0\nipython-genutils\t0.1.0\t\nipython_genutils\t0.1.0\t0.2.0\nipywidgets\t5.2.2\t6.0.0\niso8601\t0.1.11\t0.1.11\nitsdangerous\t0.24\t0.24\njavabridge\t1.0.14\t1.0.14\njdatetime\t1.8.1\t\njdcal\t1.2\t1.3\njedi\t0.8.1\t0.10.2\njedi\t0.9.0\t0.10.2\njinja2\t2.8\t2.9.6\njmespath\t0.9.0\t0.9.0\njoblib\t0.10.2\t0.11\njpeg\t9b\t9b\njsonschema\t2.5.1\t2.6.0\njupyter\t1.0.0\t1.0.0\njupyter-client\t4.3.0\t\njupyter-console\t4.1.1\t\njupyter-core\t4.1.0\t\njupyter-dashboards\t0.6.1\t\njupyter_client\t4.3.0\t5.0.1\njupyter_console\t4.1.1\t5.1.0\njupyter_core\t4.1.0\t4.3.0\njupyter_dashboards\t0.6.1\t\nkealib\t1.4.7\t1.4.6\nkmodes\t0.6\t\nkombu\t3.0.35\t\nlibgdal\t2.0.0\t2.1.0\nlibiconv\t1.14\t1.14\nlibnetcdf\t4.4.1.1\t4.3.3.1\nlibpng\t1.6.28\t1.6.27\nlibpq\t9.5.4\t9.5.4\nlibspatialite\t4.3.0a\t\nlibtiff\t4.0.6\t4.0.6\nlibxml2\t2.9.3\t\nllvmlite\t0.11.0\t0.18.0\nlocket\t0.2.0\t0.2.0\nlogilab-astng\t0.24.3\t\nlogilab-common\t1.2.2\t1.0.2\nlxml\t3.6.0\t3.8.0\nmarkupsafe\t0.23\t0.23\nmarshmallow\t2.13.5\t\nmatplotlib\t1.5.1\t2.0.2\nmenuinst\t1.4.1\t1.4.7\nmistune\t0.7.2\t0.7.4\nmkl\t11.3.3\t2017.0.1\nmkl-service\t1.1.2\t1.1.2\nmlxtend\t0.4.2\t\nmpld3\t0.3\t0.2\nmplleaflet\t0.0.5\t\nmpmath\t0.19\t0.19\nmsgpack-python\t0.4.7\t0.4.8\nmultipledispatch\t0.4.8\t0.4.9\nmysql-connector-python\t2.0.4\t2.0.4\nmystic\t0.2a2.dev0\t\nnb-anacondacloud\t1.1.0\t\nnb-conda\t1.1.0\t\nnb-conda-kernels\t1.0.3\t\nnb_anacondacloud\t1.1.0\t1.2.0\nnb_conda\t1.1.0\t2.0.0\nnb_conda_kernels\t1.0.3\t2.0.0\nnbconvert\t4.2.0\t5.2.1\nnbformat\t4.0.1\t4.3.0\nnbpresent\t3.0.2\t3.0.2\nnetworkx\t1.11\t1.11\nnltk\t3.2.1\t3.2.4\nnose\t1.3.7\t1.3.7\nnotebook\t4.2.3\t5.0.0\nnumba\t0.26.0\t0.33.0\nnumexpr\t2.6.0\t2.6.2\nnumpy\t1.11.1\t1.13.0\noauth2client\t4.0.0\t\nodo\t0.5.0\t0.5.0\nopenjpeg\t2.1.2\t\nopenpyxl\t2.3.2\t2.4.7\nopenssl\t1.0.2h\t1.0.2l\noptcomplete\t1.2-devel\t\norange\t2.7.8\t2.7.8\npandas\t0.19.2\t0.20.2\npandas-gbq\t0.1.6\t\npandasql\t0.7.3\t0.7.3\nparamiko\t2.0.2\t2.1.2\npartd\t0.3.7\t0.3.8\npatch\t2.5.9\t2.5.9\npath.py\t8.2.1\t10.3.1\npath.py\t0.0.0\t10.3.1\npathlib2\t2.1.0\t2.2.1\npatsy\t0.4.1\t0.4.1\npep8\t1.7.0\t1.7.0\npickleshare\t0.7.2\t0.7.4\npillow\t3.2.0\t4.1.1\npip\t8.1.2\t9.0.1\npivottablejs\t0.1.0\t2.7.0\npivottablejs\t2.1.0\t2.7.0\nplotly\t1.12.12\t2.0.9\nply\t3.8\t3.10\nproj4\t4.9.3\t4.9.2\npsutil\t4.0.0\t5.2.2\npsycopg2\t2.6.2\t2.7.1\npy\t1.4.31\t1.4.34\npyOpenSSL\t16.2.0\t\npyasn1\t0.1.9\t0.2.3\npyasn1-modules\t0.0.8\t0.0.8\npycosat\t0.6.1\t0.6.2\npycparser\t2.14\t2.17\npycrypto\t2.6.1\t2.6.1\npycurl\t7.43.0\t7.43.0\npyflakes\t1.2.3\t1.5.0\npygments\t2.1.3\t2.2.0\npygmo\t1.1.5\t\npylint\t0.25.0\t1.6.4\npymysql\t0.7.9\t0.7.9\npyopenssl\t16.2.0\t17.0.0\npyparsing\t2.1.4\t2.1.4\npyqt\t4.11.4\t5.6.0\npyreadline\t2.1\t2.1\npysftp\t0.2.9\t\npystan\t2.14.0.0\t\npytables\t3.4.2\t3.3.0\npytest\t2.9.2\t3.1.1\npython\t2.7.12\t3.6.1\npython-dateutil\t2.5.3\t2.6.0\npython-google-places\t1.4.0\t\npython-weka-wrapper\t0.3.9\t\npytz\t2016.4\t2017.2\npytz\t2017.2\t2017.2\npywin32\t220\t220\npyyaml\t3.11\t3.12\npyzmq\t15.2.0\t16.0.2\nqgrid\t0.3.2\t0.3.2\nqt\t4.8.7\t5.6.2\nqtconsole\t4.2.1\t4.3.0\nqtpy\t1.0.2\t1.2.1\nquandl\t2.8.9\t3.1.0\nregex\t2017.6.23\t\nregex\t2017.06.23\t\nrequests\t2.18.1\t2.14.2\nrequests\t2.12.4\t2.14.2\nrequests-toolbelt\t0.8.0\t\nrope\t0.9.4\t0.9.4\nrsa\t3.4.2\t\nruamel-yaml\t-VERSION\t\nruamel.ordereddict\t0.4.6\t\nruamel.yaml\t0.15.18\t\nruamel_yaml\t0.11.14\t0.11.14\ns3fs\t0.0.8\t0.1.0\ns3fs\t0.0.7\t0.1.0\ns3transfer\t0.1.9\t0.1.10\ns3transfer\t0.1.7\t0.1.10\nscikit-image\t0.12.3\t0.13.0\nscikit-learn\t0.18.1\t0.18.1\nscipy\t0.18.0\t0.19.0\nscp\t0.10.2\t\nseaborn\t0.7.1\t0.7.1\nselenium\t2.53.5\t\nsetuptools\t23.0.0\t27.2.0\nshortuuid\t0.5.0\t\nsimplegeneric\t0.8.1\t0.8.1\nsimplejson\t3.10.0\t3.10.0\nsingledispatch\t3.4.0.3\t3.4.0.3\nsip\t4.16.9\t4.18\nsix\t1.10.0\t1.10.0\nsklearn-deap\t0.1.7\t\nsnowballstemmer\t1.2.1\t1.2.1\nsockjs-tornado\t1.0.3\t1.0.3\nsortedcontainers\t1.5.3\t1.5.7\nsphinx\t1.4.1\t1.6.2\nsphinx-rtd-theme\t0.1.9\t\nsphinx_rtd_theme\t0.1.9\t0.2.4\nspyder\t2.3.9\t3.1.4\nsqlalchemy\t1.0.13\t1.1.10\nsqlite\t3.13.0\t3.13.0\nssl_match_hostname\t3.4.0.2\t3.4.0.2\nstatsmodels\t0.8.0\t0.8.0\ntables\t3.4.2\t\ntablib\t0.11.2\t\ntabulate\t0.7.7\t0.7.5\ntblib\t1.3.0\t1.3.2\ntk\t8.5.18\t8.5.18\ntoolz\t0.8.0\t0.8.2\ntornado\t4.3\t4.5.1\ntpot\t0.6.4\t\ntqdm\t4.8.4\t4.14.0\ntraitlets\t4.3.1\t4.3.2\ntraittypes\t0.0.6\t\ntyping\t3.6.1\t3.6.1\ntzlocal\t1.4\t\numalqurra\t0.2\t\nunicodecsv\t0.14.1\t0.14.1\nupdate-checker\t0.12\t\nupdate_checker\t0.12\t\nuritemplate\t0.6\t\nuritemplate\t3.0.0\t\nurllib3\t1.21.1\t\nvc\t9\t14\nvs2008_runtime\t9.00.30729.1\t9.00.30729.5054\nweka\t1.0.0\t\nwerkzeug\t0.11.10\t0.12.2\nwheel\t0.29.0\t0.29.0\nwhitenoise\t3.3.0\t\nwidgetsnbextension\t1.2.6\t2.0.0\nworkerpool\t0.9.4\t0.9.4\nwsgiref\t0.1.2\t\nxerces-c\t3.1.4\t3.1.4\nxlrd\t1.0.0\t1.0.0\nxlsxwriter\t0.9.2\t0.9.6\nxlwings\t0.7.2\t0.10.4\nxlwt\t1.1.2\t1.2.0\nzict\t0.1.1\t0.1.2\nzlib\t1.2.8\t1.2.8\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis: Independant:\n\nprobaDensity(X,Y) = probaDensity(X) * ProbaDensity(Y)\nDistance in distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
			"file": "/D/Dropbox/_text/msg.txt",
			"file_size": 294539,
			"file_write_time": 131923809890000000,
			"settings":
			{
				"buffer_size": 282036,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"Package Control: ",
				"Package Control: Install Package"
			]
		],
		"width": 0.0
	},
	"console":
	{
		"height": 0.0,
		"history":
		[
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/D/_devs/Python01/aws/aapackage/zzsublime.sublime-project",
		"/D/_devs/awskeypair/aws_access.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/tool/project.py",
		"/D/_devs/Python01/aws/aapackage/travis_details.txt",
		"/D/_devs/Python01/aws/aapackage/setup.py",
		"/D/_devs/Python01/aws/aapackage/zdoc.txt",
		"/D/_devs/Python01/aws/aapackage/zresearch1.txt",
		"/D/_devs/Python01/aws/aapackage/aapackage/fin/alldata.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/nlp/japanese.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/allmodule.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/inout/report.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/inout/excel2.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/inout/excel.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/fin/forecast/pprinttest.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/tool/zpypi_publish.py",
		"/D/_devs/Python01/aws/aapackage/aapackage/tool/_HELP.py",
		"/D/_devs/Python01/aws/aapackage/.gitgnore",
		"/D/Downloads/batch_daemon_launch_cli.py",
		"/C/tmp/large.txt",
		"/D/Dropbox/__research_part/stochastic_control/ideas.txt",
		"/D/Dropbox/__research_part/stochastic_control/summary.txt",
		"/D/_devs/Python01/aws/conda-environments/test_installation.py",
		"/D/_devs/Python01/aws/conda-environments/env1_py36.yml",
		"/D/_devs/Python01/aws/conda-environments/env2_py37_tf20.yml",
		"/D/_devs/Python01/aws/conda-environments/env3_py36_tf20_torchGPU.yml",
		"/C/tmp/auto-commit_debug.txt",
		"/C/tmp/auto-commit-1.txt",
		"/C/tmp/auto-commit.txt",
		"/D/_devs/Python01/aws/aapackage/batch/util_batch.py",
		"/D/_devs/Python01/awsdoc/ec_config2.json",
		"/D/_devs/Python01/aws/_latest.py",
		"/D/_devs/Python01/aws/aapackage/batch/tasks/task_demo/subprocess_pygmo.py",
		"/D/_devs/Python01/aws/aapackage/batch/tasks/task_demo/subprocess_optim.py",
		"/D/_devs/Python01/aws/aapackage/batch/tasks/task_demo/main.py",
		"/D/_devs/Python01/aws/aapackage/_batch/__init__.py",
		"/D/_devs/Python01/aws/aapackage/ztest/ztest_log_all.txt",
		"/D/_devs/Python01/aws/aapackage/_batch/batch_sequencer.py",
		"/D/_devs/Python01/aws/aapackage/_batch/elvis_prod_20160102/batch_b_subprocess_launch.py",
		"/D/_devs/Python01/aws/aapackage/_batch/elvis_prod_20160102/batch_a_tasks_launch.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/elvis_prod_20160102/elvis_pygmo_optim.py",
		"/D/_devs/Python01/aws/aapackage/_batch/elvis_prod_20160102/pygmo_batch_generic.py",
		"/D/_devs/Python01/aws/aapackage/_batch/elvis_prod_20160102/elvis_pygmo_optim.py",
		"/D/_devs/Python01/aws/aapackage/_batch/elvis_prod_20160102/subprocess_launcher_01.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/elvis_prod_20160102/subprocess_launcher_01.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/elvis_prod_20160102/pygmo_batch_generic.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/elvis_prod_20160102/05_batch_getresults.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/elvis_prod_20160102/01_batch_param_generator.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/elvis_prod_20160102/00_AWS_BATCH.py",
		"/D/_devs/Python01/aws/aapackage/zzarchive/aws_zip_latest.py",
		"/D/_devs/Python01/project27/tasks/elvis_prod_20161228/subprocess_launcher_01.py",
		"/D/_devs/Python01/awsdoc/ec_config.json",
		"/C/Users/zenbook/Desktop/START Spot EC2.bat",
		"/D/_devs/Python01/project27/tasks/elvis_prod_20161228/00_AWS_BATCH.py",
		"/D/_devs/Python01/project27/tasks/elvis_prod_20161228/ec2_config.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/_task_batch_template/01_batch_param_generator.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/_task_batch_template/ec2_config.py",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher/AWS_BATCH_launcher.py",
		"/D/_devs/Python01/aws/aapackage/batch/batch_monitor/task_parallel.py",
		"/D/_devs/Python01/aws/aapackage/batch/batch_monitor/test_main.py",
		"/D/_devs/Python01/aws/aapackage/batch/batch_monitor/ospipen.py",
		"/D/_devs/Python01/aws/aapackage/batch/batch_monitor/mybatch.py",
		"/D/_devs/Python01/aws/aapackage/batch/batch_monitor/batch.py",
		"/D/_devs/Python01/aws/aapackage/batch/batch_monitor/todo",
		"/D/_devs/Python01/aws/aapackage/batch/batch_monitor/README.md",
		"/D/Downloads/RegistryShortcutsW1064bit/Enable s Folder Win10 64 - Copy.reg",
		"/D/_devs/Python01/project27/zdisks3/results/elvis_prod_28asset_1/output/20170103/batch_20170103_081704872381/output_result.txt",
		"/D/_devs/Python01/aws/ec2_rev2.py",
		"/D/_devs/keypair/oregon/aws_ec2_oregon.pem",
		"/D/_devs/keypair/config.cfg",
		"/D/_devs/Python01/aws/aapackage/zz_ec2_instance.csv",
		"/D/_devs/Python01/py27_win.yml",
		"/D/Dropbox/_text/__credit.txt",
		"/D/_devs/keypair/config.py",
		"/D/_devs/Python01/aws/ec2_instance.csv",
		"/D/_devs/keypair/aws_access.py",
		"/D/_devs/Python01/aws/arita37/index.html",
		"/D/_devs/Python01/aws/ec2_rev1.py",
		"/D/_devs/Python01/aws/aapackage/batch/batch_old/test_aws.py",
		"/D/app/xplorer2/changes.txt",
		"/D/_devs/Python01/aws/aapackage/batch/batch_old/config.cfg",
		"/D/_devs/Python01/aws/aapackage/batch/aws_batcher_old/task/_task_batch_template/00_AWS_BATCH.py",
		"/D/_devs/Python01/aws/aapackage/.travis.yml",
		"/D/Dropbox/inter_prep.txt",
		"/D/Dropbox/_text/email1.txt",
		"/D/Dropbox/ok",
		"/D/Downloads/Downloads/_NewInstall/MediaMonkey 3.2.0.1294/info.txt",
		"/D/Dropbox/_text/msg.txt",
		"/D/Dropbox/_text/jap_listening.txt",
		"/D/Dropbox/_text/interview3.txt",
		"/D/Dropbox/_text/_raku (asus1-PC's conflicted copy 2017-10-22).txt",
		"/D/Dropbox/_text/interview.txt",
		"/D/Dropbox/_text/japanese.txt",
		"/C/Program Files (x86)/Dropbox/Client/Dropbox.exe",
		"/D/_devs/Python01/project27/cloud/google_cloud_setup.py",
		"/D/_devs/Python01/aws/aapackage/batch/batcher/task_parallel.py",
		"/D/_devs/Python01/aws/aapackage/batch/batcher/aws_batch_launcher/AWS_BATCH_launcher.py",
		"/D/_devs/Python01/aws/aapackage/batch/batcher/todo",
		"/D/_devs/Python01/aws/aapackage/batch/batcher/test_main.py",
		"/D/_devs/Python01/aws/aapackage/tool/codesource.py",
		"/D/_devs/Python01/aws/aapackage/tool/codeanalysis.py",
		"/D/_devs/Python01/project27/github/configmy/configmy/configmy.py",
		"/D/_devs/Python01/project27/github/configmy/zpypi_publish.py",
		"/D/_devs/aws/keypairs/oregon/aws_ec2_oregon.pem",
		"/D/Downloads/ACDSEE/ACD.Systems.ACDSee.Pro.v7.0.137.x64.Incl.Keymaker-CORE/ACD.Systems.ACDSee.Pro.v7.0.137.x64.Incl.Keymaker-CORE/ACD.Systems.ACDSee.Pro.v7.0.137.x64.Incl/CORE.NFO",
		"/D/_devs/Python01/pkensho/cloud/cloud_aws/aws_main.py",
		"/D/_devs/Python01/pkensho/aapackage/__init__.py",
		"/D/app/xplorer64/changes.txt"
	],
	"find":
	{
		"height": 27.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"upwork",
			"yoyaku",
			"shinkan",
			"shinka ",
			"export",
			"conda env",
			"conda export",
			"export",
			"conda env",
			"export",
			"':",
			":,",
			"linked",
			"face",
			"xplorer",
			"noelkevin1",
			"passpo"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"',",
			","
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 6,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "/D/Dropbox/_text/email1.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 128533,
						"regions":
						{
						},
						"selection":
						[
							[
								489,
								489
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 225.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "/C/tmp/large-1.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 417,
						"regions":
						{
						},
						"selection":
						[
							[
								417,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/ShellScript/Bash.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "/D/Dropbox/_text/coding.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 278508,
						"regions":
						{
						},
						"selection":
						[
							[
								3236,
								3236
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 30.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "ztodo_list.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5903,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 504.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "/D/Dropbox/_text/interview3.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7593,
						"regions":
						{
						},
						"selection":
						[
							[
								461,
								461
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "/D/Dropbox/inter_prep.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4636,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "/D/Dropbox/_text/msg.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 282036,
						"regions":
						{
						},
						"selection":
						[
							[
								1724,
								1724
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 540.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 27.0
	},
	"input":
	{
		"height": 0.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "zzsublime.sublime-project",
	"replace":
	{
		"height": 50.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": false,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 150.0,
	"status_bar_visible": false,
	"template_settings":
	{
	}
}
