###############################################################################
Improvements toDO list
1)
util_cpu.ps_find_procs_by_name(

Use RegEx to find this pattern 
    "*tasks/*main.py"    or    "*tasks/*main.sh"


batch_daemon_launch_cli.py can DIE before batch_daemon_monitor_cli.py,
so we cannot use reference
Better to have loose couplings before both


2)  
Organize better test results.





3) Create AMI or Snapshot on the same instance linux.  (without reboot)
https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html
(ie in the case instance is launched from Spot Instance )



4)




5)   









###############################################################################
###############################################################################
#######  Cloud9 usage #########################################################
### DO NOT restart the instance 
source activate py36
which python
which conda



zbatch.sh   : Launch the processes
zbatch_kill :  Kill the 2 processes


cd  /home/ubuntu/aagit/aapackage/batch     #Root of github repo


/home/ubuntu/tasks      :   Folder of the tasks
/home/ubuntu/zlog       :   Folder of the logs
/home/ubuntu/tasks_out  :   Folder of the tasks out


git add --all 
git commit -m "your msg"


topc    : Show all python processes running

gitpush :   send to github
gitpull :   get from github









###############################################################################
###############################################################################
source activate py36

cd /home/ubuntu/

batch_daemon_launch_cli.py  --task_folder  tasks  --log_file   zlog/batchdaemong.log  --mode daemon  --waitsec 60  &

sleep 3

batch_daemon_monitor_cli.py --monitor_log_file zlog/process_log_file.log   --log_file zlog/batchdaemon_monitor.log    --mode daemon   --waitsec 20








###############################################################################
batch_daemon_launch_cli.py
   scans sub-folder in /tasks/
      and execute  /tasks/taskXXXX/main.py in that folder


batch_daemon_monitor_cli.py
   Monitor CPU / RAM usage on server


batch_local_aws_cli.py
  Launch AWS Batch
  and send task to aws folders
  and run on aws batch_monitor
  retrieve from aws to local












################################################################################
################################################################################
ENV install
    conda create -n testenv --yes $TO_INSTALL --file zbuild/py36.txt
    source activate testenv
    pip install arrow==0.10.0 attrdict==2.0.0 backports.shutil-get-terminal-size==1.0.0 configmy==0.14.87 github3.py==1.2.0 jwcrypto==0.6.0 kmodes==0.9 rope-py3k==0.9.4.post1 tables==3.3.0 tabulate==0.8.2 uritemplate==3.0.0
    pip install dcgpy

    conda install -c conda-forge  pygmo  pagmo




main.py
   1) Execute python script



main.py
   2) Mode parallel process :
       For each set of hyperparams,
          launch subprocess_optim

          from batch.util_batch import *
          batch_process(   )






















- Need to review functions :
    - util_batch.py :
        - batch_run_infolder :

        - batch_generate_hyperparameters :
            - df dataframe
            - need to check if hyper_dict has "key" key, and "min"/"max" keys for "key" key

        - batch_parallel_subprocess :
            - is csv really needed ? because, for each line in the csv file,
                subprocess_script is called with --hyperparam_ii={line_number}, why ?
                A for loop can do the job
                you seems not undertstanding the algorithm....
                Think of Nneural network hyperparametrization


    - util_log.py :
        - printlog :
            - if writelog:  alternative of logging for quick debugging, logging may have issues sometimes.





#############################################################################
ENV install
    conda create -n testenv --yes $TO_INSTALL --file zbuild/py36.txt
    source activate testenv
    pip install arrow==0.10.0 attrdict==2.0.0 backports.shutil-get-terminal-size==1.0.0 configmy==0.14.87 github3.py==1.2.0 jwcrypto==0.6.0 kmodes==0.9 rope-py3k==0.9.4.post1 tables==3.3.0 tabulate==0.8.2 uritemplate==3.0.0


    conda install -c conda-forge  pygmo  pagmo
    pip install dcgpy







###############################################################################
batch_daemon_launch_cli.py
   scans sub-folder in /tasks/
      and execute  /tasks/taskXXXX/main.py in that folder


main.py
   1) Execute python script



main.py
   2) Mode parallel process :
       For each set of hyperparams,
          launch subprocess_optim

          from batch.util_batch import *
          batch_process(   )


batch_daemon_monitor_cli.py
   Monitor CPU / RAM usage on server



batch_launch_aws_cli.py
  Launch AWS Batch
  and send task to aws folders


































































meta_batch_task.py :
   ...task_folder/task1/mybatch_optim.py  + hyperparamoptim.csv +  myscript.py ...
   ...task_folder/task2/mybatch_optim.py  + hyperparamoptim.csv + .myscript.py ...


meta_batch_task.py :
   for all "task*" folders in task_working_dir  :
       run subprocess taskXXX/mybatch_optim.py



mybatch_optim.py :
  for all rows ii of hyperparams.csv
     run subprocess  myscript.py  ii
     check CPU suage with psutil.   CPU usage < 90 %   mem usage < 90%


### not needed from you
aws_batch_script.py :
   for all tasks folder in LOCAL PC :
       transfer by SFTP to REMOTE Task folder (by zip)

   subprocess  meta=batch_task.py on REMOTE PC.


##### Warning :
  Please check my initial code to make sure most of functionnalities are here in your code


####################################################################
batch_sequencer.py :
  Launch 1 subprcoess per hyperparam row.










##################################################################################
Task 1 :
  Create/Update large scale Genetic OPtimizer batch


### Existing code is here:
  https://github.com/arita37/a_aapackage/tree/master/batch/batch_aws/task/elvis_prod_20161231/



Launch the batcher for each set of hyper-parameters.
  pygmo_batch_generic.py

Launch 1 single Optimization for 1 hyper parameters
  elvis_pygmo_optim.py


A)
  this fileis using old pygmo
  code should be updated to use new pygmo 2.0


B)
  Current batch code is quite ad hoc...
  would like to make it generic as follow :
     function to be optimized in one file.
     list of hyper parameters into a csv file

     for EACH set of hyper-parameters (1 line of pandas csv file) :
        1 subprocess is launched and optimization done by  pygmo
        result are saved somewhere.




###################################################################################################
######### Architecture :  #########################################################################
1)  mybatch.py
  from aapackage.batch import batch

  batch.batch_run(
      file_script      = "folder/myscript_optim.py",
      hyperparam_file  = "hyperparam_list.csv",
      out_folder       = "/myresult/batch1/",
      schedule         = "11/02/2019 12:30"
  )  :

      for ii, row of df_hyperparam_list.iterrows() ,
          pid = subprocess  row["file_script"]  ii  "hyperparam_list.csv"  out_folder   ###All subprocess are independdant !!!!
          server_info = "_".join( util.os_server_info())
          logs(arrow.timestamp, pid, server_info,  row["file_script"]  ii  hyperparam_file )
          wait(waitsecs)

  batch.monitor_pid(auto_close=True)  ### Auto close the EC2 instance when no PID is running.




3) myscript_optim.py  ii  hyperparam_file
   from aapackage import optim  ### Layer above pygmo to abstract under functional form.

   df = pd.csv_read( args.hyperparam_file )
   df = df.iloc[ii, :]  # only 1 line

   import_load(  df["myfun_file"].values )  as myfun_file # "folder/myfun.py"   string load of module.

   optim1 = optim.optim(
              fun_eval=     myfun_file.mypersoname_eval,
              fun_bound=    myfun_file.mypersoname_bound,
              fun_gradient= None,
              algo= "de_1227",
              algo_origin="scipy/pygmo/...",
              dict_params = df.to_dict()
            )

   res = optim1.optimize()
   vv  = res.dump()
   util.logs( vv , type="csv")



3) User define function is defined into a python file : myfun.py
  myfun_file.py

     def mypersoname_eval_()
        return np.float    or vector of float

     def mypersoname_bound_()
       return [(-0.1 , 0.4)   ,  (0.1 , 0.4)   ]

     def fun_gradient_()



####################################################################################
#### Version with auto-launch of AWS  ##############################################
0) mybatch_ssh.py
  import util_aws

  ip0 = util_aws.aws_launch()  # start ec2

  ssh0 = ssh(ip0, ...)
  ssh.put( from="local/task/", to="ec2/task/"  )

  ssh.exec( "python mybatch.py"  )   ####


















  from myscript import fun_eval, boun


















