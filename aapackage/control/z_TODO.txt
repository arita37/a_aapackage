
"""
  Variance Realized =  Sum(ri*:2)
  rI**2 =   Sum(wi.ri)**2
  Return = sum(ri) = Total return


Idea to have policy-Gradient :
      discrete actions


in RL setting :
      Probability of actions --> Generate sample of action,
                                 Get reward from sample.
                                 And add reward to the cost functions.


In COntrol problem, no sampling of action, 
         we estimate the optimal value.  Does not modify the state.
         Based on estimate, we estimate the final cost.

Hybrid between Optimization and RL.
Here, we select Optimal action from ArgMax





"""


        """
          0.49538052,  0.50057834 ,0.004041157,
          0.49538052,  0.0,        0.4041157

Min Vol {0: 0.1386692125551192, 1: 0.1780535352118179, 2: 0.6832772522330629}


Min Vol {0: 0.7550561932704953, 1: 0.18876404280450715, 2: 0.0561797639249976}


Min Vol {0: 0.03636362758587138, 1: 0.6909091098785403, 2: 0.2727272625355884}


        """




# Library dependencies for the python code.  You need to install these with
# `pip install -r requirements.txt` before you can run this.


# from decimal import Decimal
# from agents.ornstein_uhlenbeck import OrnsteinUhlenbeckActionNoise
# from agents.pg import PG










os.environ





"""
Multivariate Garch
    over a time step
    
    
    p. w1 +  (1-p).W2
    
    
    dS/S 
    
    
    dS1/S1 . dS2/S2 = Correl
   
   
    Projection of S1 over Browninan
    S1 is variable.
    
    Use Variance sigma(t) as input.

    W2 = variance



Simulate Path
Correlated path
Brownian Motions



Cost
    Variance :
    
    
Weight at each steps


Xinput = PastReturns
Output :  Weights



Return target



Scenarios :

  regime change
  
  




"""





